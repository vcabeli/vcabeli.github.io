<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>VCabeli</title><link>https://vcabe.li/</link><description>Recent content on VCabeli</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 24 Nov 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://vcabe.li/index.xml" rel="self" type="application/rss+xml"/><item><title>Estimating mutual information on finite samples</title><link>https://vcabe.li/posts/estimating_mi/</link><pubDate>Wed, 24 Nov 2021 00:00:00 +0000</pubDate><guid>https://vcabe.li/posts/estimating_mi/</guid><description>The previous posts established why we would want to estimate information-theoretic quantities from data, and now we will study how and how well it can be done. Several decades of research later, and almost as many different estimators as there were applications, it may come as a surprise that many basic questions remain unanswered (although recent progress has been made, especially in the continuous case). To understand why, recall that mutual information $\II(X;Y)$ is defined for $X$ and $Y$ of any dimensions.</description></item><item><title>Mutual information with mixed variables</title><link>https://vcabe.li/posts/mi_with_mixed_variables/</link><pubDate>Wed, 17 Nov 2021 00:00:00 +0000</pubDate><guid>https://vcabe.li/posts/mi_with_mixed_variables/</guid><description>In many real-life datasets, particularly in medical records of patients, we might encounter both discrete and continuous variables and want to measure their interactions, without favoring one type of variable or the other. In other words, the dependency measure should scale with the signal-to-noise ratio the same way for continuous-continuous, discrete-discrete or discrete-continuous combinations. In the context of constraint-based approaches, another layer of difficulty is added as the same applies for the variables of the conditioning set.</description></item><item><title>Introduction to information theoretic measures</title><link>https://vcabe.li/posts/intro_to_info_theory/</link><pubDate>Wed, 10 Nov 2021 00:00:00 +0000</pubDate><guid>https://vcabe.li/posts/intro_to_info_theory/</guid><description>How do you measure the dependency between two variables ? Formally speaking, $X$ and $Y$ are dependent, i.e. they share some information, if their joint distribution is not the same as the product of their marginals : $X \indep Y \Leftrightarrow p(X,Y) \not= p(X)p(Y)$. Only a few measures can measure dependency according to this strict equivalence : Pearson correlation of course detects only linear relationships, it is generalized by the distance correlation for all distributions with finite first moments.</description></item><item><title/><link>https://vcabe.li/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://vcabe.li/about/</guid><description>I am Vincent, and I am finishing a PhD at the Physico-Chimie Curie lab on causal graph reconstruction from observational data of biological or clinical nature under the supervision of Herv√© Isambert.
During my thesis, I worked on an information-theoretic constraint-based approach to infer causality from mixed data of any type, to be able to deal with heterogeneous real-life datasets like eletronic medical records (1).
I also contributed with my colleagues to make this class of algorithms more interpretable and more reliable (2, 3).</description></item></channel></rss>