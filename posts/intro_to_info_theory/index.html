<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Introduction to information theoretic measures | VCabeli</title>
<meta name=keywords content>
<meta name=description content="How do you measure the dependency between two variables ? Formally speaking, $X$ and $Y$ are dependent, i.e. they share some information, if their joint distribution is not the same as the product of their marginals : $X \indep Y \Leftrightarrow p(X,Y) \not= p(X)p(Y)$. Only a few measures can measure dependency according to this strict equivalence : Pearson correlation of course detects only linear relationships, it is generalized by the distance correlation for all distributions with finite first moments.">
<meta name=author content>
<link rel=canonical href=https://vcabe.li/posts/intro_to_info_theory/>
<link crossorigin=anonymous href=/assets/css/stylesheet.min.9399b687cb22049d993961bb359a8b61e432681fbfb99a46ed166af966922bb7.css integrity="sha256-k5m2h8siBJ2ZOWG7NZqLYeQyaB+/uZpG7RZq+WaSK7c=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://vcabe.li/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://vcabe.li/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://vcabe.li/favicon-32x32.png>
<link rel=apple-touch-icon href=https://vcabe.li/apple-touch-icon.png>
<link rel=mask-icon href=https://vcabe.li/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.91.2">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript><img .header-img style=position:absolute;width:100%;background-size:cover src=/media/banners/Renoir_yeux.png alt=banner id=banner>
<script type=text/javascript defer>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],macros:{II:"{\\operatorname{I}}",HH:"{\\operatorname{H}}",indep:"{\\perp\\!\\!\\!\\perp}",notindep:"{\\not\\perp\\!\\!\\!\\perp}"}},svg:{fontCache:'global'}}</script>
<script type=text/javascript id=MathJax-script defer src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script>
<script src=https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js type=text/javascript></script>
<link rel=stylesheet type=text/css href=/hugo-cite.css><meta property="og:title" content="Introduction to information theoretic measures">
<meta property="og:description" content="How do you measure the dependency between two variables ? Formally speaking, $X$ and $Y$ are dependent, i.e. they share some information, if their joint distribution is not the same as the product of their marginals : $X \indep Y \Leftrightarrow p(X,Y) \not= p(X)p(Y)$. Only a few measures can measure dependency according to this strict equivalence : Pearson correlation of course detects only linear relationships, it is generalized by the distance correlation for all distributions with finite first moments.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://vcabe.li/posts/intro_to_info_theory/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-11-10T00:00:00+00:00">
<meta property="article:modified_time" content="2021-11-10T00:00:00+00:00">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Introduction to information theoretic measures">
<meta name=twitter:description content="How do you measure the dependency between two variables ? Formally speaking, $X$ and $Y$ are dependent, i.e. they share some information, if their joint distribution is not the same as the product of their marginals : $X \indep Y \Leftrightarrow p(X,Y) \not= p(X)p(Y)$. Only a few measures can measure dependency according to this strict equivalence : Pearson correlation of course detects only linear relationships, it is generalized by the distance correlation for all distributions with finite first moments.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://vcabe.li/posts/"},{"@type":"ListItem","position":3,"name":"Introduction to information theoretic measures","item":"https://vcabe.li/posts/intro_to_info_theory/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Introduction to information theoretic measures","name":"Introduction to information theoretic measures","description":"How do you measure the dependency between two variables ? Formally speaking, $X$ and $Y$ are dependent, i.e. they share some information, if their joint distribution is not the same as the product of their marginals : $X \\indep Y \\Leftrightarrow p(X,Y) \\not= p(X)p(Y)$. Only a few measures can measure dependency according to this strict equivalence : Pearson correlation of course detects only linear relationships, it is generalized by the distance correlation for all distributions with finite first moments.","keywords":[],"articleBody":"How do you measure the dependency between two variables ? Formally speaking, $X$ and $Y$ are dependent, i.e. they share some information, if their joint distribution is not the same as the product of their marginals : $X \\indep Y \\Leftrightarrow p(X,Y) \\not= p(X)p(Y)$. Only a few measures can measure dependency according to this strict equivalence : Pearson correlation of course detects only linear relationships, it is generalized by the distance correlation for all distributions with finite first moments. We can also measure the covariance in a kernel space, which gives the powerful Hilbert Schmidt Independence Criterion, or we can even use supervised learning and feature importance measures such as the SHAP scores to get a sense of what is predictive of $Y$. But each of these methods come with certain caveats and limitations, and may not be defined for the general case, without any assumption on $X$, $Y$, or $p(X,Y)$.\n  Examples of dependencies : $X \\indep Y \\Leftrightarrow p(X,Y) \\not= p(X)p(Y)$   The mutual information is a measure of the dependency between two random variables in the most general sense. It is agnostic to the nature of the random variables and of their relationship : noted $\\operatorname{I}(X;Y)$, it simply defines the quantity of information one knows about $X$ by knowing $Y$, and vice-versa. It was introduced by Claude Shannon in 1948 to characterize communication channels (Citation: Shannon, 1948Shannon, C. (1948). A mathematical theory of communication. The Bell system technical journal, 27(3). 379–423. ) but it has found success in a wide range of applications since. It is still seen by many as the ideal dependency measure, although it is difficult to use in practice. This post introduces the necessary concepts to define the mutual information, largely inspired by (Citation: Cover \u0026 Thomas, 2012Cover, T. \u0026 Thomas, J. (2012). Elements of information theory. John Wiley \u0026 Sons. ) , and hints as to how it can be applied to causal graph inference. The difficult process of estimating these values from sampled data will be discussed in a later post.\nDefinitions Entropy and mutual information Before giving the definition of the mutual information between two random variables, it is a good idea to start with the self-information contained in a single variable, called the entropy. Let $X$ be a discrete random variable with possible values in $\\mathcal{X}$ and a probability mass function $p(x) = Pr{X = x}, x \\in \\mathcal{X}$. The entropy $\\operatorname{H}(X)$ of $X$ is defined by :\n$$\\operatorname{H}(X) = - \\sum_{x \\in \\mathcal{X}}p(x)\\log p(x)$$\nIt is expressed in bits with a logarithm to the base 2, or nats with the base $e$, and it denotes the average information or \"surprise\" that is carried by a random variable. To get a better understanding of this concept, consider a game of chance where you try to predict the result of a coin flip. If the coin is balanced, each realisation has the same \"surprise\" as both outcomes, heads or tails, are equiprobable. When the coin is biased towards one outcome with probability $p$, the average surprise decreases as $p$ approaches $0$ or $1$ at which point it becomes null and your willingness to bet on the outcome increases. Note that the entropy characterises the distribution of a random variable and not the surprise of one realisation.\nThis definition can be naturally extended to a pair of random variables $X$ and $Y$ (which can be thought of a single two-dimensional variable), giving the joint entropy :\n$$\\operatorname{H}(X,Y) = - \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}}p(x,y)\\log p(x,y)$$\nWe can also define the conditional entropy, i.e. the expected \"surprise\" of the conditional distribution of a variable $Y$ given $X$ :\n$$\\operatorname{H}(Y|X) = - \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} p(x,y){\\log\\frac{p(x,y)}{p(x)}}$$\nOne desirable property of these information-theoretic values is that they can be combined in an intuitive manner with the \"chain rule\" :\n$$\\begin{aligned} \\operatorname{H}(X,Y) = \\operatorname{H}(X) + \\operatorname{H}(Y|X)\\\\ \\operatorname{H}(Y|X) = \\operatorname{H}(X,Y) - \\operatorname{H}(X) \\end{aligned}$$\nIndeed, it comes easily to think of the joint entropy of $X$ and $Y$ as the sum of the information carried by $X$ plus the residual information of $Y$ after \"removing\" the knowledge of $X$, as some information may be redundant between the two.\nSo far we have only defined the entropy of discrete variables, but our ideal dependency measure should also include continuous or mixed (part discrete, part continuous) distributions which are present in real-life datasets. Continuous variables are defined by a probability density function $f(x)$ instead of a mass function, which was naturally considered by Shannon to be equivalent in the definition for their entropy. There are however subtle differences with the discrete counterpart, which is why this value is called the differential entropy and is noted $h(X)$ :\n$$h(X) = -\\int_{S} f(x)\\log f(x),dx ,$$\nwith $S$ as the support set of $X$ where $f(x)  0$.\nThe source of the differences between differential and discrete entropies becomes evident with our previous example of predicting the value of a random variable : what is the surprise of a realisation of $X$ given that there is an infinite number of possible values in any continuous interval, each with a probability that tends to $0$ ?\nIt is better to think of the differential entropy as an estimate of the effective volume that a random variable occupies : a very focused distribution will have a low entropy as opposed to a more dispersed distribution with more room for randomness hence higher entropy. Formally, the differential entropy is the logarithm of the length of the smallest interval that contains most of the probability (Citation: Cover \u0026 Thomas, 2012Cover, T. \u0026 Thomas, J. (2012). Elements of information theory. John Wiley \u0026 Sons. ) : for example, the differential entropy of a uniform distribution on the interval $[0,a]$ is $\\log(a)$.\nAlthough the differences between entropy and differential entropy go beyond the scope of this thesis, it is still interesting to mention them to understand why discretization has been so popular for so long to estimate the entropy or mutual information between samples of continuous variables. Even though, as we will see in future posts, one must be careful to discretize a continuous variable without introducing bias.\nEntropy and differential entropy behave similarly and are interchangeable in the settings that interest us, namely for the joint and conditional differential entropy, the chain rule and especially the relationship to mutual information. For the rest of this post, the probability mass function $p(x)$ can be replaced by the density function $f(x)$, and $H(X)$ by $h(X)$ to switch from discrete to continuous random variables. The special case of mixed variables with both continuous and discrete parts will be reviewed later!\nWe have established that information theory gives us the necessary tools to define the entropy of a random variable (which can be multidimensional or conditional), i.e. the amount of information needed on average to describe it. Next we show how we can also formalize how much information two variables have in common, giving a measure of how (in)dependent they are. For this we need to introduce the Kullback-Leibler divergence, also called the relative entropy. The relative entropy $D_\\text{KL}(p \\parallel q)$ is a measure of the difference between two distributions $p$ and $q$ defined on the same space $\\mathcal{X}$:\n$$ D_\\text{KL}(p\\parallel q) = \\sum_{x \\in \\mathcal{X}} p(x) \\log \\frac{p(x)}{q(x)} $$\nAlso called the Kullback-Leibler distance (although not a distance in the usual sense as it is not symmetric), it can thought of as the cost of describing the distribution $p$ when using $q$ as a reference model. As such, it is null if and only if $p = q$ and it is always non-negative.\nNow we get back to our original goal which is to define the dependency between two random variables $X$ and $Y$. In the most general sense, $X$ and $Y$ are independent if the realization of one does not affect the probability distribution of the other. Formally put, two random variables $X$ and $Y$ with marginal distributions $p(x)$, $p(y)$ and a joint distribution $p(x,y)$ are independent if and only if $p(x,y) = p(x)p(y)$. If these two quantities differ, some information is being shared between $X$ and $Y$ : knowing about $X$ tells us something about $Y$ and vice versa.\nUsing the measure of divergence we just introduced, it becomes natural to think of the divergence between the joint distribution and the product of marginals as a direct measure of the dependency. It is in fact the definition of the mutual information $\\operatorname{I}(X;Y)$ : $$\\begin{aligned} \\operatorname{I}(X;Y) \u0026= D_\\text{KL}\\left(p(x,y) \\parallel p(x) p(y)\\right)\\\\ \u0026= \\sum_{y \\in Y} \\sum_{x \\in X} { p(x,y) \\log{ \\left(\\frac{p(x,y)}{p(x),p(y)} \\right) }} \\end{aligned}$$\nIn agreement with our interpretation of the relative entropy, assuming the independence model where $p(x,y)=p(x)p(y)$ the mutual information is literally the extra bits that are required to encode the interaction between $X$ and $Y$. It is always positive, or null if and only if $X$ and $Y$ are independent.\n Just like the other measures, it fits naturally in the \"chain rule\" and can be expressed intuitively in terms of entropies.\n$$\\begin{aligned} \\label{eq:I_H_chainrule} \\operatorname{I}(X;Y) \u0026= \\operatorname{H}(X) - \\operatorname{H}(X|Y) \\\\ \u0026= \\operatorname{H}(Y) - \\operatorname{H}(Y|X) \\\\ \u0026= \\operatorname{H}(X) + \\operatorname{H}(Y) - \\operatorname{H}(X,Y) \\end{aligned}$$\nWhat makes the mutual information a particularly interesting measure is its unique blend of desirable properties.\nFirst, it satisfies the Data Processing Inequality (DPI) which states that one cannot increase the information content of a signal by processing it. Formally, if $n$ variables form a Markov chain $X_0 \\rightarrow X_1 \\rightarrow \\cdots \\rightarrow X_n$, then $\\operatorname{I}(X_i;X_j) \\ge \\operatorname{I}(X_i; X_k)$ with $i In relation to the first point, mutual information is also widely considered to be equally sensitive to all types of relationships. This concept was termed \"equitablity\" by Reshef et al. (Citation: Reshef, Reshef \u0026 al., 2011Reshef, D., Reshef, Y., Finucane, H., Grossman, S., McVean, G., Turnbaugh, P., Lander, E., Mitzenmacher, M. \u0026 Sabeti, P. (2011). Detecting Novel Associations in Large Datasets. Science (New York, N.y.), 334(6062). 1518–1524. https://doi.org/10.1126/science.1205438 ) (although in a flawed form) and was then formally investigated by Kinney et Atwal (Citation: Kinney \u0026 Atwal, 2014Kinney, J. \u0026 Atwal, G. (2014). Equitability, mutual information, and the maximal information coefficient. Proceedings of the National Academy of Sciences, 111(9). 3354–3359. https://doi.org/10.1073/pnas.1309933111 ) . Kinney et Atwal’s \"Self-Equitability\" is defined to characterize a dependence measure $D[X;Y]$ if and only if it is symmetric between $X$ and $Y$, and : $$D[X;Y] = D[f(X);Y]$$ with $f$ any deterministic function, $X \\leftrightarrow f(X) \\leftrightarrow Y$ forming a Markov chain. Put roughly, an equitable measure means that one can measure the strength of the signal (as compared to the noise) between $Y$ and $f(X)$ without having to know the underlying function $f$.\nNot only is it invariant to invertible transformations of $X$ and $Y$, it is also invariant under any monotonic (i.e. rank preserving) transformations.\nPut together, these three properties make the mutual information particularly interesting for general case causal discovery. True causal discovery should make no assumption of the natural mechanisms that produced the observed data, whether on the scale of the unit or shape of the joint distributions. As a simple example, a case can be made to measure the human weight in a logarithmic scale instead of a linear one : for most health related aspects, a difference of 30 kilograms is much more significant between 60 and 90kgs than between 120 and 150kgs. In an experimental context, we can think of the causal diagram as the natural laws that have produced the observations, which are themselves a function of the \"observing\" process. The self-equitability property and invariance under transformation go some way towards freeing ourselves from this observation process and our own biases.\nFinally, as will be discussed later, these properties hold for any type of variable $X$ and $Y$, be it continuous, discrete (ordinal or not), or a mixture of discrete and continuous parts.\nA notable disadvantage of mutual information compared to other measures is that the bit, unit of information, is not commonly understood, and the fact that it is unbounded upwards. One usually cannot easily derive a p-value from a mutual information estimation on sampled data, which makes it harder to communicate (although the benefits of standardising the p-value have been called into question (Citation: Amrhein, Greenland \u0026 al., 2019Amrhein, V., Greenland, S. \u0026 McShane, B. (2019). Scientists rise up against statistical significance. Nature, 567(7748). 305–307. https://doi.org/10.1038/d41586-019-00857-9 ; Citation: Leek, McShane \u0026 al., 2017Leek, J., McShane, B., Gelman, A., Colquhoun, D., Nuijten, M. \u0026 Goodman, S. (2017). Five ways to fix statistics. Nature, 551(7682). 557–559. https://doi.org/10.1038/d41586-017-07522-z ) ). Different ideas to evaluate significativity will be presented in a future post, but for now we are only interested in the \"oracle\" value, when the sample size $N$ tends to infinity and the strict equivalence $X \\indep Y \\leftrightarrow \\operatorname{I}(X;Y) = 0$ holds.\nIn the discrete case, its value is actually familiar as it is in fact the G-statistic multiplied by a factor of $N$. With $O_i$ the number of observations in a contingency table between two categorical variables $X$ and $Y$, with $i$ joint levels, and $E_i$ the expected counts under the null hypothesis $X \\indep Y$, the G-statistic is defined as : $$G = 2 \\sum_i O_i \\log\\left(\\frac{O_i}{E_i}\\right)$$\nRecall the definition of the KL divergence (Eq [eq:KL_def]{reference-type=“ref” reference=“eq:KL_def”}), the same formula as above except for the frequencies (noted $o_i$ and $e_i$) instead of the counts $O_i$, $E_i$. Using the frequencies, the G-statistic becomes : $$\\begin{aligned} \\begin{split} G \u0026= 2N \\sum_i o_i \\log\\left(\\frac{o_i}{e_i}\\right)\\\\ \u0026= 2N \\cdot D_\\text{KL}(o\\parallel e)\\\\ \u0026= 2N \\cdot \\operatorname{I}(X;Y) \\end{split} \\end{aligned}$$ It also follows that the mutual information is related to the $\\chi^2$ test, as it is itself a second-order Taylor approximation of the G-statistic.\nFor two continuous variables, it may be harder to get a good intuition of what the mutual information measures. A first property that may seem odd is that if $X$ is continuous and $Y=X$, then $\\operatorname{I}(X;Y) = \\infty$. This looks as though it contradicts the chain rule (Eq [eq:I_H_chainrule]{reference-type=“ref” reference=“eq:I_H_chainrule”}), since it implies that $\\operatorname{H}(X)-\\operatorname{H}(X|X) = \\infty$. It is in fact one of the differences between entropy $\\operatorname{H}$ and differential entropy $h$ which is unbounded in the case of a singularity $h(X|X) = - \\infty$, unlike $\\operatorname{H}$ which is always finite. Thankfully, this theoretical property does not bleed into the real world for several reasons : first, even continuous distributions have always finite differential entropies since we actually treat real numbers up to a finite number of significant digits. Second, much like a correlation coefficient on observed data never reaches $1$, we should not ever need to estimate $\\operatorname{I}(X;Y)$ where $X=Y$. The analytical value of the mutual information on a bivariate Gaussian with correlation coefficient $\\rho$ is actually known :\n$$\\label{eq:mi_gaussian} \\operatorname{I}(X;Y) = -\\frac{1}{2}\\log(1 - \\rho^2)$$\n This equivalence is useful for practitioners who are unfamiliar with mutual information and wish to translate it to the better known dependence measure : thanks the self-equitability property if one could transform two variables to a bivariate Gaussian distribution preserving the signal-to-noise ratio, using the analytic result one could then get the corresponding correlation coefficient.\nConditional information and interaction information Information theory also allows us to measure the conditional dependence between two variables $X$ and $Y$ given a third, $Z$. The conditional mutual information is defined as the expected value of the mutual information between $X$ and $Y$ given a third variable $Z$: $$ \\operatorname{I}(X;Y|Z) = \\mathbb{E}_z\\left[D_\\text{KL}\\left(p((x,y)|z) \\parallel p(x|z) p(y|z)\\right)\\right] $$\nIt is symmetrically decomposable into two-points mutual informations : $$\\begin{aligned} \\begin{split} \\operatorname{I}(X;Y|Z) \u0026= \\operatorname{I}(X;Y,Z) - \\operatorname{I}(X;Z)\\\\ \u0026= \\operatorname{I}(Y;X,Z) - \\operatorname{I}(Y;Z) \\end{split} \\end{aligned}$$\nwhere $X,Z$ and $Y,Z$ are joint variables. The conditional mutual information can only be positive, or null if and only if $X \\indep Y | Z$.\nFinally, the information between more than two variables is called the interaction information. We define it for three variables $X,Y,Z$ and a conditioning set $U_i$ : $$\\begin{aligned} \\label{eq:cond_chain_rule} \\begin{split} \\operatorname{I}(X;Y;Z|{U_i}) \u0026= I(X;Y|{U_i}) - I(X;Y|{U_i}, Z)\\\\ \u0026= I(X;Z|{U_i}) - I(X;Z|{U_i}, Y)\\\\ \u0026= I(Y;Z|{U_i}) - I(Y;Z|{U_i}, X) \\end{split} \\end{aligned}$$\n Note that the three-point information can be negative when two variables are pairwise-independent but become dependent when conditioning on the third.   Unlike the other measures introduced so far, it can be both positive and negative. A positive interaction information indicates that the three variables share some common information. It is negative when there is more information when taking the three variables together than independently. To illustrate this property, we borrow the concept of V-structure from causal diagrams. Consider the 4 possible DAGs with 3 nodes and two edges, shown here :\n As Bayesian networks, the first three graphs encode the same conditional dependencies : $X \\notindep Y$, and $X \\indep Y | Z$. In terms of informations, $\\operatorname{I}(X;Y)  0$ and $\\operatorname{I}(X;Y|Z) = 0$. They all share some information, either due to a common cause or having a continuous \"flow\" of information, so $\\operatorname{I}(X;Y;Z)$ is also positive. Only the fourth graph on the right shows a different pattern : $X$ and $Y$ are marginally independent ($I(X;Y) = 0$), but become dependent when conditioning on $Z$ ($\\operatorname{I}(X;Y|Z)0$). This is the situation where we \"create\" information by looking at the interaction of the three variable, and the three-point information $\\operatorname{I}(X;Y;Z)$ is negative. It is in fact the basis of V-structures in Pearl’s causal graphs, and the signature of causality in constraint-based network inference methods.\nBibliography   Amrhein, Greenland \u0026 McShane (2019)  Amrhein, V., Greenland, S. \u0026 McShane, B. (2019). Scientists rise up against statistical significance. Nature, 567(7748). 305–307. https://doi.org/10.1038/d41586-019-00857-9    Cover \u0026 Thomas (2012)  Cover, T. \u0026 Thomas, J. (2012). Elements of information theory. John Wiley \u0026 Sons.    Kinney \u0026 Atwal (2014)  Kinney, J. \u0026 Atwal, G. (2014). Equitability, mutual information, and the maximal information coefficient. Proceedings of the National Academy of Sciences, 111(9). 3354–3359. https://doi.org/10.1073/pnas.1309933111    Leek, McShane, Gelman, Colquhoun, Nuijten \u0026 Goodman (2017)  Leek, J., McShane, B., Gelman, A., Colquhoun, D., Nuijten, M. \u0026 Goodman, S. (2017). Five ways to fix statistics. Nature, 551(7682). 557–559. https://doi.org/10.1038/d41586-017-07522-z    Reshef, Reshef, Finucane, Grossman, McVean, Turnbaugh, Lander, Mitzenmacher \u0026 Sabeti (2011)  Reshef, D., Reshef, Y., Finucane, H., Grossman, S., McVean, G., Turnbaugh, P., Lander, E., Mitzenmacher, M. \u0026 Sabeti, P. (2011). Detecting Novel Associations in Large Datasets. Science (New York, N.y.), 334(6062). 1518–1524. https://doi.org/10.1126/science.1205438    Shannon (1948)  Shannon, C. (1948). A mathematical theory of communication. The Bell system technical journal, 27(3). 379–423.     ","wordCount":"2982","inLanguage":"en","datePublished":"2021-11-10T00:00:00Z","dateModified":"2021-11-10T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://vcabe.li/posts/intro_to_info_theory/"},"publisher":{"@type":"Organization","name":"VCabeli","logo":{"@type":"ImageObject","url":"https://vcabe.li/favicon.ico"}}}</script>
</head>
<body id=top>
<script>window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<img .header-img src=/media/banners/Renoir_yeux.png alt=banner id=banner>
<nav class=nav>
<div class=logo>
<a href=https://vcabe.li accesskey=h title="VCabeli (Alt + H)">VCabeli</a>
<span class=logo-switches>
</span>
</div>
<ul id=menu>
<li>
<a href=https://vcabe.li/about/ title=About>
<span>About</span>
</a>
</li>
<li>
<a href=https://vcabe.li/search/ title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
Introduction to information theoretic measures
</h1>
<div class=post-meta><span title="2021-11-10 00:00:00 +0000 UTC">November 10, 2021</span>
</div>
</header>
<div class=post-content><p>How do you measure the dependency between two variables ?
Formally speaking, $X$ and $Y$ are dependent, <em>i.e.</em> they share some information, if their joint distribution is not the same as the product of their marginals : $X \indep Y \Leftrightarrow p(X,Y) \not= p(X)p(Y)$.
Only a few measures can measure dependency according to this strict equivalence : <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson correlation</a> of course detects only linear relationships, it is generalized by the <a href=https://en.wikipedia.org/wiki/Distance_correlation>distance correlation</a> for all distributions with finite first moments.
We can also measure the covariance in a kernel space, which gives the powerful <a href=http://alex.smola.org/teaching/iconip2006/iconip_4.pdf>Hilbert Schmidt Independence Criterion</a>, or we can even use supervised learning and feature importance measures such as the <a href=https://github.com/slundberg/shap>SHAP scores</a> to get a sense of what is predictive of $Y$.
But each of these methods come with certain caveats and limitations, and may not be defined for the general case, without any assumption on $X$, $Y$, or $p(X,Y)$.</p>
<figure>
<img loading=lazy src=/media/dependencies.png> <figcaption>
Examples of dependencies : $X \indep Y \Leftrightarrow p(X,Y) \not= p(X)p(Y)$
</figcaption>
</figure>
<p>The mutual information is a measure of the dependency between two random
variables in the most general sense. It is agnostic to the nature of the
random variables and of their relationship : noted
$\operatorname{I}(X;Y)$, it simply defines the quantity of information
one knows about $X$ by knowing $Y$, and vice-versa. It was introduced by
Claude Shannon in 1948 to characterize communication channels
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#shannon_mathematical_1948><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Claude Elwood"><span itemprop=familyName>Shannon</span></span>, <span itemprop=datePublished>1948</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Shannon</span>, 
<meta itemprop=givenName content="Claude Elwood">
C.</span>
 
(<span itemprop=datePublished>1948</span>).
 <span itemprop=name>A mathematical theory of communication</span>.<i>
<span itemprop=about>The Bell system technical journal</span>, 27(3)</i>. <span itemprop=pagination>379–423</span>.</span>
</span></span>)</span>
but it has found success in a wide range of
applications since. It is still seen by many as the ideal dependency
measure, although it is difficult to use in practice.
This post introduces the necessary concepts to define the mutual information, largely inspired by
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#cover_elements_2012><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Thomas M."><span itemprop=familyName>Cover</span></span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Joy A."><span itemprop=familyName>Thomas</span></span>, <span itemprop=datePublished>2012</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Book data-type=book><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Cover</span>, 
<meta itemprop=givenName content="Thomas M.">
T.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Thomas</span>, 
<meta itemprop=givenName content="Joy A.">
J.</span> 
(<span itemprop=datePublished>2012</span>).
 <span itemprop=name>
<i>Elements of information theory</i></span>.
 
<span itemprop=publisher itemtype=http://schema.org/Organization itemscope>
<span itemprop=name>John Wiley & Sons</span></span>.</span>
</span></span>)</span>
, and hints as to how it can be applied to causal graph inference.
The difficult process of estimating these values from sampled data will be discussed in a later post.</p>
<h2 id=sec:mutual_info_definition>Definitions<a hidden class=anchor aria-hidden=true href=#sec:mutual_info_definition>#</a></h2>
<h3 id=entropy-and-mutual-information>Entropy and mutual information<a hidden class=anchor aria-hidden=true href=#entropy-and-mutual-information>#</a></h3>
<p>Before giving the definition of the mutual information between two
random variables, it is a good idea to start with the self-information
contained in a single variable, called the entropy. Let $X$ be a
discrete random variable with possible values in $\mathcal{X}$ and a
probability mass function $p(x) = Pr{X = x}, x \in \mathcal{X}$. The
entropy $\operatorname{H}(X)$ of $X$ is defined by :</p>
<p>$$\operatorname{H}(X) = - \sum_{x \in \mathcal{X}}p(x)\log p(x)$$</p>
<p>It is expressed in <em>bits</em> with a logarithm to the base 2, or <em>nats</em> with
the base $e$, and it denotes the average information or "surprise"
that is carried by a random variable. To get a better understanding of
this concept, consider a game of chance where you try to predict the
result of a coin flip. If the coin is balanced, each realisation has the
same "surprise" as both outcomes, heads or tails, are equiprobable.
When the coin is biased towards one outcome with probability $p$, the
average surprise decreases as $p$ approaches $0$ or $1$ at which point
it becomes null and your willingness to bet on the outcome increases.
Note that the entropy characterises the distribution of a random
variable and not the surprise of one realisation.</p>
<p>This definition can be naturally extended to a pair of random variables
$X$ and $Y$ (which can be thought of a single two-dimensional variable),
giving the <em>joint entropy</em> :</p>
<p>$$\operatorname{H}(X,Y) = - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}}p(x,y)\log p(x,y)$$</p>
<p>We can also define the <em>conditional entropy</em>, i.e. the expected
"surprise" of the conditional distribution of a variable $Y$ given $X$
:</p>
<p>$$\operatorname{H}(Y|X) = - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y){\log\frac{p(x,y)}{p(x)}}$$</p>
<p>One desirable property of these information-theoretic values is that
they can be combined in an intuitive manner with the "chain rule" :</p>
<p>$$\begin{aligned}
\operatorname{H}(X,Y) = \operatorname{H}(X) + \operatorname{H}(Y|X)\\
\operatorname{H}(Y|X) = \operatorname{H}(X,Y) - \operatorname{H}(X)
\end{aligned}$$</p>
<p>Indeed, it comes easily to think of the joint entropy of $X$ and $Y$ as
the sum of the information carried by $X$ plus the residual information
of $Y$ after "removing" the knowledge of $X$, as some information may
be redundant between the two.</p>
<p>So far we have only defined the entropy of discrete variables, but our
ideal dependency measure should also include continuous or mixed (part
discrete, part continuous) distributions which are present in real-life
datasets. Continuous variables are defined by a probability density
function $f(x)$ instead of a mass function, which was naturally
considered by Shannon to be equivalent in the definition for their
entropy. There are however subtle differences with the discrete
counterpart, which is why this value is called the <em>differential</em>
entropy and is noted $h(X)$ :</p>
<p>$$h(X) = -\int_{S} f(x)\log f(x),dx ,$$</p>
<p>with $S$ as the support set of $X$ where $f(x) > 0$.</p>
<p>The source of the differences between differential and discrete
entropies becomes evident with our previous example of predicting the
value of a random variable : what is the surprise of a realisation of
$X$ given that there is an infinite number of possible values in any
continuous interval, each with a probability that tends to $0$ ?</p>
<p>It is better to think of the differential entropy as an estimate of the
effective volume that a random variable occupies : a very focused
distribution will have a low entropy as opposed to a more dispersed
distribution with more <em>room</em> for randomness hence higher entropy.
Formally, the differential entropy is the logarithm of the length of the
smallest interval that contains most of the probability
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#cover_elements_2012><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Thomas M."><span itemprop=familyName>Cover</span></span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Joy A."><span itemprop=familyName>Thomas</span></span>, <span itemprop=datePublished>2012</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Book data-type=book><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Cover</span>, 
<meta itemprop=givenName content="Thomas M.">
T.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Thomas</span>, 
<meta itemprop=givenName content="Joy A.">
J.</span> 
(<span itemprop=datePublished>2012</span>).
 <span itemprop=name>
<i>Elements of information theory</i></span>.
 
<span itemprop=publisher itemtype=http://schema.org/Organization itemscope>
<span itemprop=name>John Wiley & Sons</span></span>.</span>
</span></span>)</span>
: for example, the differential entropy of a
uniform distribution on the interval $[0,a]$ is $\log(a)$.</p>
<p>Although the differences between entropy and differential entropy go
beyond the scope of this thesis, it is still interesting to mention them
to understand why discretization has been so popular for so long to
estimate the entropy or mutual information between samples of continuous
variables. Even though, as we will see in future posts, one must be
careful to discretize a continuous variable without introducing bias.</p>
<p>Entropy and differential entropy behave similarly and are
interchangeable in the settings that interest us, namely for the joint
and conditional differential entropy, the chain rule and especially the
relationship to mutual information. For the rest of this post, the
probability mass function $p(x)$ can be replaced by the density function
$f(x)$, and $H(X)$ by $h(X)$ to switch from discrete to continuous random
variables. The special case of mixed variables with both continuous and
discrete parts will be reviewed later!</p>
<p>We have established that information theory gives us the necessary tools
to define the entropy of a random variable (which can be
multidimensional or conditional), i.e. the amount of information needed
on average to describe it. Next we show how we can also formalize how
much information two variables have in common, giving a measure of how
(in)dependent they are. For this we need to introduce the
Kullback-Leibler divergence, also called the <em>relative entropy</em>. The
relative entropy $D_\text{KL}(p \parallel q)$ is a measure of the
difference between two distributions $p$ and $q$ defined on the same
space $\mathcal{X}$:</p>
<p>$$
D_\text{KL}(p\parallel q) = \sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)}
$$</p>
<p>Also called the Kullback-Leibler distance (although not a distance in
the usual sense as it is not symmetric), it can thought of as the cost
of describing the distribution $p$ when using $q$ as a reference model.
As such, it is null if and only if $p = q$ and it is always
non-negative.</p>
<p>Now we get back to our original goal which is to define the dependency
between two random variables $X$ and $Y$. In the most general sense, $X$
and $Y$ are independent if the realization of one does not affect the
probability distribution of the other. Formally put, two random
variables $X$ and $Y$ with marginal distributions $p(x)$, $p(y)$ and a
joint distribution $p(x,y)$ are independent if and only if
$p(x,y) = p(x)p(y)$. If these two quantities differ, some information is
being shared between $X$ and $Y$ : knowing about $X$ tells us
<em>something</em> about $Y$ and vice versa.</p>
<p>Using the measure of divergence we just introduced, it becomes natural
to think of the divergence between the joint distribution and the
product of marginals as a direct measure of the dependency. It is in
fact the definition of the mutual information $\operatorname{I}(X;Y)$ :
$$\begin{aligned}
\operatorname{I}(X;Y) &= D_\text{KL}\left(p(x,y) \parallel p(x) p(y)\right)\\
&= \sum_{y \in Y} \sum_{x \in X} { p(x,y) \log{ \left(\frac{p(x,y)}{p(x),p(y)} \right) }}
\end{aligned}$$</p>
<p>In agreement with our interpretation of the relative entropy, assuming
the independence model where $p(x,y)=p(x)p(y)$ the mutual information is
literally <em>the extra bits</em> that are required to encode the interaction
between $X$ and $Y$. It is always positive, or null if and only if $X$
and $Y$ are independent.</p>
<figure class=align-center>
<img loading=lazy src=/media/MI_venn.svg#center width=50%>
</figure>
<p>Just like the other measures, it fits naturally in the "chain rule"
and can be expressed intuitively in terms of entropies.</p>
<p>$$\begin{aligned} \label{eq:I_H_chainrule}
\operatorname{I}(X;Y) &= \operatorname{H}(X) - \operatorname{H}(X|Y) \\
&= \operatorname{H}(Y) - \operatorname{H}(Y|X) \\
&= \operatorname{H}(X) + \operatorname{H}(Y) - \operatorname{H}(X,Y)
\end{aligned}$$</p>
<p>What makes the mutual information a particularly interesting measure is
its unique blend of desirable properties.</p>
<p>First, it satisfies the Data Processing Inequality (DPI) which states
that one cannot increase the information content of a signal by
processing it. Formally, if $n$ variables form a Markov chain
$X_0 \rightarrow X_1 \rightarrow \cdots \rightarrow X_n$, then
$\operatorname{I}(X_i;X_j) \ge \operatorname{I}(X_i; X_k)$ with
$i &lt; j &lt; k$.</p>
<p>In relation to the first point, mutual information is also widely
considered to be equally sensitive to all types of relationships. This
concept was termed "equitablity" by Reshef et al.
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#reshef_detecting_2011><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="David N."><span itemprop=familyName>Reshef</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Yakir A."><span itemprop=familyName>Reshef</span></span>
<em>& al.</em>, <span itemprop=datePublished>2011</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Reshef</span>, 
<meta itemprop=givenName content="David N.">
D.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Reshef</span>, 
<meta itemprop=givenName content="Yakir A.">
Y.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Finucane</span>, 
<meta itemprop=givenName content="Hilary K.">
H.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Grossman</span>, 
<meta itemprop=givenName content="Sharon R.">
S.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>McVean</span>, 
<meta itemprop=givenName content="Gilean">
G.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Turnbaugh</span>, 
<meta itemprop=givenName content="Peter J.">
P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lander</span>, 
<meta itemprop=givenName content="Eric S.">
E.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Mitzenmacher</span>, 
<meta itemprop=givenName content="Michael">
M.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Sabeti</span>, 
<meta itemprop=givenName content="Pardis C.">
P.</span>
 
(<span itemprop=datePublished>2011</span>).
 <span itemprop=name>Detecting Novel Associations in Large Datasets</span>.<i>
<span itemprop=about>Science (New York, N.y.)</span>, 334(6062)</i>. <span itemprop=pagination>1518–1524</span>.
<a href=https://doi.org/10.1126/science.1205438 itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.1126/science.1205438</a></span>
</span></span>)</span>
(although in a flawed form) and was then
formally investigated by Kinney et Atwal
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#kinney_equitability_2014><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Justin B."><span itemprop=familyName>Kinney</span></span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Gurinder S."><span itemprop=familyName>Atwal</span></span>, <span itemprop=datePublished>2014</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kinney</span>, 
<meta itemprop=givenName content="Justin B.">
J.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Atwal</span>, 
<meta itemprop=givenName content="Gurinder S.">
G.</span>
 
(<span itemprop=datePublished>2014</span>).
 <span itemprop=name>Equitability, mutual information, and the maximal information coefficient</span>.<i>
<span itemprop=about>Proceedings of the National Academy of Sciences</span>, 111(9)</i>. <span itemprop=pagination>3354–3359</span>.
<a href=https://doi.org/10.1073/pnas.1309933111 itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.1073/pnas.1309933111</a></span>
</span></span>)</span>
.
Kinney et Atwal&rsquo;s "Self-Equitability" is defined to characterize a
dependence measure $D[X;Y]$ if and only if it is symmetric between $X$
and $Y$, and : $$D[X;Y] = D[f(X);Y]$$ with $f$ any deterministic
function, $X \leftrightarrow f(X) \leftrightarrow Y$ forming a Markov
chain. Put roughly, an equitable measure means that one can measure the
strength of the signal (as compared to the noise) between $Y$ and $f(X)$
without having to know the underlying function $f$.</p>
<p>Not only is it invariant to invertible transformations of $X$ and $Y$,
it is also invariant under any monotonic (i.e. rank preserving)
transformations.</p>
<p>Put together, these three properties make the mutual information particularly interesting for general case causal discovery. True causal discovery should make no assumption of the natural mechanisms that produced the observed data, whether on the scale of the unit or shape of the joint distributions. As a simple example, a case can be made to measure the human weight in a logarithmic scale instead of a linear one : for most health related aspects, a difference of 30 kilograms is much more significant between 60 and 90kgs than between 120 and 150kgs.
In an experimental context, we can think of the causal diagram as the
natural laws that have produced the observations, which are themselves a
function of the "observing" process. The self-equitability property
and invariance under transformation go some way towards freeing
ourselves from this observation process and our own biases.</p>
<p>Finally, as will be discussed later, these properties hold for any type
of variable $X$ and $Y$, be it continuous, discrete (ordinal or not), or
a mixture of discrete and continuous parts.</p>
<p>A notable disadvantage of mutual information compared to other measures
is that the <em>bit</em>, unit of information, is not commonly understood, and
the fact that it is unbounded upwards. One usually cannot easily derive
a p-value from a mutual information estimation on sampled data, which
makes it harder to communicate (although the benefits of standardising
the p-value have been called into question
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#amrhein_scientists_2019><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Valentin"><span itemprop=familyName>Amrhein</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Sander"><span itemprop=familyName>Greenland</span></span>
<em>& al.</em>, <span itemprop=datePublished>2019</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Amrhein</span>, 
<meta itemprop=givenName content="Valentin">
V.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Greenland</span>, 
<meta itemprop=givenName content="Sander">
S.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>McShane</span>, 
<meta itemprop=givenName content="Blake">
B.</span>
 
(<span itemprop=datePublished>2019</span>).
 <span itemprop=name>Scientists rise up against statistical significance</span>.<i>
<span itemprop=about>Nature</span>, 567(7748)</i>. <span itemprop=pagination>305–307</span>.
<a href=https://doi.org/10.1038/d41586-019-00857-9 itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.1038/d41586-019-00857-9</a></span>
</span></span>; <span class=hugo-cite-group>
<a href=#leek_five_2017><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Jeff"><span itemprop=familyName>Leek</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Blakeley B."><span itemprop=familyName>McShane</span></span>
<em>& al.</em>, <span itemprop=datePublished>2017</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Leek</span>, 
<meta itemprop=givenName content="Jeff">
J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>McShane</span>, 
<meta itemprop=givenName content="Blakeley B.">
B.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Gelman</span>, 
<meta itemprop=givenName content="Andrew">
A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Colquhoun</span>, 
<meta itemprop=givenName content="David">
D.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Nuijten</span>, 
<meta itemprop=givenName content="Michèle B.">
M.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Goodman</span>, 
<meta itemprop=givenName content="Steven N.">
S.</span>
 
(<span itemprop=datePublished>2017</span>).
 <span itemprop=name>Five ways to fix statistics</span>.<i>
<span itemprop=about>Nature</span>, 551(7682)</i>. <span itemprop=pagination>557–559</span>.
<a href=https://doi.org/10.1038/d41586-017-07522-z itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.1038/d41586-017-07522-z</a></span>
</span></span>)</span>
). Different ideas to
evaluate significativity will be presented in a future post, but for now
we are only interested in the "oracle" value, when the sample size $N$
tends to infinity and the strict equivalence
$X \indep Y \leftrightarrow \operatorname{I}(X;Y) = 0$
holds.</p>
<p>In the discrete case, its value is actually familiar as it is in fact
the G-statistic multiplied by a factor of $N$. With $O_i$ the number of
observations in a contingency table between two categorical variables
$X$ and $Y$, with $i$ joint levels, and $E_i$ the expected counts under
the null hypothesis
$X \indep Y$, the G-statistic
is defined as : $$G = 2 \sum_i O_i \log\left(\frac{O_i}{E_i}\right)$$</p>
<p>Recall the definition of the KL divergence (Eq
<a href=#eq:KL_def>[eq:KL_def]</a>{reference-type=&ldquo;ref&rdquo;
reference=&ldquo;eq:KL_def&rdquo;}), the same formula as above except for the
frequencies (noted $o_i$ and $e_i$) instead of the <em>counts</em> $O_i$,
$E_i$. Using the frequencies, the G-statistic becomes :
$$\begin{aligned}
\begin{split}
G &= 2N \sum_i o_i \log\left(\frac{o_i}{e_i}\right)\\
&= 2N \cdot D_\text{KL}(o\parallel e)\\
&= 2N \cdot \operatorname{I}(X;Y)
\end{split}
\end{aligned}$$ It also follows that the mutual information is related
to the $\chi^2$ test, as it is itself a second-order Taylor
approximation of the G-statistic.</p>
<p>For two continuous variables, it may be harder to get a good intuition
of what the mutual information measures. A first property that may seem
odd is that if $X$ is continuous and $Y=X$, then
$\operatorname{I}(X;Y) = \infty$. This looks as though it contradicts
the chain rule (Eq
<a href=#eq:I_H_chainrule>[eq:I_H_chainrule]</a>{reference-type=&ldquo;ref&rdquo;
reference=&ldquo;eq:I_H_chainrule&rdquo;}), since it implies that
$\operatorname{H}(X)-\operatorname{H}(X|X) = \infty$. It is in fact one
of the differences between entropy $\operatorname{H}$ and differential
entropy $h$ which is unbounded in the case of a singularity
$h(X|X) = - \infty$, unlike $\operatorname{H}$ which is always finite.
Thankfully, this theoretical property does not bleed into the real world
for several reasons : first, even continuous distributions have always
finite differential entropies since we actually treat real numbers up to
a finite number of significant digits. Second, much like a correlation
coefficient on observed data never reaches $1$, we should not ever need
to estimate $\operatorname{I}(X;Y)$ where $X=Y$. The analytical value of
the mutual information on a bivariate Gaussian with correlation
coefficient $\rho$ is actually known :</p>
<p>$$\label{eq:mi_gaussian}
\operatorname{I}(X;Y) = -\frac{1}{2}\log(1 - \rho^2)$$</p>
<figure class=align-center>
<img loading=lazy src=/media/MI_gaussian.svg#center width=40%>
</figure>
<p>This equivalence is useful for practitioners who are unfamiliar with
mutual information and wish to translate it to the better known
dependence measure : thanks the self-equitability property if one could
transform two variables to a bivariate Gaussian distribution preserving
the signal-to-noise ratio, using the analytic result one could then get the corresponding correlation coefficient.</p>
<h3 id=conditional-information-and-interaction-information>Conditional information and interaction information<a hidden class=anchor aria-hidden=true href=#conditional-information-and-interaction-information>#</a></h3>
<p>Information theory also allows us to measure the conditional dependence
between two variables $X$ and $Y$ given a third, $Z$. The conditional
mutual information is defined as the expected value of the mutual
information between $X$ and $Y$ given a third variable $Z$:
$$
\operatorname{I}(X;Y|Z) = \mathbb{E}_z\left[D_\text{KL}\left(p((x,y)|z) \parallel p(x|z) p(y|z)\right)\right]
$$</p>
<p>It is symmetrically decomposable into two-points mutual informations :
$$\begin{aligned}
\begin{split}
\operatorname{I}(X;Y|Z) &= \operatorname{I}(X;Y,Z) - \operatorname{I}(X;Z)\\
&= \operatorname{I}(Y;X,Z) - \operatorname{I}(Y;Z)
\end{split}
\end{aligned}$$</p>
<p>where $X,Z$ and $Y,Z$ are joint variables. The conditional mutual
information can only be positive, or null if and only if
$X \indep Y | Z$.</p>
<p>Finally, the information between more than two variables is called the
interaction information. We define it for three variables $X,Y,Z$ and a
conditioning set $U_i$ : $$\begin{aligned} \label{eq:cond_chain_rule}
\begin{split}
\operatorname{I}(X;Y;Z|{U_i}) &= I(X;Y|{U_i}) - I(X;Y|{U_i}, Z)\\
&= I(X;Z|{U_i}) - I(X;Z|{U_i}, Y)\\
&= I(Y;Z|{U_i}) - I(Y;Z|{U_i}, X)
\end{split}
\end{aligned}$$</p>
<figure class=align-center>
<img loading=lazy src=/media/MI_multi.svg#center width=60%> <figcaption>
Note that the three-point information can be negative when two variables are pairwise-independent but become dependent when conditioning on the third.
</figcaption>
</figure>
<p>Unlike the other measures introduced so far, it can be both positive and
negative. A positive interaction information indicates that the three
variables share some common information. It is negative when there is
more information when taking the three variables together than
independently. To illustrate this property, we borrow the concept of
V-structure from causal diagrams. Consider the 4 possible DAGs with 3
nodes and two edges, shown here :</p>
<figure class=align-center>
<img loading=lazy src=/media/DAGs_3.svg#center width=100%>
</figure>
<p>As Bayesian networks, the first three graphs encode the same conditional
dependencies :
$X \notindep Y$, and
$X \indep Y | Z$. In terms of
informations, $\operatorname{I}(X;Y) > 0$ and
$\operatorname{I}(X;Y|Z) = 0$. They all share some information, either
due to a common cause or having a continuous "flow" of information, so
$\operatorname{I}(X;Y;Z)$ is also positive. Only the fourth graph on the
right shows a different pattern : $X$ and $Y$ are marginally independent
($I(X;Y) = 0$), but become dependent when conditioning on $Z$
($\operatorname{I}(X;Y|Z)>0$). This is the situation where we "create"
information by looking at the interaction of the three variable, and the
three-point information $\operatorname{I}(X;Y;Z)$ is negative.
It is in fact the basis of V-structures in Pearl&rsquo;s causal graphs, and
the signature of causality in constraint-based network inference methods.</p>
<h2 id=bibliography class=unnumbered>Bibliography<a hidden class=anchor aria-hidden=true href=#bibliography>#</a></h2>
<section class=hugo-cite-bibliography>
<dl>
<div id=amrhein_scientists_2019>
<dt>
Amrhein, 
Greenland & McShane
(2019)</dt>
<dd>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Amrhein</span>, 
<meta itemprop=givenName content="Valentin">
V.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Greenland</span>, 
<meta itemprop=givenName content="Sander">
S.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>McShane</span>, 
<meta itemprop=givenName content="Blake">
B.</span>
 
(<span itemprop=datePublished>2019</span>).
 <span itemprop=name>Scientists rise up against statistical significance</span>.<i>
<span itemprop=about>Nature</span>, 567(7748)</i>. <span itemprop=pagination>305–307</span>.
<a href=https://doi.org/10.1038/d41586-019-00857-9 itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.1038/d41586-019-00857-9</a></span>
</dd>
</div>
<div id=cover_elements_2012>
<dt>
Cover & Thomas
(2012)</dt>
<dd>
<span itemscope itemtype=https://schema.org/Book data-type=book><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Cover</span>, 
<meta itemprop=givenName content="Thomas M.">
T.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Thomas</span>, 
<meta itemprop=givenName content="Joy A.">
J.</span> 
(<span itemprop=datePublished>2012</span>).
 <span itemprop=name>
<i>Elements of information theory</i></span>.
 
<span itemprop=publisher itemtype=http://schema.org/Organization itemscope>
<span itemprop=name>John Wiley & Sons</span></span>.</span>
</dd>
</div>
<div id=kinney_equitability_2014>
<dt>
Kinney & Atwal
(2014)</dt>
<dd>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kinney</span>, 
<meta itemprop=givenName content="Justin B.">
J.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Atwal</span>, 
<meta itemprop=givenName content="Gurinder S.">
G.</span>
 
(<span itemprop=datePublished>2014</span>).
 <span itemprop=name>Equitability, mutual information, and the maximal information coefficient</span>.<i>
<span itemprop=about>Proceedings of the National Academy of Sciences</span>, 111(9)</i>. <span itemprop=pagination>3354–3359</span>.
<a href=https://doi.org/10.1073/pnas.1309933111 itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.1073/pnas.1309933111</a></span>
</dd>
</div>
<div id=leek_five_2017>
<dt>
Leek, 
McShane, 
Gelman, 
Colquhoun, 
Nuijten & Goodman
(2017)</dt>
<dd>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Leek</span>, 
<meta itemprop=givenName content="Jeff">
J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>McShane</span>, 
<meta itemprop=givenName content="Blakeley B.">
B.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Gelman</span>, 
<meta itemprop=givenName content="Andrew">
A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Colquhoun</span>, 
<meta itemprop=givenName content="David">
D.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Nuijten</span>, 
<meta itemprop=givenName content="Michèle B.">
M.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Goodman</span>, 
<meta itemprop=givenName content="Steven N.">
S.</span>
 
(<span itemprop=datePublished>2017</span>).
 <span itemprop=name>Five ways to fix statistics</span>.<i>
<span itemprop=about>Nature</span>, 551(7682)</i>. <span itemprop=pagination>557–559</span>.
<a href=https://doi.org/10.1038/d41586-017-07522-z itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.1038/d41586-017-07522-z</a></span>
</dd>
</div>
<div id=reshef_detecting_2011>
<dt>
Reshef, 
Reshef, 
Finucane, 
Grossman, 
McVean, 
Turnbaugh, 
Lander, 
Mitzenmacher & Sabeti
(2011)</dt>
<dd>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Reshef</span>, 
<meta itemprop=givenName content="David N.">
D.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Reshef</span>, 
<meta itemprop=givenName content="Yakir A.">
Y.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Finucane</span>, 
<meta itemprop=givenName content="Hilary K.">
H.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Grossman</span>, 
<meta itemprop=givenName content="Sharon R.">
S.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>McVean</span>, 
<meta itemprop=givenName content="Gilean">
G.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Turnbaugh</span>, 
<meta itemprop=givenName content="Peter J.">
P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lander</span>, 
<meta itemprop=givenName content="Eric S.">
E.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Mitzenmacher</span>, 
<meta itemprop=givenName content="Michael">
M.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Sabeti</span>, 
<meta itemprop=givenName content="Pardis C.">
P.</span>
 
(<span itemprop=datePublished>2011</span>).
 <span itemprop=name>Detecting Novel Associations in Large Datasets</span>.<i>
<span itemprop=about>Science (New York, N.y.)</span>, 334(6062)</i>. <span itemprop=pagination>1518–1524</span>.
<a href=https://doi.org/10.1126/science.1205438 itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.1126/science.1205438</a></span>
</dd>
</div>
<div id=shannon_mathematical_1948>
<dt>
Shannon
(1948)</dt>
<dd>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Shannon</span>, 
<meta itemprop=givenName content="Claude Elwood">
C.</span>
 
(<span itemprop=datePublished>1948</span>).
 <span itemprop=name>A mathematical theory of communication</span>.<i>
<span itemprop=about>The Bell system technical journal</span>, 27(3)</i>. <span itemprop=pagination>379–423</span>.</span>
</dd>
</div>
</dl>
</section>
</div>
<footer class=post-footer>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2022 <a href=https://vcabe.li>VCabeli</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
</body>
</html>