
@article{choi_haemopedia_2019,
	title = {Haemopedia {RNA}-seq: a database of gene expression during haematopoiesis in mice and humans},
	volume = {47},
	issn = {0305-1048},
	shorttitle = {Haemopedia {RNA}-seq},
	url = {https://doi.org/10.1093/nar/gky1020},
	doi = {10.1093/nar/gky1020},
	abstract = {During haematopoiesis, haematopoietic stem cells differentiate into restricted potential progenitors before maturing into the many lineages required for oxygen transport, wound healing and immune response. We have updated Haemopedia, a database of gene-expression profiles from a broad spectrum of haematopoietic cells, to include RNA-seq gene-expression data from both mice and humans. The Haemopedia RNA-seq data set covers a wide range of lineages and progenitors, with 57 mouse blood cell types (flow sorted populations from healthy mice) and 12 human blood cell types. This data set has been made accessible for exploration and analysis, to researchers and clinicians with limited bioinformatics experience, on our online portal Haemosphere: https://www.haemosphere.org. Haemosphere also includes nine other publicly available high-quality data sets relevant to haematopoiesis. We have added the ability to compare gene expression across data sets and species by curating data sets with shared lineage designations or to view expression gene vs gene, with all plots available for download by the user.},
	number = {D1},
	urldate = {2021-11-03},
	journal = {Nucleic Acids Research},
	author = {Choi, Jarny and Baldwin, Tracey M and Wong, Mae and Bolden, Jessica E and Fairfax, Kirsten A and Lucas, Erin C and Cole, Rebecca and Biben, Christine and Morgan, Clare and Ramsay, Kerry A and Ng, Ashley P and Kauppi, Maria and Corcoran, Lynn M and Shi, Wei and Wilson, Nick and Wilson, Michael J and Alexander, Warren S and Hilton, Douglas J and de Graaf, Carolyn A},
	month = jan,
	year = {2019},
	pages = {D780--D785},
}

@techreport{wang_learning_2021,
	title = {Learning dynamics by computational integration of single cell genomic and lineage information},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2021.05.06.443026v1},
	abstract = {A goal of single cell genome-wide profiling is to reconstruct dynamic transitions during cell differentiation, disease onset, and drug response. Single cell assays have recently been integrated with lineage tracing, a set of methods that identify cells of common ancestry to establish bona fide dynamic relationships between cell states. These integrated methods have revealed unappreciated cell dynamics, but their analysis faces recurrent challenges arising from noisy, dispersed lineage data. Here, we develop coherent, sparse optimization (CoSpar) as a robust computational approach to infer cell dynamics from single-cell genomics integrated with lineage tracing. CoSpar is robust to severe down-sampling and dispersion of lineage data, which enables simpler, lower-cost experimental designs and requires less calibration. In datasets representing hematopoiesis, reprogramming, and directed differentiation, CoSpar identifies fate biases not previously detected, predicting transcription factors and receptors implicated in fate choice. Documentation and detailed examples for common experimental designs are available at https://cospar.readthedocs.io/.},
	language = {en},
	urldate = {2021-11-03},
	author = {Wang, Shou-Wen and Klein, Allon M.},
	month = may,
	year = {2021},
	doi = {10.1101/2021.05.06.443026},
	note = {Company: Cold Spring Harbor Laboratory
Distributor: Cold Spring Harbor Laboratory
Label: Cold Spring Harbor Laboratory
Section: New Results
Type: article},
	pages = {2021.05.06.443026},
}

@article{oburoglu_glucose_2014,
	title = {Glucose and glutamine metabolism regulate human hematopoietic stem cell lineage specification},
	volume = {15},
	number = {2},
	journal = {Cell stem cell},
	author = {Oburoglu, Leal and Tardito, Saverio and Fritz, Vanessa and De Barros, Stéphanie C. and Merida, Peggy and Craveiro, Marco and Mamede, João and Cretenet, Gaspard and Mongellaz, Cédric and An, Xiuli},
	year = {2014},
	note = {Publisher: Elsevier},
	pages = {169--184},
}

@article{nakamura-ishizu_hematopoietic_2020,
	title = {Hematopoietic stem cell metabolism during development and aging},
	volume = {54},
	number = {2},
	journal = {Developmental cell},
	author = {Nakamura-Ishizu, Ayako and Ito, Keisuke and Suda, Toshio},
	year = {2020},
	note = {Publisher: Elsevier},
	pages = {239--255},
}

@article{ito_hematopoietic_2018,
	title = {Hematopoietic stem cell fate through metabolic control},
	volume = {64},
	journal = {Experimental hematology},
	author = {Ito, Kyoko and Ito, Keisuke},
	year = {2018},
	note = {Publisher: Elsevier},
	pages = {1--11},
}

@article{desterke_inferring_2020,
	title = {Inferring {Gene} {Networks} in {Bone} {Marrow} {Hematopoietic} {Stem} {Cell}-{Supporting} {Stromal} {Niche} {Populations}},
	volume = {23},
	issn = {2589-0042},
	url = {https://www.sciencedirect.com/science/article/pii/S2589004220304077},
	doi = {10.1016/j.isci.2020.101222},
	abstract = {The cardinal property of bone marrow (BM) stromal cells is their capacity to contribute to hematopoietic stem cell (HSC) niches by providing mediators assisting HSC functions. In this study we first contrasted transcriptomes of stromal cells at different developmental stages and then included large number of HSC-supportive and non-supportive samples. Application of a combination of algorithms, comprising one identifying reliable paths and potential causative relationships in complex systems, revealed gene networks characteristic of the BM stromal HSC-supportive capacity and of defined niche populations of perivascular cells, osteoblasts, and mesenchymal stromal cells. Inclusion of single-cell transcriptomes enabled establishing for the perivascular cell subset a partially oriented graph of direct gene-to-gene interactions. As proof of concept we showed that R-spondin-2, expressed by the perivascular subset, synergized with Kit ligand to amplify ex vivo hematopoietic precursors. This study by identifying classifiers and hubs constitutes a resource to unravel candidate BM stromal mediators.},
	language = {en},
	number = {6},
	urldate = {2021-11-03},
	journal = {iScience},
	author = {Desterke, Christophe and Petit, Laurence and Sella, Nadir and Chevallier, Nathalie and Cabeli, Vincent and Coquelin, Laura and Durand, Charles and Oostendorp, Robert A. J. and Isambert, Hervé and Jaffredo, Thierry and Charbord, Pierre},
	month = jun,
	year = {2020},
	keywords = {Bioinformatics, Cell Biology, Stem Cells Research},
	pages = {101222},
}

@misc{noauthor_inferring_nodate,
	title = {Inferring {Gene} {Networks} in {Bone} {Marrow} {Hematopoietic} {Stem} {Cell}-{Supporting} {Stromal} {Niche} {Populations} - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S2589004220304077},
	urldate = {2021-11-03},
}

@article{huynh-thu_inferring_2010,
	title = {Inferring regulatory networks from expression data using tree-based methods},
	volume = {5},
	number = {9},
	journal = {PloS one},
	author = {Huynh-Thu, Vân Anh and Irrthum, Alexandre and Wehenkel, Louis and Geurts, Pierre},
	year = {2010},
	note = {Publisher: Public Library of Science San Francisco, USA},
	pages = {e12776},
}

@inproceedings{gretton_kernel_2007,
	title = {A kernel statistical test of independence.},
	volume = {20},
	booktitle = {Nips},
	publisher = {Citeseer},
	author = {Gretton, Arthur and Fukumizu, Kenji and Teo, Choon Hui and Song, Le and Schölkopf, Bernhard and Smola, Alexander J.},
	year = {2007},
	pages = {585--592},
}

@article{hoeffding_non-parametric_1948,
	title = {A non-parametric test of independence},
	journal = {The annals of mathematical statistics},
	author = {Hoeffding, Wassily},
	year = {1948},
	note = {Publisher: JSTOR},
	pages = {546--557},
}

@misc{noauthor_hoeffding_nodate,
	title = {Hoeffding: {A} non-parametric test of independence - {Google} {Scholar}},
	url = {https://scholar.google.com/scholar_lookup?author=W+Hoeffding&title=A+non-parametric+test+of+independence&publication_year=1948&journal=Ann+Math+Stat&volume=19&pages=546-557},
	urldate = {2021-11-03},
}

@misc{noauthor_non-parametric_nodate,
	title = {A {Non}-{Parametric} {Test} of {Independence} on {JSTOR}},
	url = {https://www.jstor.org/stable/2236021?casa_token=qe0O_jmy7eoAAAAA%3AiMLfScs3iL0OLlrtXOTblp-3uORcUOet9UNSqur9iTzCAp9X1GFcve0DcMyf7ddpehu0W8d9eBKjb0fGV0JQFt3SrZboVBKhYJaq6hVacG5dDuA34w&seq=1#metadata_info_tab_contents},
	urldate = {2021-11-03},
}

@article{ramsey_adjacency-faithfulness_2012,
	title = {Adjacency-faithfulness and conservative causal inference},
	journal = {arXiv preprint arXiv:1206.6843},
	author = {Ramsey, Joseph and Zhang, Jiji and Spirtes, Peter L.},
	year = {2012},
}

@article{zheng_dags_2018,
	title = {Dags with no tears: {Continuous} optimization for structure learning},
	shorttitle = {Dags with no tears},
	journal = {arXiv preprint arXiv:1803.01422},
	author = {Zheng, Xun and Aragam, Bryon and Ravikumar, Pradeep and Xing, Eric P.},
	year = {2018},
}

@article{verma_equivalence_1991,
	title = {Equivalence and synthesis of causal models},
	author = {Verma, Thomas and Pearl, Judea},
	year = {1991},
	note = {Publisher: UCLA, Computer Science Department Los Angeles, CA},
}

@phdthesis{peters_restricted_2012,
	type = {{PhD} {Thesis}},
	title = {Restricted structural equation models for causal inference},
	school = {ETH Zurich},
	author = {Peters, Jonas Martin},
	year = {2012},
}

@article{heckerman_likelihoods_2021,
	title = {Likelihoods and parameter priors for {Bayesian} networks},
	journal = {arXiv preprint arXiv:2105.06241},
	author = {Heckerman, David and Geiger, Dan},
	year = {2021},
}

@article{heckerman_bayesian_1999,
	title = {A {Bayesian} approach to causal discovery},
	volume = {19},
	journal = {Computation, causation, and discovery},
	author = {Heckerman, David and Meek, Christopher and Cooper, Gregory},
	year = {1999},
	pages = {141--166},
}

@incollection{geiger_learning_1994,
	title = {Learning gaussian networks},
	booktitle = {Uncertainty {Proceedings} 1994},
	publisher = {Elsevier},
	author = {Geiger, Dan and Heckerman, David},
	year = {1994},
	pages = {235--243},
}

@article{chickering_optimal_2002,
	title = {Optimal structure identification with greedy search},
	volume = {3},
	number = {Nov},
	journal = {Journal of machine learning research},
	author = {Chickering, David Maxwell},
	year = {2002},
	pages = {507--554},
}

@article{martens_instrumental_2006,
	title = {Instrumental variables: application and limitations},
	shorttitle = {Instrumental variables},
	journal = {Epidemiology},
	author = {Martens, Edwin P. and Pestman, Wiebe R. and de Boer, Anthonius and Belitser, Svetlana V. and Klungel, Olaf H.},
	year = {2006},
	note = {Publisher: JSTOR},
	pages = {260--267},
}

@article{heckman_randomization_1992,
	title = {Randomization and social policy evaluation},
	volume = {1},
	journal = {Evaluating welfare and training programs},
	author = {Heckman, James J.},
	year = {1992},
	pages = {201--30},
}

@article{rubin_design_2007,
	title = {The design versus the analysis of observational studies for causal effects: parallels with the design of randomized trials},
	volume = {26},
	issn = {1097-0258},
	shorttitle = {The design versus the analysis of observational studies for causal effects},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.2739},
	doi = {10.1002/sim.2739},
	abstract = {For estimating causal effects of treatments, randomized experiments are generally considered the gold standard. Nevertheless, they are often infeasible to conduct for a variety of reasons, such as ethical concerns, excessive expense, or timeliness. Consequently, much of our knowledge of causal effects must come from non-randomized observational studies. This article will advocate the position that observational studies can and should be designed to approximate randomized experiments as closely as possible. In particular, observational studies should be designed using only background information to create subgroups of similar treated and control units, where ‘similar’ here refers to their distributions of background variables. Of great importance, this activity should be conducted without any access to any outcome data, thereby assuring the objectivity of the design. In many situations, this objective creation of subgroups of similar treated and control units, which are balanced with respect to covariates, can be accomplished using propensity score methods. The theoretical perspective underlying this position will be presented followed by a particular application in the context of the US tobacco litigation. This application uses propensity score methods to create subgroups of treated units (male current smokers) and control units (male never smokers) who are at least as similar with respect to their distributions of observed background characteristics as if they had been randomized. The collection of these subgroups then ‘approximate’ a randomized block experiment with respect to the observed covariates. Copyright © 2006 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {1},
	urldate = {2021-11-02},
	journal = {Statistics in Medicine},
	author = {Rubin, Donald B.},
	year = {2007},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.2739},
	keywords = {Rubin causal model, assignment mechanism, causal inference, objective design, propensity scores, tobacco litigation},
	pages = {20--36},
}

@article{rosenbaum_central_1983,
	title = {The central role of the propensity score in observational studies for causal effects},
	volume = {70},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/70.1.41},
	doi = {10.1093/biomet/70.1.41},
	abstract = {The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a two- dimensional plot.},
	number = {1},
	urldate = {2021-11-02},
	journal = {Biometrika},
	author = {ROSENBAUM, PAUL R. and RUBIN, DONALD B.},
	month = apr,
	year = {1983},
	pages = {41--55},
}

@article{price_principal_2006,
	title = {Principal components analysis corrects for stratification in genome-wide association studies},
	volume = {38},
	copyright = {2006 Nature Publishing Group},
	issn = {1546-1718},
	url = {https://www.nature.com/articles/ng1847},
	doi = {10.1038/ng1847},
	abstract = {Population stratification—allele frequency differences between cases and controls due to systematic ancestry differences—can cause spurious associations in disease studies. We describe a method that enables explicit detection and correction of population stratification on a genome-wide scale. Our method uses principal components analysis to explicitly model ancestry differences between cases and controls. The resulting correction is specific to a candidate marker's variation in frequency across ancestral populations, minimizing spurious associations while maximizing power to detect true associations. Our simple, efficient approach can easily be applied to disease studies with hundreds of thousands of markers.},
	language = {en},
	number = {8},
	urldate = {2021-11-02},
	journal = {Nature Genetics},
	author = {Price, Alkes L. and Patterson, Nick J. and Plenge, Robert M. and Weinblatt, Michael E. and Shadick, Nancy A. and Reich, David},
	month = aug,
	year = {2006},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 8
Primary\_atype: Research
Publisher: Nature Publishing Group},
	keywords = {Agriculture, Animal Genetics and Genomics, Biomedicine, Cancer Research, Gene Function, Human Genetics, general},
	pages = {904--909},
}

@misc{mackenzie_causal_2020,
	title = {Causal {Analysis} in {Theory} and {Practice} » {Race}, {COVID} {Mortality}, and {Simpson}’s {Paradox} (by {Dana} {Mackenzie})},
	url = {http://causality.cs.ucla.edu/blog/index.php/2020/07/06/race-covid-mortality-and-simpsons-paradox-by-dana-mackenzie/},
	urldate = {2020-08-31},
	author = {Mackenzie, Dana},
	month = jul,
	year = {2020},
	keywords = {manuscrit material},
}

@article{matthews_storks_2000,
	title = {Storks deliver babies (p= 0.008)},
	volume = {22},
	number = {2},
	journal = {Teaching Statistics},
	author = {Matthews, Robert},
	year = {2000},
	note = {Publisher: Wiley Online Library},
	pages = {36--38},
}

@inproceedings{leray_bayesian_2005,
	title = {Bayesian network structural learning and incomplete data},
	booktitle = {Proceedings of the {International} and {Interdisciplinary} {Conference} on {Adaptive} {Knowledge} {Representation} and {Reasoning} ({AKRR} 2005), {Espoo}, {Finland}},
	author = {Leray, Philippe and François, Olivier},
	year = {2005},
	pages = {33--40},
}

@inproceedings{leray_bayesian_2005-1,
	title = {Bayesian network structural learning and incomplete data},
	booktitle = {Proceedings of the {International} and {Interdisciplinary} {Conference} on {Adaptive} {Knowledge} {Representation} and {Reasoning} ({AKRR} 2005), {Espoo}, {Finland}},
	author = {Leray, Philippe and François, Olivier},
	year = {2005},
	pages = {33--40},
}

@misc{noauthor_bayesian_nodate,
	title = {‪{Bayesian} network structural learning and incomplete data‬},
	url = {https://scholar.google.fr/citations?view_op=view_citation&hl=en&user=je4uPl0AAAAJ&citation_for_view=je4uPl0AAAAJ:2osOgNQ5qMEC},
	abstract = {‪P Leray, O François‬, ‪Proceedings of the International and Interdisciplinary Conference on …, 2005‬ - ‪Cited by 39‬},
	urldate = {2021-10-30},
}

@inproceedings{dash_robust_2003,
	title = {Robust {Independence} {Testing} for {Constraint}-{Based} {Learning} of {Causal} {Structure}.},
	volume = {3},
	booktitle = {{UAI}},
	author = {Dash, Denver and Druzdzel, Marek J.},
	year = {2003},
	pages = {167--174},
}

@article{leray_feature_1999,
	title = {Feature {Selection} {With} {Neural} {Networks}},
	volume = {26},
	issn = {1349-6964},
	url = {https://doi.org/10.2333/bhmk.26.145},
	doi = {10.2333/bhmk.26.145},
	abstract = {The observed features of a given phenomenon are not all equally informative: some may be noisy, others correlated or irrelevant. The purpose of feature selection is to select a set of features pertinent to a given task. This is a complex process, but it is an important issue in many fields. In neural networks, feature selection has been studied for the last ten years, using conventional and original methods. This paper is a review of neural network approaches to feature selection. We first briefly introduce baseline statistical methods used in regression and classification. We then describe families of methods which have been developed specifically for neural networks. Representative methods are then compared on different test problems.},
	language = {en},
	number = {1},
	urldate = {2021-10-30},
	journal = {Behaviormetrika},
	author = {Leray, Philippe and Gallinari, Patrick},
	month = jan,
	year = {1999},
	pages = {145--166},
}

@article{mohan_graphical_2013,
	title = {Graphical {Models} for {Inference} with {Missing} {Data}},
	journal = {Advances in Neural Information Processing Systems 26},
	author = {Mohan, Karthika and Pearl, Judea and Tian, Jin},
	year = {2013},
	note = {Publisher: Citeseer},
	pages = {1277--1285},
}

@article{strobl_fast_2017,
	title = {Fast {Causal} {Inference} with {Non}-{Random} {Missingness} by {Test}-{Wise} {Deletion}},
	url = {http://arxiv.org/abs/1705.09031},
	abstract = {Many real datasets contain values missing not at random (MNAR). In this scenario, investigators often perform list-wise deletion, or delete samples with any missing values, before applying causal discovery algorithms. List-wise deletion is a sound and general strategy when paired with algorithms such as FCI and RFCI, but the deletion procedure also eliminates otherwise good samples that contain only a few missing values. In this report, we show that we can more efficiently utilize the observed values with test-wise deletion while still maintaining algorithmic soundness. Here, test-wise deletion refers to the process of list-wise deleting samples only among the variables required for each conditional independence (CI) test used in constraint-based searches. Test-wise deletion therefore often saves more samples than list-wise deletion for each CI test, especially when we have a sparse underlying graph. Our theoretical results show that test-wise deletion is sound under the justifiable assumption that none of the missingness mechanisms causally affect each other in the underlying causal graph. We also find that FCI and RFCI with test-wise deletion outperform their list-wise deletion and imputation counterparts on average when MNAR holds in both synthetic and real data.},
	urldate = {2021-10-30},
	journal = {arXiv:1705.09031 [stat]},
	author = {Strobl, Eric V. and Visweswaran, Shyam and Spirtes, Peter L.},
	month = may,
	year = {2017},
	note = {arXiv: 1705.09031},
	keywords = {Statistics - Machine Learning, Statistics - Methodology},
}

@article{rubin_inference_1976,
	title = {Inference and missing data},
	volume = {63},
	number = {3},
	journal = {Biometrika},
	author = {Rubin, Donald B.},
	year = {1976},
	note = {Publisher: Oxford University Press},
	pages = {581--592},
}

@book{little_statistical_2019,
	title = {Statistical analysis with missing data},
	volume = {793},
	publisher = {John Wiley \& Sons},
	author = {Little, Roderick JA and Rubin, Donald B.},
	year = {2019},
}

@article{anderson_two-sample_1994,
	title = {Two-sample test statistics for measuring discrepancies between two multivariate probability density functions using kernel-based density estimates},
	volume = {50},
	number = {1},
	journal = {Journal of Multivariate Analysis},
	author = {Anderson, Niall H. and Hall, Peter and Titterington, D. Michael},
	year = {1994},
	note = {Publisher: Elsevier},
	pages = {41--54},
}

@misc{blanco_nanoflann_2014,
	title = {nanoflann: a \{{C}\}++ header-only fork of \{{FLANN}\}, a library for {Nearest} {Neighbor} (\{{NN}\}) with {KD}-trees},
	url = {https://github.com/jlblancoc/nanoflann},
	author = {Blanco, Jose Luis and Rai, Pranjal Kumar},
	year = {2014},
}

@inproceedings{perez-cruz_estimation_2009,
	title = {Estimation of {Information} {Theoretic} {Measures} for {Continuous} {Random} {Variables}},
	volume = {21},
	url = {https://proceedings.neurips.cc/paper/2008/hash/ccb0989662211f61edae2e26d58ea92f-Abstract.html},
	urldate = {2021-10-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Pérez-Cruz, Fernando},
	year = {2009},
}

@incollection{kontkanen_mdl_2005,
	title = {An {MDL} {Framework} for {Data} {Clustering}},
	booktitle = {Advances in {Minimum} {Description} {Length}},
	publisher = {The MIT press},
	author = {Kontkanen, Petri and Myllymäki, Petri and Buntine, Wray and Rissanen, Jorma and Tirri, Henry},
	year = {2005},
	pages = {323},
}

@inproceedings{roos_behavior_2005,
	title = {On the {Behavior} of {MDL} {Denoising}},
	url = {https://proceedings.mlr.press/r5/roos05a.html},
	abstract = {On the Behavior of MDL DenoisingTeemu Roos, Petri Myllymäki, Henry Tirri},
	language = {en},
	urldate = {2021-10-29},
	booktitle = {International {Workshop} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Roos, Teemu and Myllymäki, Petri and Tirri, Henry},
	month = jan,
	year = {2005},
	note = {ISSN: 2640-3498},
	pages = {309--316},
}

@article{marquez_improved_2010,
	title = {Improved reliability modeling using {Bayesian} networks and dynamic discretization},
	volume = {95},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832009002646},
	doi = {10.1016/j.ress.2009.11.012},
	abstract = {This paper shows how recent Bayesian network (BN) algorithms can be used to model time to failure distributions and perform reliability analysis of complex systems in a simple unified way. The algorithms work for so-called hybrid BNs, which are BNs that can contain a mixture of both discrete and continuous variables. Our BN approach extends fault trees by defining the time-to-failure of the fault tree constructs as deterministic functions of the corresponding input components’ time-to-failure. This helps solve any configuration of static and dynamic gates with general time-to-failure distributions. Unlike other approaches (which tend to be restricted to using exponential failure distributions) our approach can use any parametric or empirical distribution for the time-to-failure of the system components. We demonstrate that the approach produces results equivalent to the state of the practice and art for small examples; more importantly our approach produces solutions hitherto unobtainable for more complex examples, involving non-standard assumptions.. The approach offers a powerful framework for analysts and decision makers to successfully perform robust reliability assessment. Sensitivity, uncertainty, diagnosis analysis, common cause failures and warranty analysis can also be easily performed within this framework.},
	language = {en},
	number = {4},
	urldate = {2021-10-28},
	journal = {Reliability Engineering \& System Safety},
	author = {Marquez, David and Neil, Martin and Fenton, Norman},
	month = apr,
	year = {2010},
	keywords = {Bayesian networks, Dynamic discretization, Dynamic fault trees, Systems reliability},
	pages = {412--425},
}

@article{neil_inference_2007,
	title = {Inference in hybrid {Bayesian} networks using dynamic discretization},
	volume = {17},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-007-9018-y},
	doi = {10.1007/s11222-007-9018-y},
	abstract = {We consider approximate inference in hybrid Bayesian Networks (BNs) and present a new iterative algorithm that efficiently combines dynamic discretization with robust propagation algorithms on junction trees. Our approach offers a significant extension to Bayesian Network theory and practice by offering a flexible way of modeling continuous nodes in BNs conditioned on complex configurations of evidence and intermixed with discrete nodes as both parents and children of continuous nodes. Our algorithm is implemented in a commercial Bayesian Network software package, AgenaRisk, which allows model construction and testing to be carried out easily. The results from the empirical trials clearly show how our software can deal effectively with different type of hybrid models containing elements of expert judgment as well as statistical inference. In particular, we show how the rapid convergence of the algorithm towards zones of high probability density, make robust inference analysis possible even in situations where, due to the lack of information in both prior and data, robust sampling becomes unfeasible.},
	language = {en},
	number = {3},
	urldate = {2021-10-28},
	journal = {Statistics and Computing},
	author = {Neil, Martin and Tailor, Manesh and Marquez, David},
	month = sep,
	year = {2007},
	pages = {219--233},
}

@article{pearson_vii_1896,
	title = {{VII}. {Mathematical} contributions to the theory of evolution.—{III}. {Regression}, heredity, and panmixia},
	number = {187},
	journal = {Philosophical Transactions of the Royal Society of London. Series A, containing papers of a mathematical or physical character},
	author = {Pearson, Karl},
	year = {1896},
	note = {Publisher: The Royal Society London},
	pages = {253--318},
}

@inproceedings{moon_ensemble_2017,
	title = {Ensemble estimation of mutual information},
	doi = {10.1109/ISIT.2017.8007086},
	abstract = {We derive the mean squared error convergence rates of kernel density-based plug-in estimators of mutual information measures between two multidimensional random variables X and Y for two cases: 1) X and Y are both continuous; 2) X is continuous and Y is discrete. Using the derived rates, we propose an ensemble estimator of these information measures for the second case by taking a weighted sum of the plug-in estimators with varied bandwidths. The resulting ensemble estimator achieves the 1 /N parametric convergence rate when the conditional densities of the continuous variables are sufficiently smooth. To the best of our knowledge, this is the first nonparametric mutual information estimator known to achieve the parametric convergence rate for this case, which frequently arises in applications (e.g. variable selection in classification). The estimator is simple to implement as it uses the solution to an offline convex optimization problem and simple plug-in estimators. Ensemble estimators that achieve the parametric rate are also derived for the first case (X and Y are both continuous) and another case: 3) X and Y may have any mixture of discrete and continuous components.},
	booktitle = {2017 {IEEE} {International} {Symposium} on {Information} {Theory} ({ISIT})},
	author = {Moon, Kevin R. and Sricharan, Kumar and Hero, Alfred O.},
	month = jun,
	year = {2017},
	note = {ISSN: 2157-8117},
	keywords = {Convergence, Estimation, Feature extraction, Kernel, Mutual information, Random variables},
	pages = {3030--3034},
}

@article{singh_nearest_2003,
	title = {Nearest neighbor estimates of entropy},
	volume = {23},
	number = {3-4},
	journal = {American journal of mathematical and management sciences},
	author = {Singh, Harshinder and Misra, Neeraj and Hnizdo, Vladimir and Fedorowicz, Adam and Demchuk, Eugene},
	year = {2003},
	note = {Publisher: Taylor \& Francis},
	pages = {301--321},
}

@article{kozachenko_sample_1987,
	title = {Sample estimate of the entropy of a random vector},
	volume = {23},
	number = {2},
	journal = {Problemy Peredachi Informatsii},
	author = {Kozachenko, L. F. and Leonenko, Nikolai N.},
	year = {1987},
	note = {Publisher: Russian Academy of Sciences, Branch of Informatics, Computer Equipment and …},
	pages = {9--16},
}

@article{suzuki_estimator_2016,
	title = {An {Estimator} of {Mutual} {Information} and its {Application} to {Independence} {Testing}},
	volume = {18},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/18/4/109},
	doi = {10.3390/e18040109},
	abstract = {This paper proposes a novel estimator of mutual information for discrete and continuous variables. The main feature of this estimator is that it is zero for a large sample size n if and only if the two variables are independent. The estimator can be used to construct several histograms, compute estimations of mutual information, and choose the maximum value. We prove that the number of histograms constructed has an upper bound of O(log n) and apply this fact to the search. We compare the performance of the proposed estimator with an estimator of the Hilbert-Schmidt independence criterion (HSIC), though the proposed method is based on the minimum description length (MDL) principle and the HSIC provides a statistical test. The proposed method completes the estimation in O(n log n) time, whereas the HSIC kernel computation requires O(n3) time. We also present examples in which the HSIC fails to detect independence but the proposed method successfully detects it.},
	language = {en},
	number = {4},
	urldate = {2021-10-26},
	journal = {Entropy},
	author = {Suzuki, Joe},
	month = apr,
	year = {2016},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Hilbert-Schmidt independence criterion (HSIC), histogram, independence testing, kernel, minimum description length (MDL) principle, mutual information},
	pages = {109},
}

@article{nemenman_entropy_2004,
	title = {Entropy and information in neural spike trains: {Progress} on the sampling problem},
	volume = {69},
	shorttitle = {Entropy and information in neural spike trains},
	number = {5},
	journal = {Physical Review E},
	author = {Nemenman, Ilya and Bialek, William and Van Steveninck, Rob De Ruyter},
	year = {2004},
	note = {Publisher: APS},
	pages = {056111},
}

@article{schurmann_entropy_1996,
	title = {Entropy estimation of symbol sequences},
	volume = {6},
	issn = {1054-1500, 1089-7682},
	url = {http://arxiv.org/abs/cond-mat/0203436},
	doi = {10.1063/1.166191},
	abstract = {We discuss algorithms for estimating the Shannon entropy h of finite symbol sequences with long range correlations. In particular, we consider algorithms which estimate h from the code lengths produced by some compression algorithm. Our interest is in describing their convergence with sequence length, assuming no limits for the space and time complexities of the compression algorithms. A scaling law is proposed for extrapolation from finite sample lengths. This is applied to sequences of dynamical systems in non-trivial chaotic regimes, a 1-D cellular automaton, and to written English texts.},
	number = {3},
	urldate = {2021-10-26},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Schürmann, Thomas and Grassberger, Peter},
	month = sep,
	year = {1996},
	note = {arXiv: cond-mat/0203436},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Theory, Condensed Matter - Statistical Mechanics, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
	pages = {414--427},
}

@article{grassberger_entropy_2008,
	title = {Entropy {Estimates} from {Insufficient} {Samplings}},
	url = {http://arxiv.org/abs/physics/0307138},
	abstract = {We present a detailed derivation of some estimators of Shannon entropy for discrete distributions. They hold for finite samples of N points distributed into M "boxes", with N and M -{\textgreater} oo, but N/M {\textless} oo. In the high sampling regime ({\textless}{\textless} 1 points in each box) they have exponentially small biases. In the low sampling regime the errors increase but are still much smaller than for most other estimators. One advantage is that our main estimators are given analytically, with explicitly known analytical formulas for the biases.},
	urldate = {2021-10-26},
	journal = {arXiv:physics/0307138},
	author = {Grassberger, P.},
	month = jan,
	year = {2008},
	note = {arXiv: physics/0307138},
	keywords = {Physics - Computational Physics, Physics - Data Analysis, Statistics and Probability},
}

@article{efron_jackknife_1981,
	title = {The {Jackknife} {Estimate} of {Variance}},
	volume = {9},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-9/issue-3/The-Jackknife-Estimate-of-Variance/10.1214/aos/1176345462.full},
	doi = {10.1214/aos/1176345462},
	abstract = {Tukey's jackknife estimate of variance for a statistic \$S(X\_1, X\_2, {\textbackslash}cdots, X\_n)\$ which is a symmetric function of i.i.d. random variables \$X\_i\$, is investigated using an ANOVA-like decomposition of \$S\$. It is shown that the jackknife variance estimate tends always to be biased upwards, a theorem to this effect being proved for the natural jackknife estimate of \${\textbackslash}operatorname\{Var\} S(X\_1, X\_2, {\textbackslash}cdots, X\_\{n-1\})\$ based on \$X\_1, X\_2, {\textbackslash}cdots, X\_n\$.},
	number = {3},
	urldate = {2021-10-25},
	journal = {The Annals of Statistics},
	author = {Efron, B. and Stein, C.},
	month = may,
	year = {1981},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {\$U\$ statistics, 62G05, ANOVA decomposition, bootstrap, jackknife, variance estimation},
	pages = {586--596},
}

@article{miller_note_1955,
	title = {Note on the bias of information estimates},
	abstract = {Semantic Scholar extracted view of \&quot;Note on the bias of information estimates\&quot; by G. Miller et al.},
	language = {en},
	urldate = {2021-10-25},
	journal = {Information theory in psychology},
	author = {Miller, G.},
	year = {1955},
}

@misc{noauthor_note_nodate,
	title = {Note on the bias of information estimates – {ScienceOpen}},
	url = {https://www.scienceopen.com/document?vid=357d299f-62fa-4bda-8dd2-e4d5b5abde5d},
	urldate = {2021-10-25},
}

@article{grassberger_entropy_2008-1,
	title = {Entropy {Estimates} from {Insufficient} {Samplings}},
	url = {http://arxiv.org/abs/physics/0307138},
	abstract = {We present a detailed derivation of some estimators of Shannon entropy for discrete distributions. They hold for finite samples of N points distributed into M "boxes", with N and M -{\textgreater} oo, but N/M {\textless} oo. In the high sampling regime ({\textless}{\textless} 1 points in each box) they have exponentially small biases. In the low sampling regime the errors increase but are still much smaller than for most other estimators. One advantage is that our main estimators are given analytically, with explicitly known analytical formulas for the biases.},
	urldate = {2021-10-25},
	journal = {arXiv:physics/0307138},
	author = {Grassberger, P.},
	month = jan,
	year = {2008},
	note = {arXiv: physics/0307138},
	keywords = {Physics - Computational Physics, Physics - Data Analysis, Statistics and Probability},
}

@article{wieczorek_information_2019,
	title = {Information {Theoretic} {Causal} {Effect} {Quantification}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/21/10/975},
	doi = {10.3390/e21100975},
	abstract = {Modelling causal relationships has become popular across various disciplines. Most common frameworks for causality are the Pearlian causal directed acyclic graphs (DAGs) and the Neyman-Rubin potential outcome framework. In this paper, we propose an information theoretic framework for causal effect quantification. To this end, we formulate a two step causal deduction procedure in the Pearl and Rubin frameworks and introduce its equivalent which uses information theoretic terms only. The first step of the procedure consists of ensuring no confounding or finding an adjustment set with directed information. In the second step, the causal effect is quantified. We subsequently unify previous definitions of directed information present in the literature and clarify the confusion surrounding them. We also motivate using chain graphs for directed information in time series and extend our approach to chain graphs. The proposed approach serves as a translation between causality modelling and information theory.},
	language = {en},
	number = {10},
	urldate = {2021-10-25},
	journal = {Entropy},
	author = {Wieczorek, Aleksander and Roth, Volker},
	month = oct,
	year = {2019},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {average treatment effect, back-door criterion, causal effect, chain graph, conditional mutual information, confounding, directed information, directed mutual information, potential outcomes, time series},
	pages = {975},
}

@inproceedings{lopez-paz_discovering_2017,
	title = {Discovering causal signals in images},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Lopez-Paz, David and Nishihara, Robert and Chintala, Soumith and Scholkopf, Bernhard and Bottou, Léon},
	year = {2017},
	pages = {6979--6987},
}

@article{arjovsky_invariant_2019,
	title = {Invariant risk minimization},
	journal = {arXiv preprint arXiv:1907.02893},
	author = {Arjovsky, Martin and Bottou, Léon and Gulrajani, Ishaan and Lopez-Paz, David},
	year = {2019},
}

@article{arjovsky_invariant_2019-1,
	title = {Invariant risk minimization},
	journal = {arXiv preprint arXiv:1907.02893},
	author = {Arjovsky, Martin and Bottou, Léon and Gulrajani, Ishaan and Lopez-Paz, David},
	year = {2019},
}

@article{marx_weaker_2020,
	title = {A {Weaker} {Faithfulness} {Assumption} based on {Triple} {Interactions}},
	journal = {arXiv preprint arXiv:2010.14265},
	author = {Marx, Alexander and Gretton, Arthur and Mooij, Joris M.},
	year = {2020},
}

@article{zhang_detection_2008,
	title = {Detection of {Unfaithfulness} and {Robust} {Causal} {Inference}},
	volume = {18},
	issn = {1572-8641},
	url = {https://doi.org/10.1007/s11023-008-9096-4},
	doi = {10.1007/s11023-008-9096-4},
	abstract = {Much of the recent work on the epistemology of causation has centered on two assumptions, known as the Causal Markov Condition and the Causal Faithfulness Condition. Philosophical discussions of the latter condition have exhibited situations in which it is likely to fail. This paper studies the Causal Faithfulness Condition as a conjunction of weaker conditions. We show that some of the weaker conjuncts can be empirically tested, and hence do not have to be assumed a priori. Our results lead to two methodologically significant observations: (1) some common types of counterexamples to the Faithfulness condition constitute objections only to the empirically testable part of the condition; and (2) some common defenses of the Faithfulness condition do not provide justification or evidence for the testable parts of the condition. It is thus worthwhile to study the possibility of reliable causal inference under weaker Faithfulness conditions. As it turns out, the modification needed to make standard procedures work under a weaker version of the Faithfulness condition also has the practical effect of making them more robust when the standard Faithfulness condition actually holds. This, we argue, is related to the possibility of controlling error probabilities with finite sample size (“uniform consistency”) in causal inference.},
	language = {en},
	number = {2},
	urldate = {2021-10-24},
	journal = {Minds and Machines},
	author = {Zhang, Jiji and Spirtes, Peter},
	month = jun,
	year = {2008},
	pages = {239--271},
}

@article{noshad_scalable_2018,
	title = {Scalable {Mutual} {Information} {Estimation} using {Dependence} {Graphs}},
	url = {http://arxiv.org/abs/1801.09125},
	abstract = {The Mutual Information (MI) is an often used measure of dependency between two random variables utilized in information theory, statistics and machine learning. Recently several MI estimators have been proposed that can achieve parametric MSE convergence rate. However, most of the previously proposed estimators have the high computational complexity of at least \$O(N{\textasciicircum}2)\$. We propose a unified method for empirical non-parametric estimation of general MI function between random vectors in \${\textbackslash}mathbb\{R\}{\textasciicircum}d\$ based on \$N\$ i.i.d. samples. The reduced complexity MI estimator, called the ensemble dependency graph estimator (EDGE), combines randomized locality sensitive hashing (LSH), dependency graphs, and ensemble bias-reduction methods. We prove that EDGE achieves optimal computational complexity \$O(N)\$, and can achieve the optimal parametric MSE rate of \$O(1/N)\$ if the density is \$d\$ times differentiable. To the best of our knowledge EDGE is the first non-parametric MI estimator that can achieve parametric MSE rates with linear time complexity. We illustrate the utility of EDGE for the analysis of the information plane (IP) in deep learning. Using EDGE we shed light on a controversy on whether or not the compression property of information bottleneck (IB) in fact holds for ReLu and other rectification functions in deep neural networks (DNN).},
	urldate = {2021-10-23},
	journal = {arXiv:1801.09125 [cs, math, stat]},
	author = {Noshad, Morteza and Zeng, Yu and Hero III, Alfred O.},
	month = nov,
	year = {2018},
	note = {arXiv: 1801.09125},
	keywords = {Computer Science - Information Theory, Statistics - Machine Learning},
}

@article{shwartz-ziv_opening_2017,
	title = {Opening the {Black} {Box} of {Deep} {Neural} {Networks} via {Information}},
	url = {http://arxiv.org/abs/1703.00810},
	abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the {\textbackslash}textit\{Information Plane\}; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on \{{\textbackslash}emph compression\} of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.},
	urldate = {2021-10-22},
	journal = {arXiv:1703.00810 [cs]},
	author = {Shwartz-Ziv, Ravid and Tishby, Naftali},
	month = apr,
	year = {2017},
	note = {arXiv: 1703.00810},
	keywords = {Computer Science - Machine Learning},
}

@article{geiger_information_2021,
	title = {On {Information} {Plane} {Analyses} of {Neural} {Network} {Classifiers} -- {A} {Review}},
	url = {http://arxiv.org/abs/2003.09671},
	abstract = {We review the current literature concerned with information plane analyses of neural network classifiers. While the underlying information bottleneck theory and the claim that information-theoretic compression is causally linked to generalization are plausible, empirical evidence was found to be both supporting and conflicting. We review this evidence together with a detailed analysis of how the respective information quantities were estimated. Our survey suggests that compression visualized in information planes is not necessarily information-theoretic, but is rather often compatible with geometric compression of the latent representations. This insight gives the information plane a renewed justification. Aside from this, we shed light on the problem of estimating mutual information in deterministic neural networks and its consequences. Specifically, we argue that even in feed-forward neural networks the data processing inequality need not hold for estimates of mutual information. Similarly, while a fitting phase, in which the mutual information between the latent representation and the target increases, is necessary (but not sufficient) for good classification performance, depending on the specifics of mutual information estimation such a fitting phase need not be visible in the information plane.},
	urldate = {2021-10-22},
	journal = {arXiv:2003.09671 [cs, math, stat]},
	author = {Geiger, Bernhard C.},
	month = jun,
	year = {2021},
	note = {arXiv: 2003.09671},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{goldfeld_estimating_2019,
	title = {Estimating {Information} {Flow} in {Deep} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v97/goldfeld19a.html},
	abstract = {We study the estimation of the mutual information I(X;T\_ℓℓ{\textbackslash}ell) between the input X to a deep neural network (DNN) and the output vector T\_ℓℓ{\textbackslash}ell of its ℓℓ{\textbackslash}ell-th hidden layer (an “internal representation”). Focusing on feedforward networks with fixed weights and noisy internal representations, we develop a rigorous framework for accurate estimation of I(X;T\_ℓℓ{\textbackslash}ell). By relating I(X;T\_ℓℓ{\textbackslash}ell) to information transmission over additive white Gaussian noise channels, we reveal that compression, i.e. reduction in I(X;T\_ℓℓ{\textbackslash}ell) over the course of training, is driven by progressive geometric clustering of the representations of samples from the same class. Experimental results verify this connection. Finally, we shift focus to purely deterministic DNNs, where I(X;T\_ℓℓ{\textbackslash}ell) is provably vacuous, and show that nevertheless, these models also cluster inputs belonging to the same class. The binning-based approximation of I(X;T\_ℓℓ{\textbackslash}ell) employed in past works to measure compression is identified as a measure of clustering, thus clarifying that these experiments were in fact tracking the same clustering phenomenon. Leveraging the clustering perspective, we provide new evidence that compression and generalization may not be causally related and discuss potential future research ideas.},
	language = {en},
	urldate = {2021-10-22},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Goldfeld, Ziv and Berg, Ewout Van Den and Greenewald, Kristjan and Melnyk, Igor and Nguyen, Nam and Kingsbury, Brian and Polyanskiy, Yury},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2299--2308},
}

@article{tishby_information_2000,
	title = {The information bottleneck method},
	url = {http://arxiv.org/abs/physics/0004057},
	abstract = {We define the relevant information in a signal \$x{\textbackslash}in X\$ as being the information that this signal provides about another signal \$y{\textbackslash}in {\textbackslash}Y\$. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal \$x\$ requires more than just predicting \$y\$, it also requires specifying which features of \${\textbackslash}X\$ play a role in the prediction. We formalize this problem as that of finding a short code for \${\textbackslash}X\$ that preserves the maximum information about \${\textbackslash}Y\$. That is, we squeeze the information that \${\textbackslash}X\$ provides about \${\textbackslash}Y\$ through a `bottleneck' formed by a limited set of codewords \${\textbackslash}tX\$. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure \$d(x,{\textbackslash}x)\$ emerges from the joint statistics of \${\textbackslash}X\$ and \${\textbackslash}Y\$. This approach yields an exact set of self consistent equations for the coding rules \$X {\textbackslash}to {\textbackslash}tX\$ and \${\textbackslash}tX {\textbackslash}to {\textbackslash}Y\$. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut-Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.},
	urldate = {2021-10-22},
	journal = {arXiv:physics/0004057},
	author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
	month = apr,
	year = {2000},
	note = {arXiv: physics/0004057},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Physics - Data Analysis, Statistics and Probability},
}

@inproceedings{saxe_information_2018,
	title = {On the {Information} {Bottleneck} {Theory} of {Deep} {Learning}},
	url = {https://openreview.net/forum?id=ry_WPG-A-},
	abstract = {We show that several claims of the information bottleneck theory of deep learning are not true in the general case.},
	language = {en},
	urldate = {2021-10-22},
	author = {Saxe, Andrew Michael and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan Daniel and Cox, David Daniel},
	month = feb,
	year = {2018},
}

@article{symmans_measurement_2007,
	title = {Measurement of residual breast cancer burden to predict survival after neoadjuvant chemotherapy},
	volume = {25},
	number = {28},
	journal = {Journal of Clinical Oncology},
	author = {Symmans, W. Fraser and Peintinger, Florentia and Hatzis, Christos and Rajan, Radhika and Kuerer, Henry and Valero, Vicente and Assad, Lina and Poniecka, Anna and Hennessy, Bryan and Green, Marjorie},
	year = {2007},
	note = {Publisher: American Society of Clinical Oncology},
	pages = {4414--4422},
}

@article{schreiber_measuring_2000,
	title = {Measuring information transfer},
	volume = {85},
	number = {2},
	journal = {Physical review letters},
	author = {Schreiber, Thomas},
	year = {2000},
	pages = {461},
}

@article{granger_investigating_1969,
	title = {Investigating causal relations by econometric models and cross-spectral methods},
	journal = {Econometrica: journal of the Econometric Society},
	author = {Granger, Clive WJ},
	year = {1969},
	pages = {424--438},
}

@inproceedings{marx_estimating_2021,
	title = {Estimating {Conditional} {Mutual} {Information} for {Discrete}-{Continuous} {Mixtures} using {Multi}-{Dimensional} {Adaptive} {Histograms}},
	booktitle = {Proceedings of the 2021 {SIAM} {International} {Conference} on {Data} {Mining} ({SDM})},
	publisher = {SIAM},
	author = {Marx, Alexander and Yang, Lincen and van Leeuwen, Matthijs},
	year = {2021},
	pages = {387--395},
}

@article{pal_estimation_2010,
	title = {Estimation of {R}{\textbackslash}'enyi {Entropy} and {Mutual} {Information} {Based} on {Generalized} {Nearest}-{Neighbor} {Graphs}},
	url = {http://arxiv.org/abs/1003.1954},
	abstract = {We present simple and computationally efficient nonparametric estimators of R{\textbackslash}'enyi entropy and mutual information based on an i.i.d. sample drawn from an unknown, absolutely continuous distribution over \${\textbackslash}R{\textasciicircum}d\$. The estimators are calculated as the sum of \$p\$-th powers of the Euclidean lengths of the edges of the `generalized nearest-neighbor' graph of the sample and the empirical copula of the sample respectively. For the first time, we prove the almost sure consistency of these estimators and upper bounds on their rates of convergence, the latter of which under the assumption that the density underlying the sample is Lipschitz continuous. Experiments demonstrate their usefulness in independent subspace analysis.},
	urldate = {2021-10-19},
	journal = {arXiv:1003.1954 [cs, stat]},
	author = {Pál, Dávid and Póczos, Barnabás and Szepesvári, Csaba},
	month = oct,
	year = {2010},
	note = {arXiv: 1003.1954},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Machine Learning},
}

@incollection{marx_estimating_2021-1,
	series = {Proceedings},
	title = {Estimating {Conditional} {Mutual} {Information} for {Discrete}-{Continuous} {Mixtures} using {Multi}-{Dimensional} {Adaptive} {Histograms}},
	url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611976700.44},
	abstract = {Estimating conditional mutual information (CMI) is an essential yet challenging step in many machine learning and data mining tasks. Estimating CMI from data that contains both discrete and continuous variables, or even discrete-continuous mixture variables, is a particularly hard problem. In this paper, we show that CMI for such mixture variables, defined based on the Radon-Nikodym derivate, can be written as a sum of entropies, just like CMI for purely discrete or continuous data. Further, we show that CMI can be consistently estimated for discrete-continuous mixture variables by learning an adaptive histogram model. In practice, we estimate such a model by iteratively discretizing the continuous data points in the mixture variables. To evaluate the performance of our estimator, we benchmark it against state-of-the-art CMI estimators as well as evaluate it in a causal discovery setting.},
	urldate = {2021-10-19},
	booktitle = {Proceedings of the 2021 {SIAM} {International} {Conference} on {Data} {Mining} ({SDM})},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Marx, Alexander and Yang, Lincen and van Leeuwen, Matthijs},
	month = jan,
	year = {2021},
	doi = {10.1137/1.9781611976700.44},
	pages = {387--395},
}

@article{jiao_nearest_2018,
	title = {The {Nearest} {Neighbor} {Information} {Estimator} is {Adaptively} {Near} {Minimax} {Rate}-{Optimal}},
	url = {http://arxiv.org/abs/1711.08824},
	abstract = {We analyze the Kozachenko--Leonenko (KL) nearest neighbor estimator for the differential entropy. We obtain the first uniform upper bound on its performance over H{\textbackslash}"older balls on a torus without assuming any conditions on how close the density could be from zero. Accompanying a new minimax lower bound over the H{\textbackslash}"older ball, we show that the KL estimator is achieving the minimax rates up to logarithmic factors without cognizance of the smoothness parameter \$s\$ of the H{\textbackslash}"older ball for \$s{\textbackslash}in (0,2]\$ and arbitrary dimension \$d\$, rendering it the first estimator that provably satisfies this property.},
	urldate = {2021-10-18},
	journal = {arXiv:1711.08824 [cs, math, stat]},
	author = {Jiao, Jiantao and Gao, Weihao and Han, Yanjun},
	month = sep,
	year = {2018},
	note = {arXiv: 1711.08824},
	keywords = {Computer Science - Information Theory, Statistics - Machine Learning},
}

@misc{noauthor_nearest_nodate,
	title = {‪{The} nearest neighbor information estimator is adaptively near minimax rate-optimal‬},
	url = {https://scholar.google.fr/citations?view_op=view_citation&hl=en&user=E__5Lr0AAAAJ&sortby=pubdate&citation_for_view=E__5Lr0AAAAJ:ufrVoPGSRksC},
	abstract = {‪J Jiao, W Gao, Y Han‬, ‪arXiv preprint arXiv:1711.08824, 2017‬ - ‪Cited by 41‬},
	urldate = {2021-10-18},
}

@article{gao_demystifying_2016,
	title = {Demystifying {Fixed} k-{Nearest} {Neighbor} {Information} {Estimators}},
	url = {http://arxiv.org/abs/1604.03006},
	abstract = {Estimating mutual information from i.i.d. samples drawn from an unknown joint density function is a basic statistical problem of broad interest with multitudinous applications. The most popular estimator is one proposed by Kraskov and St{\textbackslash}"ogbauer and Grassberger (KSG) in 2004, and is nonparametric and based on the distances of each sample to its \$k{\textasciicircum}\{{\textbackslash}rm th\}\$ nearest neighboring sample, where \$k\$ is a fixed small integer. Despite its widespread use (part of scientific software packages), theoretical properties of this estimator have been largely unexplored. In this paper we demonstrate that the estimator is consistent and also identify an upper bound on the rate of convergence of the bias as a function of number of samples. We argue that the superior performance benefits of the KSG estimator stems from a curious "correlation boosting" effect and build on this intuition to modify the KSG estimator in novel ways to construct a superior estimator. As a byproduct of our investigations, we obtain nearly tight rates of convergence of the \${\textbackslash}ell\_2\$ error of the well known fixed \$k\$ nearest neighbor estimator of differential entropy by Kozachenko and Leonenko.},
	urldate = {2021-10-18},
	journal = {arXiv:1604.03006 [cs, math, stat]},
	author = {Gao, Weihao and Oh, Sewoong and Viswanath, Pramod},
	month = aug,
	year = {2016},
	note = {arXiv: 1604.03006},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{leek_five_2017,
	title = {Five ways to fix statistics},
	volume = {551},
	copyright = {2021 Nature},
	url = {https://www.nature.com/articles/d41586-017-07522-z},
	doi = {10.1038/d41586-017-07522-z},
	abstract = {As debate rumbles on about how and how much poor statistics is to blame for poor reproducibility, Nature asked influential statisticians to recommend one change to improve science. The common theme? The problem is not our maths, but ourselves.},
	language = {en},
	number = {7682},
	urldate = {2021-10-17},
	journal = {Nature},
	author = {Leek, Jeff and McShane, Blakeley B. and Gelman, Andrew and Colquhoun, David and Nuijten, Michèle B. and Goodman, Steven N.},
	month = nov,
	year = {2017},
	note = {Bandiera\_abtest: a
Cg\_type: Comment
Number: 7682
Publisher: Nature Publishing Group
Subject\_term: Research data, Lab life, Mathematics and computing},
	pages = {557--559},
}

@article{amrhein_scientists_2019,
	title = {Scientists rise up against statistical significance},
	volume = {567},
	copyright = {2021 Nature},
	url = {https://www.nature.com/articles/d41586-019-00857-9},
	doi = {10.1038/d41586-019-00857-9},
	abstract = {Valentin Amrhein, Sander Greenland, Blake McShane and more than 800 signatories call for an end to hyped claims and the dismissal of possibly crucial effects.},
	language = {en},
	number = {7748},
	urldate = {2021-10-17},
	journal = {Nature},
	author = {Amrhein, Valentin and Greenland, Sander and McShane, Blake},
	month = mar,
	year = {2019},
	note = {Bandiera\_abtest: a
Cg\_type: Comment
Number: 7748
Publisher: Nature Publishing Group
Subject\_term: Research data, Research management},
	pages = {305--307},
}

@article{rissanen_fisher_1996,
	title = {Fisher information and stochastic complexity},
	volume = {42},
	issn = {1557-9654},
	doi = {10.1109/18.481776},
	abstract = {By taking into account the Fisher information and removing an inherent redundancy in earlier two-part codes, a sharper code length as the stochastic complexity and the associated universal process are derived for a class of parametric processes. The main condition required is that the maximum-likelihood estimates satisfy the central limit theorem. The same code length is also obtained from the so-called maximum-likelihood code.},
	number = {1},
	journal = {IEEE Transactions on Information Theory},
	author = {Rissanen, J.J.},
	month = jan,
	year = {1996},
	note = {Conference Name: IEEE Transactions on Information Theory},
	keywords = {Bayesian methods, Channel capacity, Entropy, Information theory, Markov processes, Mutual information, Senior members, Statistics, Stochastic processes},
	pages = {40--47},
}

@article{rissanen_strong_2001,
	title = {Strong optimality of the normalized {ML} models as universal codes and information in data},
	volume = {47},
	number = {5},
	journal = {IEEE Transactions on Information Theory},
	author = {Rissanen, Jorma},
	year = {2001},
	note = {Publisher: IEEE},
	pages = {1712--1717},
}

@article{shtarkov_universal_1987,
	title = {Universal sequential coding of single messages},
	volume = {23},
	number = {3},
	journal = {Problemy Peredachi Informatsii},
	author = {Shtar'kov, Yurii Mikhailovich},
	year = {1987},
	note = {Publisher: Russian Academy of Sciences, Branch of Informatics, Computer Equipment and …},
	pages = {3--17},
}

@article{azadkia_simple_2021,
	title = {A simple measure of conditional dependence},
	url = {http://arxiv.org/abs/1910.12327},
	abstract = {We propose a coefficient of conditional dependence between two random variables \$Y\$ and \$Z\$ given a set of other variables \$X\_1,{\textbackslash}ldots,X\_p\$, based on an i.i.d. sample. The coefficient has a long list of desirable properties, the most important of which is that under absolutely no distributional assumptions, it converges to a limit in \$[0,1]\$, where the limit is \$0\$ if and only if \$Y\$ and \$Z\$ are conditionally independent given \$X\_1,{\textbackslash}ldots,X\_p\$, and is \$1\$ if and only if \$Y\$ is equal to a measurable function of \$Z\$ given \$X\_1,{\textbackslash}ldots,X\_p\$. Moreover, it has a natural interpretation as a nonlinear generalization of the familiar partial \$R{\textasciicircum}2\$ statistic for measuring conditional dependence by regression. Using this statistic, we devise a new variable selection algorithm, called Feature Ordering by Conditional Independence (FOCI), which is model-free, has no tuning parameters, and is provably consistent under sparsity assumptions. A number of applications to synthetic and real datasets are worked out.},
	urldate = {2021-09-21},
	journal = {arXiv:1910.12327 [cs, math, stat]},
	author = {Azadkia, Mona and Chatterjee, Sourav},
	month = mar,
	year = {2021},
	note = {arXiv: 1910.12327},
	keywords = {62G05, 62H20, Computer Science - Information Theory, Mathematics - Probability, Mathematics - Statistics Theory, Statistics - Methodology},
}

@misc{noauthor_mona_nodate,
	title = {Mona {Azadkia}},
	url = {https://scholar.google.com/citations?user=_Z2QY7sAAAAJ&hl=en},
	abstract = {‪ETH Zürich‬ - ‪‪Cited by 22‬‬ - ‪Statistics‬ - ‪Probability‬ - ‪Machine Learning‬},
	urldate = {2021-09-21},
}

@misc{noauthor_simple_nodate,
	title = {‪{A} simple measure of conditional dependence‬},
	url = {https://scholar.google.fr/citations?view_op=view_citation&hl=en&user=_Z2QY7sAAAAJ&citation_for_view=_Z2QY7sAAAAJ:d1gkVwhDpl0C},
	abstract = {‪M Azadkia, S Chatterjee‬, ‪arXiv preprint arXiv:1910.12327, 2019‬ - ‪Cited by 17‬},
	urldate = {2021-09-21},
}

@article{shannon_mathematical_1948,
	title = {A mathematical theory of communication},
	volume = {27},
	number = {3},
	journal = {The Bell system technical journal},
	author = {Shannon, Claude Elwood},
	year = {1948},
	note = {Publisher: Nokia Bell Labs},
	pages = {379--423},
}

@article{fazekas_mr_1987,
	title = {{MR} signal abnormalities at 1.5 {T} in {Alzheimer}'s dementia and normal aging},
	volume = {149},
	number = {2},
	journal = {American journal of roentgenology},
	author = {Fazekas, Franz and Chawluk, John B. and Alavi, Abass and Hurtig, Howard I. and Zimmerman, Robert A.},
	year = {1987},
	pages = {351--356},
}

@article{scheltens_white_1992,
	title = {White matter lesions on magnetic resonance imaging in clinically diagnosed {Alzheimer}'s disease: evidence for heterogeneity},
	volume = {115},
	shorttitle = {White matter lesions on magnetic resonance imaging in clinically diagnosed {Alzheimer}'s disease},
	number = {3},
	journal = {Brain},
	author = {Scheltens, P. H. and Barkhof, F. and Valk, J. and Algra, P. R. and HOOP, R. GERRITSEN VAN DER and Nauta, J. and Wolters, E. Ch},
	year = {1992},
	pages = {735--748},
}

@article{kontkanen_computationally_2009,
	title = {Computationally efficient methods for {MDL}-optimal density estimation and data clustering},
	author = {Kontkanen, Petri},
	year = {2009},
}

@article{kontkanen_computationally_2009-1,
	title = {Computationally efficient methods for {MDL}-optimal density estimation and data clustering},
	author = {Kontkanen, Petri},
	year = {2009},
}

@inproceedings{kontkanen_efficient_2003,
	title = {Efficient computation of stochastic complexity},
	booktitle = {Proceedings of the {Ninth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {Citeseer},
	author = {Kontkanen, Petri and Buntine, Wray and Myllymäki, Petri and Rissanen, Jorma and Tirri, Henry},
	year = {2003},
}

@book{szpankowski_average_2011,
	title = {Average case analysis of algorithms on sequences},
	volume = {50},
	publisher = {John Wiley \& Sons},
	author = {Szpankowski, Wojciech},
	year = {2011},
}

@inproceedings{roos_bayesian_2008,
	title = {Bayesian network structure learning using factorized {NML} universal models},
	booktitle = {2008 {Information} {Theory} and {Applications} {Workshop}},
	publisher = {IEEE},
	author = {Roos, Teemu and Silander, Tomi and Kontkanen, Petri and Myllymaki, Petri},
	year = {2008},
	pages = {272--276},
}

@article{kim_ppcor_2015,
	title = {ppcor: an {R} package for a fast calculation to semi-partial correlation coefficients},
	volume = {22},
	shorttitle = {ppcor},
	number = {6},
	journal = {Communications for statistical applications and methods},
	author = {Kim, Seongho},
	year = {2015},
	pages = {665},
}

@article{mooij_joint_2016,
	title = {Joint causal inference from multiple contexts},
	journal = {arXiv preprint arXiv:1611.10351},
	author = {Mooij, Joris M. and Magliacane, Sara and Claassen, Tom},
	year = {2016},
}

@article{boeken_bayesian_2020,
	title = {A bayesian nonparametric conditional two-sample test with an application to local causal discovery},
	journal = {arXiv preprint arXiv:2008.07382},
	author = {Boeken, Philip A. and Mooij, Joris M.},
	year = {2020},
}

@misc{noauthor_bayesian_nodate-1,
	title = {'{A} {Bayesian} {Nonparametric} {Conditional} {Two}-sample... - {Google} {Scholar}},
	url = {https://scholar.google.fr/scholar?hl=en&as_sdt=0%2C5&q=%27A+Bayesian+Nonparametric+Conditional+Two-sample+Test+with+an+Application+to+Local+Causal+Discovery%27+%28Boeken+and+Mooij%2C+2021%29&btnG=},
	urldate = {2021-07-25},
}

@article{shah_hardness_2020,
	title = {The {Hardness} of {Conditional} {Independence} {Testing} and the {Generalised} {Covariance} {Measure}},
	volume = {48},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/1804.07203},
	doi = {10.1214/19-AOS1857},
	abstract = {It is a common saying that testing for conditional independence, i.e., testing whether whether two random vectors \$X\$ and \$Y\$ are independent, given \$Z\$, is a hard statistical problem if \$Z\$ is a continuous random variable (or vector). In this paper, we prove that conditional independence is indeed a particularly difficult hypothesis to test for. Valid statistical tests are required to have a size that is smaller than a predefined significance level, and different tests usually have power against a different class of alternatives. We prove that a valid test for conditional independence does not have power against any alternative. Given the non-existence of a uniformly valid conditional independence test, we argue that tests must be designed so their suitability for a particular problem may be judged easily. To address this need, we propose in the case where \$X\$ and \$Y\$ are univariate to nonlinearly regress \$X\$ on \$Z\$, and \$Y\$ on \$Z\$ and then compute a test statistic based on the sample covariance between the residuals, which we call the generalised covariance measure (GCM). We prove that validity of this form of test relies almost entirely on the weak requirement that the regression procedures are able to estimate the conditional means \$X\$ given \$Z\$, and \$Y\$ given \$Z\$, at a slow rate. We extend the methodology to handle settings where \$X\$ and \$Y\$ may be multivariate or even high-dimensional. While our general procedure can be tailored to the setting at hand by combining it with any regression technique, we develop the theoretical guarantees for kernel ridge regression. A simulation study shows that the test based on GCM is competitive with state of the art conditional independence tests. Code is available as the R package GeneralisedCovarianceMeasure on CRAN.},
	number = {3},
	urldate = {2021-07-21},
	journal = {The Annals of Statistics},
	author = {Shah, Rajen D. and Peters, Jonas},
	month = jun,
	year = {2020},
	note = {arXiv: 1804.07203},
	keywords = {Mathematics - Statistics Theory},
}

@article{spirtes_constructing_2000,
	title = {Constructing {Bayesian} network models of gene expression networks from microarray data},
	author = {Spirtes, Pater and Glymour, Clark and Scheines, Richard and Kauffman, Stuart and Aimale, Valerio and Wimberly, Frank},
	year = {2000},
}

@inproceedings{ramsey_tetradtoolbox_2018,
	title = {{TETRAD}—{A} toolbox for causal discovery},
	booktitle = {8th {International} {Workshop} on {Climate} {Informatics}},
	author = {Ramsey, Joseph D. and Zhang, Kun and Glymour, Madelyn and Romero, Ruben Sanchez and Huang, Biwei and Ebert-Uphoff, Imme and Samarasinghe, Savini and Barnes, Elizabeth A. and Glymour, Clark},
	year = {2018},
}

@inproceedings{ramsey_tetradtoolbox_2018-1,
	title = {{TETRAD}—{A} toolbox for causal discovery},
	booktitle = {8th {International} {Workshop} on {Climate} {Informatics}},
	author = {Ramsey, Joseph D. and Zhang, Kun and Glymour, Madelyn and Romero, Ruben Sanchez and Huang, Biwei and Ebert-Uphoff, Imme and Samarasinghe, Savini and Barnes, Elizabeth A. and Glymour, Clark},
	year = {2018},
}

@article{hacine-gharbi_low_2012,
	title = {Low bias histogram-based estimation of mutual information for feature selection},
	volume = {33},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865512000761},
	doi = {10.1016/j.patrec.2012.02.022},
	abstract = {This paper presents a low bias histogram-based estimation of mutual information and its application to feature selection problems. By canceling the first order bias, the estimation avoids the bias accumulation problem that affects classical methods. As a consequence, on a synthetic feature selection problem, only the proposed method results in the exact number of features to be chosen in the Gaussian case when compared to four other approaches. In a speech recognition application, the proposed method and the Sturges method are the only ones that lead to a correct number of selected features in the noise free case. In the reduced data case, only the proposed method points out the optimal number of features to select. Finally, in the noisy case, only the proposed method leads to results of high quality; other methods show severely underestimated numbers of selected features.},
	language = {en},
	number = {10},
	urldate = {2021-07-08},
	journal = {Pattern Recognition Letters},
	author = {Hacine-Gharbi, Abdenour and Ravier, Philippe and Harba, Rachid and Mohamadi, Tayeb},
	month = jul,
	year = {2012},
	keywords = {Bias, Dimensionality reduction, Feature selection, Mutual information, Shannon entropy, Speech recognition},
	pages = {1302--1308},
}

@article{hacine-gharbi_binning_2018,
	title = {A binning formula of bi-histogram for joint entropy estimation using mean square error minimization},
	volume = {101},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865517304142},
	doi = {10.1016/j.patrec.2017.11.007},
	abstract = {Histograms have extensively been used as a simple tool for nonparametric probability density function estimation. However, practically, the accuracy of some histogram-based derived quantities, such as the marginal entropy (ME), the joint entropy (JE), or the mutual information (MI) depends on the number of bins chosen for the histogram. In this paper, we investigate the binning problem of bi-histogram for the estimation of JE. By minimizing a theoretical mean square error (MSE) of JE estimation, we derive a new formula for the optimal number of bins of bi-histogram for continuous random variables. This novel JE estimation has been used in the MI estimation to avoid the error accumulation of joint MI between the class variable and feature subset in the feature selection. In a synthetic Gaussian feature selection problem, only the proposed method permits to retrieve the exact number of relevant features that explain the class variable when compared to a concurrent univariate estimator based on binning formula that has been proposed for ME estimation. In speech and speaker recognition applications, the proposed method permits to select a limited number of features which guaranties approximately the same or an even better recognition rate than using the total number of features.},
	language = {en},
	urldate = {2021-07-08},
	journal = {Pattern Recognition Letters},
	author = {Hacine-Gharbi, Abdenour and Ravier, Philippe},
	month = jan,
	year = {2018},
	keywords = {Bi-histogram, Feature selection, Histogram bin number, Joint entropy, MFCC feature, Marginal entropy, Mean square error minimization, Mutual information, Shannon entropy, Speaker recognition, Speech recognition},
	pages = {21--28},
}

@article{runge_causal_2018,
	title = {Causal network reconstruction from time series: {From} theoretical assumptions to practical estimation},
	volume = {28},
	issn = {1054-1500},
	shorttitle = {Causal network reconstruction from time series},
	url = {https://aip.scitation.org/doi/full/10.1063/1.5025050},
	doi = {10.1063/1.5025050},
	abstract = {Causal network reconstruction from time series is an emerging topic in many fields of science. Beyond inferring directionality between two time series, the goal of causal network reconstruction or causal discovery is to distinguish direct from indirect dependencies and common drivers among multiple time series. Here, the problem of inferring causal networks including time lags from multivariate time series is recapitulated from the underlying causal assumptions to practical estimation problems. Each aspect is illustrated with simple examples including unobserved variables, sampling issues, determinism, stationarity, nonlinearity, measurement error, and significance testing. The effects of dynamical noise, autocorrelation, and high dimensionality are highlighted in comparison studies of common causal reconstruction methods. Finally, method performance evaluation approaches and criteria are suggested. The article is intended to briefly review and accessibly illustrate the foundations and practical problems of time series-based causal discovery and stimulate further methodological developments.},
	number = {7},
	urldate = {2021-06-21},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Runge, J.},
	month = jul,
	year = {2018},
	note = {Publisher: American Institute of Physics},
	keywords = {manuscrit material},
	pages = {075310},
}

@article{james_information_2016,
	title = {Information flows? {A} critique of transfer entropies},
	volume = {116},
	shorttitle = {Information flows?},
	number = {23},
	journal = {Physical review letters},
	author = {James, Ryan G. and Barnett, Nix and Crutchfield, James P.},
	year = {2016},
	note = {Publisher: APS},
	pages = {238701},
}

@inproceedings{patel_multi-class_2020,
	title = {Multi-{Class} {Uncertainty} {Calibration} via {Mutual} {Information} {Maximization}-based {Binning}},
	url = {https://openreview.net/forum?id=AICNpd8ke-m},
	abstract = {Post-hoc multi-class calibration is a common approach for providing high-quality confidence estimates of deep neural network predictions. Recent work has shown that widely used scaling methods...},
	language = {en},
	urldate = {2021-06-17},
	author = {Patel, Kanil and Beluch, William H. and Yang, Bin and Pfeiffer, Michael and Zhang, Dan},
	month = sep,
	year = {2020},
}

@article{budhathoki_origo_2018,
	title = {Origo: causal inference by compression},
	volume = {56},
	shorttitle = {Origo},
	number = {2},
	journal = {Knowledge and Information Systems},
	author = {Budhathoki, Kailash and Vreeken, Jilles},
	year = {2018},
	note = {Publisher: Springer},
	pages = {285--307},
}

@article{glymour_review_2019,
	title = {Review of {Causal} {Discovery} {Methods} {Based} on {Graphical} {Models}},
	volume = {10},
	issn = {1664-8021},
	url = {https://www.frontiersin.org/articles/10.3389/fgene.2019.00524/full},
	doi = {10.3389/fgene.2019.00524},
	abstract = {A fundamental task in various disciplines of science, including biology, is to find underlying causal relations and make use of them. Causal relations can be seen if interventions are properly applied; however, in many cases they are difficult or even impossible to conduct. It is then necessary to discover causal relations by analyzing statistical properties of purely observational data, which is known as causal discovery or causal structure search. This paper aims to give a introduction to and a brief review of the computational methods for causal discovery that were developed in the past three decades, including constraint-based and score-based methods and those based on functional causal models, supplemented by some illustrations and applications.},
	language = {English},
	urldate = {2020-09-01},
	journal = {Frontiers in Genetics},
	author = {Glymour, Clark and Zhang, Kun and Spirtes, Peter},
	year = {2019},
	note = {Publisher: Frontiers},
	keywords = {Conditional independence, Directed graphical causal models, Non-linear models, Statistical independence, Structural Equation Models, causal discovery, manuscrit material, non-Gaussian distribution, toread},
}

@article{duan_direct_2013,
	title = {Direct causality detection via the transfer entropy approach},
	volume = {21},
	number = {6},
	journal = {IEEE transactions on control systems technology},
	author = {Duan, Ping and Yang, Fan and Chen, Tongwen and Shah, Sirish L.},
	year = {2013},
	note = {Publisher: IEEE},
	keywords = {manuscrit material},
	pages = {2052--2066},
}

@article{assaad_entropy-based_2021,
	title = {Entropy-based {Discovery} of {Summary} {Causal} {Graphs} in {Time} {Series}},
	url = {http://arxiv.org/abs/2105.10381},
	abstract = {We address in this study the problem of learning a summary causal graph on time series with potentially different sampling rates. To do so, we first propose a new temporal mutual information measure defined on a window-based representation of time series. We then show how this measure relates to an entropy reduction principle that can be seen as a special case of the Probabilistic Raising Principle. We finally combine these two ingredients in a PC-like algorithm to construct the summary causal graph. This algorithm is evaluated on several datasets that shows both its efficacy and efficiency.},
	urldate = {2021-06-14},
	journal = {arXiv:2105.10381 [cs]},
	author = {Assaad, Karim and Devijver, Emilie and Gaussier, Eric and Ait-Bachir, Ali},
	month = may,
	year = {2021},
	note = {arXiv: 2105.10381},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{kaiser_unsuitability_2021,
	title = {Unsuitability of {NOTEARS} for {Causal} {Graph} {Discovery}},
	url = {http://arxiv.org/abs/2104.05441},
	abstract = {Causal Discovery methods aim to identify a DAG structure that represents causal relationships from observational data. In this article, we stress that it is important to test such methods for robustness in practical settings. As our main example, we analyze the NOTEARS method, for which we demonstrate a lack of scale-invariance. We show that NOTEARS is a method that aims to identify a parsimonious DAG from the data that explains the residual variance. We conclude that NOTEARS is not suitable for identifying truly causal relationships from the data.},
	urldate = {2021-04-22},
	journal = {arXiv:2104.05441 [cs, math, stat]},
	author = {Kaiser, Marcus and Sipos, Maksim},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.05441},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@article{nabi_causal_2020,
	title = {Causal {Inference} in the {Presence} of {Interference} in {Sponsored} {Search} {Advertising}},
	url = {http://arxiv.org/abs/2010.07458},
	abstract = {In classical causal inference, inferring cause-effect relations from data relies on the assumption that units are independent and identically distributed. This assumption is violated in settings where units are related through a network of dependencies. An example of such a setting is ad placement in sponsored search advertising, where the clickability of a particular ad is potentially influenced by where it is placed and where other ads are placed on the search result page. In such scenarios, confounding arises due to not only the individual ad-level covariates but also the placements and covariates of other ads in the system. In this paper, we leverage the language of causal inference in the presence of interference to model interactions among the ads. Quantification of such interactions allows us to better understand the click behavior of users, which in turn impacts the revenue of the host search engine and enhances user satisfaction. We illustrate the utility of our formalization through experiments carried out on the ad placement system of the Bing search engine.},
	urldate = {2020-10-29},
	journal = {arXiv:2010.07458 [cs]},
	author = {Nabi, Razieh and Pfeiffer, Joel and Bayir, Murat Ali and Charles, Denis and Kıcıman, Emre},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.07458},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, toread},
}

@article{wei_dags_2020,
	title = {{DAGs} with {No} {Fears}: {A} {Closer} {Look} at {Continuous} {Optimization} for {Learning} {Bayesian} {Networks}},
	shorttitle = {{DAGs} with {No} {Fears}},
	url = {http://arxiv.org/abs/2010.09133},
	abstract = {This paper re-examines a continuous optimization framework dubbed NOTEARS for learning Bayesian networks. We first generalize existing algebraic characterizations of acyclicity to a class of matrix polynomials. Next, focusing on a one-parameter-per-edge setting, it is shown that the Karush-Kuhn-Tucker (KKT) optimality conditions for the NOTEARS formulation cannot be satisfied except in a trivial case, which explains a behavior of the associated algorithm. We then derive the KKT conditions for an equivalent reformulation, show that they are indeed necessary, and relate them to explicit constraints that certain edges be absent from the graph. If the score function is convex, these KKT conditions are also sufficient for local minimality despite the non-convexity of the constraint. Informed by the KKT conditions, a local search post-processing algorithm is proposed and shown to substantially and universally improve the structural Hamming distance of all tested algorithms, typically by a factor of 2 or more. Some combinations with local search are both more accurate and more efficient than the original NOTEARS.},
	urldate = {2020-10-29},
	journal = {arXiv:2010.09133 [cs, stat]},
	author = {Wei, Dennis and Gao, Tian and Yu, Yue},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.09133},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, toread},
}

@article{wang_generalizing_2020,
	title = {Generalizing from a few examples: {A} survey on few-shot learning},
	volume = {53},
	shorttitle = {Generalizing from a few examples},
	number = {3},
	journal = {ACM Computing Surveys (CSUR)},
	author = {Wang, Yaqing and Yao, Quanming and Kwok, James T. and Ni, Lionel M.},
	year = {2020},
	note = {Publisher: ACM New York, NY, USA},
	pages = {1--34},
}

@article{wang_generalizing_2020-1,
	title = {Generalizing from a few examples: {A} survey on few-shot learning},
	volume = {53},
	shorttitle = {Generalizing from a few examples},
	number = {3},
	journal = {ACM Computing Surveys (CSUR)},
	author = {Wang, Yaqing and Yao, Quanming and Kwok, James T. and Ni, Lionel M.},
	year = {2020},
	note = {Publisher: ACM New York, NY, USA},
	pages = {1--34},
}

@article{wang_generalizing_2020-2,
	title = {Generalizing from a few examples: {A} survey on few-shot learning},
	volume = {53},
	shorttitle = {Generalizing from a few examples},
	number = {3},
	journal = {ACM Computing Surveys (CSUR)},
	author = {Wang, Yaqing and Yao, Quanming and Kwok, James T. and Ni, Lionel M.},
	year = {2020},
	note = {Publisher: ACM New York, NY, USA},
	pages = {1--34},
}

@article{runge_inferring_2019,
	title = {Inferring causation from time series in {Earth} system sciences},
	volume = {10},
	copyright = {2019 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-019-10105-3/},
	doi = {10.1038/s41467-019-10105-3},
	abstract = {The heart of the scientific enterprise is a rational effort to understand the causes behind the phenomena we observe. In large-scale complex dynamical systems such as the Earth system, real experiments are rarely feasible. However, a rapidly increasing amount of observational and simulated data opens up the use of novel data-driven causal methods beyond the commonly adopted correlation techniques. Here, we give an overview of causal inference frameworks and identify promising generic application cases common in Earth system sciences and beyond. We discuss challenges and initiate the benchmark platform causeme.netto close the gap between method users and developers.},
	language = {en},
	number = {1},
	urldate = {2020-10-09},
	journal = {Nature Communications},
	author = {Runge, Jakob and Bathiany, Sebastian and Bollt, Erik and Camps-Valls, Gustau and Coumou, Dim and Deyle, Ethan and Glymour, Clark and Kretschmer, Marlene and Mahecha, Miguel D. and Muñoz-Marí, Jordi and van Nes, Egbert H. and Peters, Jonas and Quax, Rick and Reichstein, Markus and Scheffer, Marten and Schölkopf, Bernhard and Spirtes, Peter and Sugihara, George and Sun, Jie and Zhang, Kun and Zscheischler, Jakob},
	month = jun,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {2553},
}

@article{varando_learning_2020,
	title = {Learning {DAGs} without imposing acyclicity},
	journal = {arXiv preprint arXiv:2006.03005},
	author = {Varando, Gherardo},
	year = {2020},
}

@inproceedings{weichwald_causal_2020,
	title = {Causal structure learning from time series: {Large} regression coefficients may predict causal links better in practice than small p-values},
	shorttitle = {Causal structure learning from time series},
	url = {http://proceedings.mlr.press/v123/weichwald20a.html},
	abstract = {In this article, we describe the algorithms for causal structure learning from time series data that won the Causality 4 Climate competition at the Conference on Neural Information Processing Syste...},
	language = {en},
	urldate = {2020-10-08},
	booktitle = {{NeurIPS} 2019 {Competition} and {Demonstration} {Track}},
	publisher = {PMLR},
	author = {Weichwald, Sebastian and Jakobsen, Martin E. and Mogensen, Phillip B. and Petersen, Lasse and Thams, Nikolaj and Varando, Gherardo},
	month = aug,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {27--36},
}

@inproceedings{runge_discovering_2020,
	title = {Discovering contemporaneous and lagged causal relations in autocorrelated nonlinear time series datasets},
	url = {http://proceedings.mlr.press/v124/runge20a.html},
	abstract = {The paper introduces a novel conditional independence (CI) based method for linear and nonlinear, lagged and contemporaneous causal discovery from observational time series in the causally sufficie...},
	language = {en},
	urldate = {2020-09-30},
	booktitle = {Conference on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Runge, Jakob},
	month = aug,
	year = {2020},
	note = {ISSN: 2640-3498},
	keywords = {toread},
	pages = {1388--1397},
}

@article{weinreb_lineage_2020,
	title = {Lineage tracing on transcriptional landscapes links state to fate during differentiation},
	volume = {367},
	copyright = {Copyright © 2020 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/367/6479/eaaw3381},
	doi = {10.1126/science.aaw3381},
	abstract = {Mapping cell fate during hematopoiesis
Biologists have long attempted to understand how stem and progenitor cells in regenerating and embryonic tissues differentiate into mature cell types. Through the use of recent technical advances to sequence the genes expressed in thousands of individual cells, differentiation mechanisms are being revealed. Weinreb et al. extended these methods to track clones of cells (cell families) across time. Their approach reveals differences in cellular gene expression as cells progress through hematopoiesis, which is the process of blood production. Using machine learning, they tested how well gene expression measurements account for the choices that cells make. This work reveals that a considerable gap still exists in understanding differentiation mechanisms, and future methods are needed to fully understand—and ultimately control—cell differentiation.
Science, this issue p. eaaw3381
Structured Abstract
INTRODUCTIONDuring tissue turnover, stem and progenitor cells differentiate to produce mature cell types. To understand and ultimately control differentiation, it is important to establish how initial differences between cells influence their ultimate choice of cell fate. This challenge is exemplified in hematopoiesis, the ongoing process of blood regeneration in bone marrow, in which multipotent progenitors give rise to red cells of the blood, as well as myeloid and lymphoid immune cell types.In hematopoiesis, progenitor cell states have been canonically defined by their expression of several antigens. However, as in several other tissues, recent transcriptome analysis by single-cell RNA sequencing (scSeq) showed that the canonically defined intermediate cell types are not uniform, but rather contain cells in a variety of gene expression states. scSeq also showed that the states of hematopoietic progenitors form a continuum, differing from classic depictions of a discrete stepwise hierarchy.
RATIONALEIn this study, we set out to establish how variation in transcriptional state biases future cell fate and whether scSeq is sufficient to completely distinguish cells with distinct fate biases. Directly linking whole-transcriptome descriptions of cells to their future fate is challenging because cells are destroyed during scSeq measurement. We therefore developed a tool we call LARRY (lineage and RNA recovery) that clonally tags cells with DNA barcodes that can be read using scSeq. Using LARRY, we aimed to reconstruct the genome-wide transcriptional trajectories of cells as they differentiate.
RESULTSWe linked transcriptional progenitor states with their clonal fates by barcoding heterogeneous cells, allowing cell division, and then sampling cells for scSeq immediately or at later time points after differentiation in culture or in transplanted mice. We profiled {\textgreater}300,000 cells in total, comprising 10,968 clones that gave information on lineage relationships at single time points and 2632 clones spanning multiple time points in culture or in mice. We confirmed that clonal trajectories over time approximated the trajectories of single cells and were thus able to identify states of primed fate potential on the continuous transcriptional landscape. From this analysis, we identified genes correlating with fate, established a lineage hierarchy for hematopoiesis in culture and after transplantation, and revealed two routes of monocyte differentiation that give rise to distinct gene expression programs in mature cells. The data made it possible to test state-of the-art algorithms of scSeq analysis, and we found that fate choice occurs earlier than predicted algorithmically but that computationally predicted pseudotime orderings faithfully describe clonal dynamics.We investigated whether there are stable cellular properties that have a cell-autonomous influence on fate choice yet are not detected by scSeq. By analyzing clones split between wells or transplanted into separate mice, we found that the variance in cell fate choice attributable to cell-autonomous fate bias was greater than what could be explained by initial transcriptional state. Less formally, sister cells tended to be far more similar in their fate choice than pairs of cells with similar transcriptomes. These results suggest that current scSeq measurements cannot fully separate progenitor cells with distinct fate bias. The missing signature of future fate choice might be detectable in the RNA that is not sampled during scSeq. Alternatively, other stable cellular properties such as chromatin state could encode the missing information.
CONCLUSIONBy integrating transcriptome and lineage measurements, we established a map of clonal fate on a continuous transcriptional landscape. The map revealed transcriptional correlates of fate among putatively multipotent cells, convergent differentiation trajectories, and fate boundaries that could be not be predicted using current trajectory inference methods. However, the map is far from complete because scSeq cannot separate cells with distinct fate bias. Our results argue for looking beyond scSeq to define cellular maps of stem and progenitor cells and offer an approach for linking cell state and fate in other tissues. {\textless}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/367/6479/eaaw3381/F1.medium.gif"/{\textgreater} Download high-res image Open in new tab Download Powerpoint Lineage and transcriptome measurements allow fate mapping on continuous cell state landscapes.A tool we named LARRY labels cell clones with an scSeq-compatible barcode. By barcoding cells, letting them divide, and then sampling them immediately or after differentiation, it is possible to link the initial states of cells with their differentiation outcomes and produce a map of cell fate bias on a continuous transcriptional landscape.
A challenge in biology is to associate molecular differences among progenitor cells with their capacity to generate mature cell types. Here, we used expressed DNA barcodes to clonally trace transcriptomes over time and applied this to study fate determination in hematopoiesis. We identified states of primed fate potential and located them on a continuous transcriptional landscape. We identified two routes of monocyte differentiation that leave an imprint on mature cells. Analysis of sister cells also revealed cells to have intrinsic fate biases not detectable by single-cell RNA sequencing. Finally, we benchmarked computational methods of dynamic inference from single-cell snapshots, showing that fate choice occurs earlier than is detected by state-of the-art algorithms and that cells progress steadily through pseudotime with precise and consistent dynamics.
Single-cell barcoding allows the tracking of gene expression states over time as blood cells differentiate during hematopoiesis.
Single-cell barcoding allows the tracking of gene expression states over time as blood cells differentiate during hematopoiesis.},
	language = {en},
	number = {6479},
	urldate = {2020-09-28},
	journal = {Science},
	author = {Weinreb, Caleb and Rodriguez-Fraticelli, Alejo and Camargo, Fernando D. and Klein, Allon M.},
	month = feb,
	year = {2020},
	pmid = {31974159},
	note = {Publisher: American Association for the Advancement of Science
Section: Research Article},
}

@article{herman_fateid_2018,
	title = {{FateID} infers cell fate bias in multipotent progenitors from single-cell {RNA}-seq data},
	volume = {15},
	copyright = {2018 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.4662},
	doi = {10.1038/nmeth.4662},
	abstract = {FateID identifies fate biases in early multipotent progenitor populations from single-cell RNA-seq data.},
	language = {en},
	number = {5},
	urldate = {2020-09-28},
	journal = {Nature Methods},
	author = {Herman, Josip S. and Sagar and Grün, Dominic},
	month = may,
	year = {2018},
	note = {Number: 5
Publisher: Nature Publishing Group},
	pages = {379--386},
}

@article{wolf_paga_2019,
	title = {{PAGA}: graph abstraction reconciles clustering with trajectory inference through a topology preserving map of single cells},
	volume = {20},
	issn = {1474-760X},
	shorttitle = {{PAGA}},
	url = {https://doi.org/10.1186/s13059-019-1663-x},
	doi = {10.1186/s13059-019-1663-x},
	abstract = {Single-cell RNA-seq quantifies biological heterogeneity across both discrete cell types and continuous cell transitions. Partition-based graph abstraction (PAGA) provides an interpretable graph-like map of the arising data manifold, based on estimating connectivity of manifold partitions (https://github.com/theislab/paga). PAGA maps preserve the global topology of data, allow analyzing data at different resolutions, and result in much higher computational efficiency of the typical exploratory data analysis workflow. We demonstrate the method by inferring structure-rich cell maps with consistent topology across four hematopoietic datasets, adult planaria and the zebrafish embryo and benchmark computational performance on one million neurons.},
	number = {1},
	urldate = {2020-09-28},
	journal = {Genome Biology},
	author = {Wolf, F. Alexander and Hamey, Fiona K. and Plass, Mireya and Solana, Jordi and Dahlin, Joakim S. and Göttgens, Berthold and Rajewsky, Nikolaus and Simon, Lukas and Theis, Fabian J.},
	month = mar,
	year = {2019},
	pages = {59},
}

@article{dahlin_single-cell_2018,
	title = {A single-cell hematopoietic landscape resolves 8 lineage trajectories and defects in {Kit} mutant mice},
	volume = {131},
	issn = {1528-0020},
	doi = {10.1182/blood-2017-12-821413},
	abstract = {Hematopoietic stem and progenitor cells (HSPCs) maintain the adult blood system, and their dysregulation causes a multitude of diseases. However, the differentiation journeys toward specific hematopoietic lineages remain ill defined, and system-wide disease interpretation remains challenging. Here, we have profiled 44 802 mouse bone marrow HSPCs using single-cell RNA sequencing to provide a comprehensive transcriptional landscape with entry points to 8 different blood lineages (lymphoid, megakaryocyte, erythroid, neutrophil, monocyte, eosinophil, mast cell, and basophil progenitors). We identified a common basophil/mast cell bone marrow progenitor and characterized its molecular profile at the single-cell level. Transcriptional profiling of 13 815 HSPCs from the c-Kit mutant (W41/W41) mouse model revealed the absence of a distinct mast cell lineage entry point, together with global shifts in cell type abundance. Proliferative defects were accompanied by reduced Myc expression. Potential compensatory processes included upregulation of the integrated stress response pathway and downregulation of proapoptotic gene expression in erythroid progenitors, thus providing a template of how large-scale single-cell transcriptomic studies can bridge between molecular phenotypes and quantitative population changes.},
	language = {eng},
	number = {21},
	journal = {Blood},
	author = {Dahlin, Joakim S. and Hamey, Fiona K. and Pijuan-Sala, Blanca and Shepherd, Mairi and Lau, Winnie W. Y. and Nestorowa, Sonia and Weinreb, Caleb and Wolock, Samuel and Hannah, Rebecca and Diamanti, Evangelia and Kent, David G. and Göttgens, Berthold and Wilson, Nicola K.},
	year = {2018},
	pmid = {29588278},
	pmcid = {PMC5969381},
	keywords = {Animals, Bone Marrow Cells, Cell Differentiation, Cell Line, Tumor, Cell Lineage, Cells, Cultured, Gene Expression Profiling, Hematopoietic Stem Cells, Mice, Mice, Knockout, Mutation, Proto-Oncogene Proteins c-kit, Signal Transduction, Single-Cell Analysis, Transcriptome},
	pages = {e1--e11},
}

@article{herman_fateid_2018-1,
	title = {{FateID} infers cell fate bias in multipotent progenitors from single-cell {RNA}-seq data},
	volume = {15},
	number = {5},
	journal = {Nature methods},
	author = {Herman, Josip S. and Grün, Dominic},
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	pages = {379},
}

@article{balabanov_causal_2020,
	title = {Causal inference from data. {On} some inadequacy problems of structures with hidden causes},
	volume = {0},
	copyright = {Copyright (c) 2020 PROBLEMS IN PROGRAMMING},
	issn = {1727-4907},
	url = {http://pp.isofts.kiev.ua/ojs1/article/view/432},
	abstract = {The reliability of causal inference from data (by independence-based methods) is analyzed. We uncover some mechanisms which may result in model inadequacy due to sample bias and hidden variables. We detect some specific problems in recognition of direction of influence when some causes are hidden. Incorrectness of known rule for edge orientation (under causal insufficiency) is revealed. We suggest the correction to the rule aiming to retain model adequacy.Problems in programming 2020; 2-3: 392-406},
	language = {uk},
	number = {2-3},
	urldate = {2020-09-24},
	journal = {PROBLEMS IN PROGRAMMING},
	author = {Balabanov, O. S.},
	month = sep,
	year = {2020},
	note = {Number: 2-3},
	keywords = {causal network, chain, collider, conditional independence, d-separation, d-сепарация, d-сепарація, edge orientation, каузальна мережа, каузальная сеть, коллайдер, колізор, ланцюг, ориентация ребер, орієнтація ребер, умовна незалежність, условная независимость, цепь},
	pages = {392--406},
}

@techreport{cinelli_generalizing_2019,
	title = {Generalizing {Experimental} {Results} by {Leveraging} {Knowledge} of {Mechanisms}},
	institution = {Technical Report. http://ftp. cs. ucla. edu/pub/stat\_ser/r492. pdf},
	author = {Cinelli, Carlos and Pearl, Judea},
	year = {2019},
}

@misc{noauthor_pdf_nodate,
	title = {({PDF}) {Generalizing} {Experimental} {Results} by {Leveraging} {Knowledge} of {Mechanisms}},
	url = {https://www.researchgate.net/publication/338105463_Generalizing_Experimental_Results_by_Leveraging_Knowledge_of_Mechanisms},
	urldate = {2020-09-22},
}

@misc{noauthor_generalizing_nodate,
	title = {Generalizing {Experimental} {Results} {byLeveraging} {Knowledge} of {Mechanisms} - {Google} {Search}},
	url = {https://www.google.com/search?client=firefox-b-d&q=Generalizing+Experimental+Results+byLeveraging+Knowledge+of+Mechanisms},
	urldate = {2020-09-22},
}

@article{boeken_bayesian_2020,
	title = {A {Bayesian} {Nonparametric} {Conditional} {Two}-sample {Test} with an {Application} to {Local} {Causal} {Discovery}},
	url = {http://arxiv.org/abs/2008.07382},
	abstract = {The performance of constraint-based causal discovery algorithms is prominently determined by the performance of the (conditional) independence tests that are being used. A default choice for the (conditional) independence test is the (partial) correlation test, which can fail in presence of nonlinear relations between the variables. Recent research proposes a Bayesian nonparametric two-sample test (Holmes et al., 2015), an independence test between continuous variables (Filippi and Holmes, 2017), and a conditional independence test between continuous variables (Teymur and Filippi, 2019). We extend this work by proposing a novel Bayesian nonparametric conditional two-sample test. We utilise this conditional two-sample test for testing the conditional independence \$C {\textbackslash}perp {\textbackslash}!{\textbackslash}!{\textbackslash}! {\textbackslash}perp Y {\textbar}X\$ where C denotes a Bernoulli random variable, and X and Y are continuous one-dimensional random variables. This enables a nonparametric implementation of the Local Causal Discovery (LCD) algorithm with binary variables in the experimental setup (e.g. an indicator of treatment/control group). We propose a fair performance measure for comparing frequentist and Bayesian tests in the LCD setting. We utilise this performance measure for comparing our Bayesian ensemble with state-of-the-art frequentist tests, and conclude that the Bayesian ensemble has better performance than its frequentist counterparts. We apply our nonparametric implementation of the LCD algorithm to protein expression data.},
	urldate = {2020-09-10},
	journal = {arXiv:2008.07382 [math, stat]},
	author = {Boeken, Philip A. and Mooij, Joris M.},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.07382},
	keywords = {Mathematics - Statistics Theory, toread},
}

@article{benkeser_nonparametric_2020,
	title = {A {Nonparametric} {Super}-{Efficient} {Estimator} of the {Average} {Treatment} {Effect}},
	volume = {35},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/euclid.ss/1599789707},
	doi = {10.1214/19-STS735},
	abstract = {Doubly robust estimators are a popular means of estimating causal effects. Such estimators combine an estimate of the conditional mean of the outcome given treatment and confounders (the so-called outcome regression) with an estimate of the conditional probability of treatment given confounders (the propensity score) to generate an estimate of the effect of interest. In addition to enjoying the double-robustness property, these estimators have additional benefits. First, flexible regression tools, such as those developed in the field of machine learning, can be utilized to estimate the relevant regressions, while the estimators of the treatment effects retain desirable statistical properties. Furthermore, these estimators are often statistically efficient, achieving the lower bound on the variance of regular, asymptotically linear estimators. However, in spite of their asymptotic optimality, in problems where causal estimands are weakly identifiable, these estimators may behave erratically. We propose new estimation techniques for use in these challenging settings. Our estimators build on two existing frameworks for efficient estimation: targeted minimum loss estimation and one-step estimation. However, rather than using an estimate of the propensity score in their construction, we instead opt for an alternative regression quantity when building our estimators: the conditional probability of treatment given the conditional mean outcome. We discuss the theoretical implications and demonstrate the estimators’ performance in simulated and real data.},
	language = {EN},
	number = {3},
	urldate = {2020-09-16},
	journal = {Statistical Science},
	author = {Benkeser, David and Cai, Weixin and Laan, Mark J. van der},
	month = aug,
	year = {2020},
	mrnumber = {MR4148225},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Causal inference, asymptotic linearity, average treatment effect, collaborative targeted minimum loss estimation, efficient influence function, super efficiency, toread},
	pages = {484--495},
}

@inproceedings{zheng_learning_2020,
	title = {Learning sparse nonparametric {DAGs}},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Zheng, Xun and Dan, Chen and Aragam, Bryon and Ravikumar, Pradeep and Xing, Eric},
	year = {2020},
	keywords = {toread},
	pages = {3414--3425},
}

@inproceedings{zheng_dags_2018,
	title = {{DAGs} with {NO} {TEARS}: {Continuous} optimization for structure learning},
	shorttitle = {{DAGs} with {NO} {TEARS}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Zheng, Xun and Aragam, Bryon and Ravikumar, Pradeep K. and Xing, Eric P.},
	year = {2018},
	keywords = {toread},
	pages = {9472--9483},
}

@article{bregoli_constraint-based_2020,
	title = {Constraint-{Based} {Learning} for {Continuous}-{Time} {Bayesian} {Networks}},
	url = {http://arxiv.org/abs/2007.03248},
	abstract = {Dynamic Bayesian networks have been well explored in the literature as discrete-time models; however, their continuous-time extensions have seen comparatively little attention. In this paper, we propose the first constraint-based algorithm for learning the structure of continuous-time Bayesian networks. We discuss the different statistical tests and the underlying hypotheses used by our proposal to establish conditional independence. Finally, we validate its performance using synthetic data, and discuss its strengths and limitations. We find that score-based is more accurate in learning networks with binary variables, while our constraint-based approach is more accurate with variables assuming more than two values. However, more experiments are needed for confirmation.},
	urldate = {2020-09-10},
	journal = {arXiv:2007.03248 [cs, stat]},
	author = {Bregoli, Alessandro and Scutari, Marco and Stella, Fabio},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.03248},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{mandros_discovering_2020,
	address = {New York, NY, USA},
	series = {{KDD} '20},
	title = {Discovering {Functional} {Dependencies} from {Mixed}-{Type} {Data}},
	isbn = {978-1-4503-7998-4},
	url = {https://doi.org/10.1145/3394486.3403193},
	doi = {10.1145/3394486.3403193},
	abstract = {Given complex data collections, practitioners can perform non-parametric functional dependency discovery (FDD) to uncover relationships between variables that were previously unknown. However, known FDD methods are applicable to nominal data, and in practice non-nominal variables are discretized, e.g., in a pre-processing step. This is problematic because, as soon as a mix of discrete and continuous variables is involved, the interaction of discretization with the various dependency measures from the literature is poorly understood. In particular, it is unclear whether a given discretization method even leads to a consistent dependency estimate. In this paper, we analyze these fundamental questions and derive formal criteria as to when a discretization process applied to a mixed set of random variables leads to consistent estimates of mutual information. With these insights, we derive an estimator framework applicable to any task that involves estimating mutual information from multivariate and mixed-type data. Last, we extend with this framework a previously proposed FDD approach for reliable dependencies. Experimental evaluation shows that the derived reliable estimator is both computationally and statistically efficient, and leads to effective FDD algorithms for mixed-type data.},
	urldate = {2020-09-10},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Mandros, Panagiotis and Kaltenpoth, David and Boley, Mario and Vreeken, Jilles},
	month = aug,
	year = {2020},
	keywords = {functional dependency discovery, mixed data, mutual information},
	pages = {1404--1414},
}

@inproceedings{li_constraint-based_2019,
	title = {Constraint-based {Causal} {Structure} {Learning} with {Consistent} {Separating} {Sets}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Li, Honghao and Cabeli, Vincent and Sella, Nadir and Isambert, Hervé},
	year = {2019},
	keywords = {team},
	pages = {14257--14266},
}

@inproceedings{li_constraint-based_2019-1,
	title = {Constraint-based causal structure learning with consistent separating sets},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Li, Honghao and Cabeli, Vincent and Sella, Nadir and Isambert, Hervé},
	year = {2019},
	pages = {14257--14266},
}

@inproceedings{andrews_completeness_2020,
	title = {On the {Completeness} of {Causal} {Discovery} in the {Presence} of {Latent} {Confounding} with {Tiered} {Background} {Knowledge}},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Andrews, Bryan and Spirtes, Peter and Cooper, Gregory F.},
	year = {2020},
	keywords = {toread},
	pages = {4002--4011},
}

@article{strobl_improved_2019,
	title = {Improved causal discovery from longitudinal data using a mixture of {DAGs}},
	journal = {arXiv preprint arXiv:1901.09475},
	author = {Strobl, Eric V.},
	year = {2019},
	keywords = {contextual variables, toread},
}

@article{mastakouri_necessary_2020,
	title = {Necessary and sufficient conditions for causal feature selection in time series with latent common causes},
	url = {http://arxiv.org/abs/2005.08543},
	abstract = {We study the identification of direct and indirect causes on time series and provide necessary and sufficient conditions in the presence of latent variables. Our theoretical results and estimation algorithms require two conditional independence tests for each observed candidate time series to determine whether or not it is a cause of an observed target time series. We provide experimental results in simulations, where the ground truth is known, as well as in real data. Our results show that our method leads to essentially no false positives and relatively low false negative rates, even in confounded environments with non-unique lag effects, outperforming the widely used Granger causality and two more methods.},
	urldate = {2020-08-18},
	journal = {arXiv:2005.08543 [stat]},
	author = {Mastakouri, Atalanti A. and Schölkopf, Bernhard and Janzing, Dominik},
	month = jun,
	year = {2020},
	note = {arXiv: 2005.08543},
	keywords = {Statistics - Machine Learning, Statistics - Methodology, manuscrit material},
}

@article{strobl_improved_2019-1,
	title = {Improved causal discovery from longitudinal data using a mixture of {DAGs}},
	journal = {arXiv preprint arXiv:1901.09475},
	author = {Strobl, Eric V.},
	year = {2019},
}

@inproceedings{pamfil_dynotears_2020,
	title = {{DYNOTEARS}: {Structure} {Learning} from {Time}-{Series} {Data}},
	shorttitle = {{DYNOTEARS}},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Pamfil, Roxana and Sriwattanaworachai, Nisara and Desai, Shaan and Pilgerstorfer, Philip and Georgatzis, Konstantinos and Beaumont, Paul and Aragam, Bryon},
	year = {2020},
	keywords = {toread},
	pages = {1595--1605},
}

@inproceedings{andrews_completeness_2020-1,
	title = {On the {Completeness} of {Causal} {Discovery} in the {Presence} of {Latent} {Confounding} with {Tiered} {Background} {Knowledge}},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Andrews, Bryan and Spirtes, Peter and Cooper, Gregory F.},
	year = {2020},
	pages = {4002--4011},
}

@inproceedings{malinsky_causal_2018,
	title = {Causal structure learning from multivariate time series in settings with unmeasured confounding},
	booktitle = {Proceedings of 2018 {ACM} {SIGKDD} {Workshop} on {Causal} {Discovery}},
	author = {Malinsky, Daniel and Spirtes, Peter},
	year = {2018},
	keywords = {toread},
	pages = {23--47},
}

@article{runge_detecting_2019,
	title = {Detecting and quantifying causal associations in large nonlinear time series datasets},
	volume = {5},
	copyright = {Copyright © 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution License 4.0 (CC BY).. This is an open-access article distributed under the terms of the Creative Commons Attribution license, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.},
	issn = {2375-2548},
	url = {https://advances.sciencemag.org/content/5/11/eaau4996},
	doi = {10.1126/sciadv.aau4996},
	abstract = {Identifying causal relationships and quantifying their strength from observational time series data are key problems in disciplines dealing with complex dynamical systems such as the Earth system or the human body. Data-driven causal inference in such systems is challenging since datasets are often high dimensional and nonlinear with limited sample sizes. Here, we introduce a novel method that flexibly combines linear or nonlinear conditional independence tests with a causal discovery algorithm to estimate causal networks from large-scale time series datasets. We validate the method on time series of well-understood physical mechanisms in the climate system and the human heart and using large-scale synthetic datasets mimicking the typical properties of real-world data. The experiments demonstrate that our method outperforms state-of-the-art techniques in detection power, which opens up entirely new possibilities to discover and quantify causal networks from time series across a range of research fields.
A novel causal discovery method for estimating nonlinear interdependency networks from large time series datasets.
A novel causal discovery method for estimating nonlinear interdependency networks from large time series datasets.},
	language = {en},
	number = {11},
	urldate = {2020-07-15},
	journal = {Science Advances},
	author = {Runge, Jakob and Nowack, Peer and Kretschmer, Marlene and Flaxman, Seth and Sejdinovic, Dino},
	month = nov,
	year = {2019},
	pages = {eaau4996},
}

@article{mohan_graphical_2019,
	title = {Graphical {Models} for {Processing} {Missing} {Data}},
	journal = {Forthcoming, Journal of American Statistical Association (JASA)},
	author = {Mohan, Karthika and Pearl, Judea},
	year = {2019},
	keywords = {toread},
}

@article{hernan_using_2016,
	title = {Using {Big} {Data} to {Emulate} a {Target} {Trial} {When} a {Randomized} {Trial} {Is} {Not} {Available}},
	volume = {183},
	issn = {0002-9262},
	url = {https://academic.oup.com/aje/article/183/8/758/1739860},
	doi = {10.1093/aje/kwv254},
	abstract = {Abstract.  Ideally, questions about comparative effectiveness or safety would be answered using an appropriately designed and conducted randomized experiment. W},
	language = {en},
	number = {8},
	urldate = {2020-09-02},
	journal = {American Journal of Epidemiology},
	author = {Hernán, Miguel A. and Robins, James M.},
	month = apr,
	year = {2016},
	note = {Publisher: Oxford Academic},
	pages = {758--764},
}

@article{mooij_distinguishing_2016,
	title = {Distinguishing {Cause} from {Effect} {Using} {Observational} {Data}: {Methods} and {Benchmarks}},
	volume = {17},
	issn = {1533-7928},
	shorttitle = {Distinguishing {Cause} from {Effect} {Using} {Observational} {Data}},
	url = {http://jmlr.org/papers/v17/14-518.html},
	number = {32},
	urldate = {2020-09-01},
	journal = {Journal of Machine Learning Research},
	author = {Mooij, Joris M. and Peters, Jonas and Janzing, Dominik and Zscheischler, Jakob and Schölkopf, Bernhard},
	year = {2016},
	pages = {1--102},
}

@article{mooij_distinguishing_2016-1,
	title = {Distinguishing {Cause} from {Effect} {Using} {Observational} {Data}: {Methods} and {Benchmarks}},
	volume = {17},
	shorttitle = {Distinguishing {Cause} from {Effect} {Using} {Observational} {Data}},
	url = {https://dare.uva.nl/search?identifier=f4e35a2b-cd22-41a4-93e5-7f0b1cf8090a},
	number = {32},
	urldate = {2017-12-18},
	journal = {Journal of Machine Learning Research},
	author = {Mooij, J. M. and Peters, J. and Janzing, D. and Zscheischler, J. and Schölkopf, B.},
	year = {2016},
	keywords = {toread},
}

@article{pearl_3_2010,
	title = {3. {The} {Foundations} of {Causal} {Inference}},
	volume = {40},
	issn = {0081-1750, 1467-9531},
	url = {http://journals.sagepub.com/doi/10.1111/j.1467-9531.2010.01228.x},
	doi = {10.1111/j.1467-9531.2010.01228.x},
	language = {en},
	number = {1},
	urldate = {2020-09-01},
	journal = {Sociological Methodology},
	author = {Pearl, Judea},
	month = aug,
	year = {2010},
	keywords = {toread},
	pages = {75--149},
}

@article{sesia_controlling_2020,
	title = {Controlling the false discovery rate in {GWAS} with population structure},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.08.04.236703v1},
	doi = {10.1101/2020.08.04.236703},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}p{\textgreater}This paper proposes a novel statistical method to address population structure in genome-wide association studies while controlling the false discovery rate, which overcomes some limitations of existing approaches. Our solution accounts for linkage disequilibrium and diverse ancestries by combining conditional testing via knockoffs with hidden Markov models from state-of-the-art phasing methods. Furthermore, we account for familial relatedness by describing the joint distribution of haplotypes sharing long identical-by-descent segments with a generalized hidden Markov model. Extensive simulations affirm the validity of this method, while applications to UK Biobank phenotypes yield many more discoveries compared to BOLT-LMM, most of which are confirmed by the Japan Biobank and FinnGen data.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2020-08-31},
	journal = {bioRxiv},
	author = {Sesia, Matteo and Bates, Stephen and Candès, Emmanuel and Marchini, Jonathan and Sabatti, Chiara},
	month = aug,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	keywords = {toread},
	pages = {2020.08.04.236703},
}

@misc{noauthor_controlling_nodate,
	title = {Controlling the false discovery rate in {GWAS} with population structure {\textbar} {bioRxiv}},
	url = {https://www.biorxiv.org/content/10.1101/2020.08.04.236703v1},
	urldate = {2020-08-31},
}

@article{wang_blessings_2018,
	title = {The {Blessings} of {Multiple} {Causes}},
	url = {http://arxiv.org/abs/1805.06826},
	abstract = {Causal inference from observational data often assumes "strong ignorability," that all confounders are observed. This assumption is standard yet untestable. However, many scientific studies involve multiple causes, different variables whose effects are simultaneously of interest. We propose the deconfounder, an algorithm that combines unsupervised machine learning and predictive model checking to perform causal inference in multiple-cause settings. The deconfounder infers a latent variable as a substitute for unobserved confounders and then uses that substitute to perform causal inference. We develop theory for when the deconfounder leads to unbiased causal estimates, and show that it requires weaker assumptions than classical causal inference. We analyze its performance in three types of studies: semi-simulated data around smoking and lung cancer, semi-simulated data around genomewide association studies, and a real dataset about actors and movie revenue. The deconfounder provides a checkable approach to estimating close-to-truth causal effects.},
	urldate = {2018-09-06},
	journal = {arXiv:1805.06826 [cs, stat]},
	author = {Wang, Yixin and Blei, David M.},
	month = may,
	year = {2018},
	note = {arXiv: 1805.06826},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology, toread},
}

@article{forre_constraint-based_2018,
	title = {Constraint-based {Causal} {Discovery} for {Non}-{Linear} {Structural} {Causal} {Models} with {Cycles} and {Latent} {Confounders}},
	url = {http://arxiv.org/abs/1807.03024},
	abstract = {We address the problem of causal discovery from data, making use of the recently proposed causal modeling framework of modular structural causal models (mSCM) to handle cycles, latent confounders and non-linearities. We introduce \{{\textbackslash}sigma\}-connection graphs (\{{\textbackslash}sigma\}-CG), a new class of mixed graphs (containing undirected, bidirected and directed edges) with additional structure, and extend the concept of \{{\textbackslash}sigma\}-separation, the appropriate generalization of the well-known notion of d-separation in this setting, to apply to \{{\textbackslash}sigma\}-CGs. We prove the closedness of \{{\textbackslash}sigma\}-separation under marginalisation and conditioning and exploit this to implement a test of \{{\textbackslash}sigma\}-separation on a \{{\textbackslash}sigma\}-CG. This then leads us to the first causal discovery algorithm that can handle non-linear functional relations, latent confounders, cyclic causal relationships, and data from different (stochastic) perfect interventions. As a proof of concept, we show on synthetic data how well the algorithm recovers features of the causal graph of modular structural causal models.},
	urldate = {2018-10-03},
	journal = {arXiv:1807.03024 [cs, stat]},
	author = {Forré, Patrick and Mooij, Joris M.},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.03024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, toread},
}

@article{lederer_control_2019,
	title = {Control of confounding and reporting of results in causal inference studies. {Guidance} for authors from editors of respiratory, sleep, and critical care journals},
	volume = {16},
	number = {1},
	journal = {Annals of the American Thoracic Society},
	author = {Lederer, David J. and Bell, Scott C. and Branson, Richard D. and Chalmers, James D. and Marshall, Rachel and Maslove, David M. and Ost, David E. and Punjabi, Naresh M. and Schatz, Michael and Smyth, Alan R.},
	year = {2019},
	keywords = {toread},
	pages = {22--28},
}

@article{zhang_simultaneous_2019,
	title = {A {Simultaneous} {Discover}-{Identify} {Approach} to {Causal} {Inference} in {Linear} {Models}},
	author = {Zhang, Chi and Chen, Bryant and Pearl, Judea},
	year = {2019},
	keywords = {toread},
}

@article{dickerman_avoidable_2019,
	title = {Avoidable flaws in observational analyses: an application to statins and cancer},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-170X},
	shorttitle = {Avoidable flaws in observational analyses},
	url = {https://www.nature.com/articles/s41591-019-0597-x},
	doi = {10.1038/s41591-019-0597-x},
	abstract = {The increasing availability of large healthcare databases is fueling an intense debate on whether real-world data should play a role in the assessment of the benefit–risk of medical treatments. In many observational studies, for example, statin users were found to have a substantially lower risk of cancer than in meta-analyses of randomized trials. Although such discrepancies are often attributed to a lack of randomization in the observational studies, they might be explained by flaws that can be avoided by explicitly emulating a target trial (the randomized trial that would answer the question of interest). Using the electronic health records of 733,804 UK adults, we emulated a target trial of statins and cancer and compared our estimates with those obtained using previously applied analytic approaches. Over the 10-yr follow-up, 28,408 individuals developed cancer. Under the target trial approach, estimated observational analogs of intention-to-treat and per-protocol 10-yr cancer-free survival differences were −0.5\% (95\% confidence interval (CI) −1.0\%, 0.0\%) and −0.3\% (95\% CI −1.5\%, 0.5\%), respectively. By contrast, previous analytic approaches yielded estimates that appeared to be strongly protective. Our findings highlight the importance of explicitly emulating a target trial to reduce bias in the effect estimates derived from observational analyses. Explicitly emulating a target trial using real-world data can avoid common biases in observational studies.},
	language = {en},
	urldate = {2019-10-09},
	journal = {Nature Medicine},
	author = {Dickerman, Barbra A. and García-Albéniz, Xabier and Logan, Roger W. and Denaxas, Spiros and Hernán, Miguel A.},
	month = oct,
	year = {2019},
	keywords = {toread},
	pages = {1--6},
}

@article{wong_computational_2020,
	title = {Computational {Causal} {Inference}},
	url = {http://arxiv.org/abs/2007.10979},
	abstract = {We introduce computational causal inference as an interdisciplinary field across causal inference, algorithms design and numerical computing. The field aims to develop software specializing in causal inference that can analyze massive datasets with a variety of causal effects, in a performant, general, and robust way. The focus on software improves research agility, and enables causal inference to be easily integrated into large engineering systems. In particular, we use computational causal inference to deepen the relationship between causal inference, online experimentation, and algorithmic decision making. This paper describes the new field, the demand, opportunities for scalability, open challenges, and begins the discussion for how the community can unite to solve challenges for scaling causal inference and decision making.},
	urldate = {2020-08-31},
	journal = {arXiv:2007.10979 [cs, stat]},
	author = {Wong, Jeffrey C.},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.10979},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, toread},
}

@article{scholkopf_causality_2019,
	title = {Causality for {Machine} {Learning}},
	url = {http://arxiv.org/abs/1911.10500},
	abstract = {Graphical causal inference as pioneered by Judea Pearl arose from research on artificial intelligence (AI), and for a long time had little connection to the field of machine learning. This article discusses where links have been and should be established, introducing key concepts along the way. It argues that the hard open problems of machine learning and AI are intrinsically related to causality, and explains how the field is beginning to understand them.},
	urldate = {2020-07-15},
	journal = {arXiv:1911.10500 [cs, stat]},
	author = {Schölkopf, Bernhard},
	month = dec,
	year = {2019},
	note = {arXiv: 1911.10500},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2, I.2, I.5, K.4, I.5, K.4, Statistics - Machine Learning, toread},
}

@article{papana_evaluation_2009,
	title = {Evaluation of {Mutual} {Information} {Estimators} for {Time} {Series}},
	volume = {19},
	issn = {0218-1274, 1793-6551},
	url = {http://arxiv.org/abs/0904.4753},
	doi = {10.1142/S0218127409025298},
	abstract = {We study some of the most commonly used mutual information estimators, based on histograms of fixed or adaptive bin size, \$k\$-nearest neighbors and kernels, and focus on optimal selection of their free parameters. We examine the consistency of the estimators (convergence to a stable value with the increase of time series length) and the degree of deviation among the estimators. The optimization of parameters is assessed by quantifying the deviation of the estimated mutual information from its true or asymptotic value as a function of the free parameter. Moreover, some common-used criteria for parameter selection are evaluated for each estimator. The comparative study is based on Monte Carlo simulations on time series from several linear and nonlinear systems of different lengths and noise levels. The results show that the \$k\$-nearest neighbor is the most stable and less affected by the method-specific parameter. A data adaptive criterion for optimal binning is suggested for linear systems but it is found to be rather conservative for nonlinear systems. It turns out that the binning and kernel estimators give the least deviation in identifying the lag of the first minimum of mutual information from nonlinear systems, and are stable in the presence of noise.},
	number = {12},
	urldate = {2020-07-15},
	journal = {International Journal of Bifurcation and Chaos},
	author = {Papana, Angeliki and Kugiumtzis, Dimitris},
	month = dec,
	year = {2009},
	note = {arXiv: 0904.4753},
	keywords = {Physics - Data Analysis, Statistics and Probability, manuscrit material},
	pages = {4197--4215},
}

@article{papana_simulation_2013,
	title = {Simulation study of direct causality measures in multivariate time series},
	volume = {15},
	number = {7},
	journal = {Entropy},
	author = {Papana, Angeliki and Kyrtsou, Catherine and Kugiumtzis, Dimitris and Diks, Cees},
	year = {2013},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {toread},
	pages = {2635--2661},
}

@article{gerhardus_high-recall_2020,
	title = {High-recall causal discovery for autocorrelated time series with latent confounders},
	url = {http://arxiv.org/abs/2007.01884},
	abstract = {We present a new method for linear and nonlinear, lagged and contemporaneous constraint-based causal discovery from observational time series in the presence of latent confounders. We show that existing causal discovery methods such as FCI and variants suffer from low recall in the autocorrelated time series case and identify low effect size of conditional independence tests as the main reason. Information-theoretical arguments show that effect size can often be increased if causal parents are included in the conditioning sets. To identify parents early on, we suggest an iterative procedure that utilizes novel orientation rules to determine ancestral relationships already during the edge removal phase. We prove that the method is order-independent, and sound and complete in the oracle case. Extensive simulation studies for different numbers of variables, time lags, sample sizes, and further cases demonstrate that our method indeed achieves much higher recall than existing methods while keeping false positives at the desired level. This performance gain grows with stronger autocorrelation. Our method also covers causal discovery for non-time series data as a special case. We provide Python code for all methods involved in the simulation studies.},
	urldate = {2020-08-20},
	journal = {arXiv:2007.01884 [cs, stat]},
	author = {Gerhardus, Andreas and Runge, Jakob},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.01884},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology, toread},
}

@inproceedings{malinsky_causal_2018-1,
	title = {Causal structure learning from multivariate time series in settings with unmeasured confounding},
	booktitle = {Proceedings of 2018 {ACM} {SIGKDD} {Workshop} on {Causal} {Discovery}},
	author = {Malinsky, Daniel and Spirtes, Peter},
	year = {2018},
	keywords = {toread},
	pages = {23--47},
}

@article{bhave_information_2020,
	title = {Information {Theoretic} {Approaches} for {Testing} {Missingness} in {Predictive} {Models}},
	url = {https://openreview.net/forum?id=6Y05VJfGlFM},
	abstract = {In predictive modeling, missing data can often result in learning biased models despite application of imputation approaches. Therefore, it is important to assess the missingness process of the...},
	urldate = {2020-07-15},
	author = {Bhave, Shreyas A. and Ranganath, Rajesh and Perotte, Adler},
	month = jun,
	year = {2020},
	keywords = {toread},
}

@article{berrett_nonparametric_2019,
	title = {Nonparametric independence testing via mutual information},
	volume = {106},
	issn = {0006-3444},
	url = {https://academic.oup.com/biomet/article/106/3/547/5511208},
	doi = {10.1093/biomet/asz024},
	abstract = {Summary.  We propose a test of independence of two multivariate random vectors, given a sample from the underlying population. Our approach is based on the esti},
	language = {en},
	number = {3},
	urldate = {2020-07-15},
	journal = {Biometrika},
	author = {Berrett, T. B. and Samworth, R. J.},
	month = sep,
	year = {2019},
	keywords = {toread},
	pages = {547--566},
}

@article{molavipour_neural_2020,
	title = {On {Neural} {Estimators} for {Conditional} {Mutual} {Information} {Using} {Nearest} {Neighbors} {Sampling}},
	url = {http://arxiv.org/abs/2006.07225},
	abstract = {The estimation of mutual information (MI) or conditional mutual information (CMI) from a set of samples is a long-standing problem. A recent line of work in this area has leveraged the approximation power of artificial neural networks and has shown improvements over conventional methods. One important challenge in this new approach is the need to obtain, given the original dataset, a different set where the samples are distributed according to a specific product density function. This is particularly challenging when estimating CMI. In this paper, we introduce a new technique, based on k nearest neighbors (k-NN), to perform the resampling and derive high-confidence concentration bounds for the sample average. Then the technique is employed to train a neural network classifier and the CMI is estimated accordingly. We propose three estimators using this technique and prove their consistency, make a comparison between them and similar approaches in the literature, and experimentally show improvements in estimating the CMI in terms of accuracy and variance of the estimators.},
	urldate = {2020-07-15},
	journal = {arXiv:2006.07225 [cs, math]},
	author = {Molavipour, Sina and Bassi, Germán and Skoglund, Mikael},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.07225},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, toread},
}

@article{cui_novel_2019,
	title = {A novel {Bayesian} approach for latent variable modeling from mixed data with missing values},
	volume = {29},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-018-09849-7},
	doi = {10.1007/s11222-018-09849-7},
	abstract = {We consider the problem of learning parameters of latent variable models from mixed (continuous and ordinal) data with missing values. We propose a novel Bayesian Gaussian copula factor (BGCF) approach that is proven to be consistent when the data are missing completely at random (MCAR) and that is empirically quite robust when the data are missing at random, a less restrictive assumption than MCAR. In simulations, BGCF substantially outperforms two state-of-the-art alternative approaches. An illustration on the ‘Holzinger \& Swineford 1939’ dataset indicates that BGCF is favorable over the so-called robust maximum likelihood.},
	language = {en},
	number = {5},
	urldate = {2020-07-15},
	journal = {Statistics and Computing},
	author = {Cui, Ruifei and Bucur, Ioan Gabriel and Groot, Perry and Heskes, Tom},
	month = sep,
	year = {2019},
	keywords = {toread},
	pages = {977--993},
}

@article{gretton_consistent_2010,
	title = {Consistent {Nonparametric} {Tests} of {Independence}},
	volume = {11},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v11/gretton10a.html},
	number = {46},
	urldate = {2020-07-17},
	journal = {Journal of Machine Learning Research},
	author = {Gretton, Arthur and Györfi, László},
	year = {2010},
	keywords = {toread},
	pages = {1391--1423},
}

@article{heller_consistent_2016,
	title = {Consistent {Distribution}-{Free} \${K}\$-{Sample} and {Independence} {Tests} for {Univariate} {Random} {Variables}},
	volume = {17},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v17/14-441.html},
	number = {29},
	urldate = {2020-07-17},
	journal = {Journal of Machine Learning Research},
	author = {Heller, Ruth and Heller, Yair and Kaufman, Shachar and Brill, Barak and Gorfine, Malka},
	year = {2016},
	keywords = {toread},
	pages = {1--54},
}

@article{berrett_nonparametric_2017,
	title = {Nonparametric independence testing via mutual information},
	url = {http://arxiv.org/abs/1711.06642},
	abstract = {We propose a test of independence of two multivariate random vectors, given a sample from the underlying population. Our approach, which we call MINT, is based on the estimation of mutual information, whose decomposition into joint and marginal entropies facilitates the use of recently-developed efficient entropy estimators derived from nearest neighbour distances. The proposed critical values, which may be obtained from simulation (in the case where one marginal is known) or resampling, guarantee that the test has nominal size, and we provide local power analyses, uniformly over classes of densities whose mutual information satisfies a lower bound. Our ideas may be extended to provide a new goodness-of-fit tests of normal linear models based on assessing the independence of our vector of covariates and an appropriately-defined notion of an error vector. The theory is supported by numerical studies on both simulated and real data.},
	urldate = {2020-07-17},
	journal = {arXiv:1711.06642 [cs, math, stat]},
	author = {Berrett, Thomas B. and Samworth, Richard J.},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.06642},
	keywords = {62G10, Computer Science - Information Theory, Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Methodology, toread},
}

@article{tu_causal_2020,
	title = {Causal {Discovery} in the {Presence} of {Missing} {Values} for {Neuropathic} {Pain} {Diagnosis}},
	url = {https://openreview.net/forum?id=ouMvHDxAFQ},
	abstract = {The missing data issue is a common phenomenon in many applications such as healthcare. When applying causal discovery algorithms, such as PC, to a data set with missing values, not properly...},
	urldate = {2020-07-17},
	author = {Tu, Ruibo and Zhang, Kun and Bertilson, Bo Christer and Glymour, Clark and Kjellstr\&\#xF6, Hedvig and m and Zhang, Cheng},
	month = may,
	year = {2020},
	keywords = {toread},
}

@article{tu_causal_2020-1,
	title = {Causal {Discovery} in the {Presence} of {Missing} {Data}},
	url = {http://arxiv.org/abs/1807.04010},
	abstract = {Missing data are ubiquitous in many domains including healthcare. When these data entries are not missing completely at random, the (conditional) independence relations in the observed data may be different from those in the complete data generated by the underlying causal process. Consequently, simply applying existing causal discovery methods to the observed data may lead to wrong conclusions. In this paper, we aim at developing a causal discovery method to recover the underlying causal structure from observed data that follow different missingness mechanisms, including missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). With missingness mechanisms represented by missingness graphs, we analyse conditions under which additional correction is needed to derive conditional independence/dependence relations in the complete data. Based on our analysis, we propose the Missing Value PC (MVPC) algorithm for both continuous and binary variables, which extends the PC algorithm to incorporate additional corrections. Our proposed MVPC is shown in theory to give asymptotically correct results even on data that are MAR or MNAR. Experimental results on synthetic data show that the proposed algorithm is able to find correct causal relations even in the general case of MNAR. Moreover, we create a neuropathic pain diagnostic simulator for evaluating causal discovery methods. Evaluated on such simulated neuropathic pain diagnosis records and the other two real world applications, MVPC outperforms the other benchmark methods.},
	urldate = {2020-07-17},
	journal = {arXiv:1807.04010 [cs, stat]},
	author = {Tu, Ruibo and Zhang, Kun and Ackermann, Paul and Bertilson, Bo Christer and Glymour, Clark and Kjellström, Hedvig and Zhang, Cheng},
	month = jul,
	year = {2020},
	note = {arXiv: 1807.04010},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, toread},
}

@article{christiansen_towards_2020,
	title = {Towards {Causal} {Inference} for {Spatio}-{Temporal} {Data}: {Conflict} and {Forest} {Loss} in {Colombia}},
	shorttitle = {Towards {Causal} {Inference} for {Spatio}-{Temporal} {Data}},
	url = {http://arxiv.org/abs/2005.08639},
	abstract = {In many data scientific problems, we are interested not only in modeling the behaviour of a system that is passively observed, but also in inferring how the system reacts to changes in the data generating mechanism. Given knowledge of the underlying causal structure, such behaviour can be estimated from purely observational data. To do so, one typically assumes that the causal structure of the data generating mechanism can be fully specified. Furthermore, many methods assume that data are generated as independent replications from that mechanism. Both of these assumptions are usually hard to justify in practice: datasets often have complex dependence structures, as is the case for spatio-temporal data, and the full causal structure between all involved variables is hardly known. Here, we present causal models that are adapted to the characteristics of spatio-temporal data, and which allow us to define and quantify causal effects despite incomplete causal background knowledge. We further introduce a simple approach for estimating causal effects, and a non-parametric hypothesis test for these effects being zero. The proposed methods do not rely on any distributional assumptions on the data, and allow for arbitrarily many latent confounders, given that these confounders do not vary across time (or, alternatively, they do not vary across space). Our theoretical findings are supported by simulations and code is available online. This work has been motivated by the following real-world question: how has the Colombian conflict influenced tropical forest loss? There is evidence for both enhancing and reducing impacts, but most literature analyzing this problem is not using formal causal methodology. When applying our method to data from 2000 to 2018, we find a reducing but insignificant causal effect of conflict on forest loss. Regionally, both enhancing and reducing effects can be identified.},
	urldate = {2020-07-15},
	journal = {arXiv:2005.08639 [stat]},
	author = {Christiansen, Rune and Baumann, Matthias and Kuemmerle, Tobias and Mahecha, Miguel D. and Peters, Jonas},
	month = jun,
	year = {2020},
	note = {arXiv: 2005.08639},
	keywords = {Statistics - Methodology, toread},
}

@book{azadkia_nonparametric_2020,
	title = {A {Nonparametric} {Measure} of {Conditional} {Dependence}},
	abstract = {There are numerous problems where one needs to quantify the dependence between two random variables and how this dependence changes by conditioning on a third random variable. Correlated random variables might become independent when we observe a third random variable or two independent random variables might become dependent after conditioning on the third one. Thanks to the wide potential application range e.g., bioinformatics, economics, psychology, etc, finding efficient measures of conditional dependence has been an active area of research in many subareas of statistics and machine learning. However, the literature on measures of conditional dependence is not so large, especially in the non-parametric setting. We introduce two novel measures of conditional dependence, and propose estimators based on i.i.d. samples. Using these statistics, we devise a new variable selection algorithm, called Feature Ordering by Conditional Independence (FOCI). FOCI is model-free with no tuning parameters and is provably consistent under sparsity assumptions. We provide a number of example application analyses to both synthetic and real datasets.},
	language = {en},
	publisher = {Stanford University},
	author = {Azadkia, Mona},
	year = {2020},
	note = {Google-Books-ID: k6SqzQEACAAJ},
}

@article{lizier_jidt_2014,
	title = {{JIDT}: {An} information-theoretic toolkit for studying the dynamics of complex systems},
	volume = {1},
	shorttitle = {{JIDT}},
	journal = {Frontiers in Robotics and AI},
	author = {Lizier, Joseph T.},
	year = {2014},
	pages = {11},
}

@article{albanese_minerva_2013,
	title = {Minerva and minepy: a {C} engine for the {MINE} suite and its {R}, {Python} and {MATLAB} wrappers},
	volume = {29},
	issn = {1367-4811},
	shorttitle = {Minerva and minepy},
	doi = {10.1093/bioinformatics/bts707},
	abstract = {We introduce a novel implementation in ANSI C of the MINE family of algorithms for computing maximal information-based measures of dependence between two variables in large datasets, with the aim of a low memory footprint and ease of integration within bioinformatics pipelines. We provide the libraries minerva (with the R interface) and minepy for Python, MATLAB, Octave and C++. The C solution reduces the large memory requirement of the original Java implementation, has good upscaling properties and offers a native parallelization for the R interface. Low memory requirements are demonstrated on the MINE benchmarks as well as on large ( = 1340) microarray and Illumina GAII RNA-seq transcriptomics datasets.
AVAILABILITY AND IMPLEMENTATION: Source code and binaries are freely available for download under GPL3 licence at http://minepy.sourceforge.net for minepy and through the CRAN repository http://cran.r-project.org for the R package minerva. All software is multiplatform (MS Windows, Linux and OSX).},
	language = {eng},
	number = {3},
	journal = {Bioinformatics (Oxford, England)},
	author = {Albanese, Davide and Filosi, Michele and Visintainer, Roberto and Riccadonna, Samantha and Jurman, Giuseppe and Furlanello, Cesare},
	month = feb,
	year = {2013},
	pmid = {23242262},
	keywords = {Algorithms, Computational Biology, Data Mining, Gene Expression Profiling, Metagenome, Software},
	pages = {407--408},
}

@article{zoppoli_timedelay-aracne_2010,
	title = {{TimeDelay}-{ARACNE}: {Reverse} engineering of gene networks from time-course data by an information theoretic approach},
	volume = {11},
	issn = {1471-2105},
	shorttitle = {{TimeDelay}-{ARACNE}},
	url = {https://doi.org/10.1186/1471-2105-11-154},
	doi = {10.1186/1471-2105-11-154},
	abstract = {One of main aims of Molecular Biology is the gain of knowledge about how molecular components interact each other and to understand gene function regulations. Using microarray technology, it is possible to extract measurements of thousands of genes into a single analysis step having a picture of the cell gene expression. Several methods have been developed to infer gene networks from steady-state data, much less literature is produced about time-course data, so the development of algorithms to infer gene networks from time-series measurements is a current challenge into bioinformatics research area. In order to detect dependencies between genes at different time delays, we propose an approach to infer gene regulatory networks from time-series measurements starting from a well known algorithm based on information theory.},
	number = {1},
	urldate = {2020-07-15},
	journal = {BMC Bioinformatics},
	author = {Zoppoli, Pietro and Morganella, Sandro and Ceccarelli, Michele},
	month = mar,
	year = {2010},
	pages = {154},
}

@article{holmes_estimation_2019,
	title = {Estimation of mutual information for real-valued data with error bars and controlled bias},
	copyright = {© 2019, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/589929v2-0},
	doi = {10.1101/589929},
	abstract = {{\textless}p{\textgreater}Estimation of mutual information between (multidimensional) real-valued variables is used in analysis of complex systems, biological systems, and recently also quantum systems. This estimation is a hard problem, and universally good estimators provably do not exist. Kraskov et al. (PRE, 2004) introduced a successful mutual information estimation approach based on the statistics of distances between neighboring data points, which empirically works for a wide class of underlying probability distributions. Here we improve this estimator by (i) expanding its range of applicability, and by providing (ii) a self-consistent way of verifying the absence of bias, (iii) a method for estimation of its variance, and (iv) a criterion for choosing the values of the free parameter of the estimator. We demonstrate the performance of our estimator on synthetic data sets, as well as on neurophysiological and systems biology data sets.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2019-05-29},
	journal = {bioRxiv},
	author = {Holmes, Caroline M. and Nemenman, Ilya},
	month = mar,
	year = {2019},
	pages = {589929},
}

@article{nguyen_dissecting_2018,
	title = {Dissecting effects of anti-cancer drugs and cancer-associated fibroblasts by on-chip reconstitution of immunocompetent tumor microenvironments},
	volume = {25},
	number = {13},
	journal = {Cell reports},
	author = {Nguyen, Marie and De Ninno, Adele and Mencattini, Arianna and Mermet-Meillon, Fanny and Fornabaio, Giulia and Evans, Sophia S. and Cossutta, Mélissande and Khira, Yasmine and Han, Weijing and Sirven, Philémon},
	year = {2018},
	pages = {3884--3893},
}

@article{fiford_white_2017,
	title = {White matter hyperintensities are associated with disproportionate progressive hippocampal atrophy},
	volume = {27},
	issn = {1098-1063},
	doi = {10.1002/hipo.22690},
	abstract = {This study investigates relationships between white matter hyperintensity (WMH) volume, cerebrospinal fluid (CSF) Alzheimer's disease (AD) pathology markers, and brain and hippocampal volume loss. Subjects included 198 controls, 345 mild cognitive impairment (MCI), and 154 AD subjects with serial volumetric 1.5-T MRI. CSF Aβ42 and total tau were measured (n = 353). Brain and hippocampal loss were quantified from serial MRI using the boundary shift integral (BSI). Multiple linear regression models assessed the relationships between WMHs and hippocampal and brain atrophy rates. Models were refitted adjusting for (a) concurrent brain/hippocampal atrophy rates and (b) CSF Aβ42 and tau in subjects with CSF data. WMH burden was positively associated with hippocampal atrophy rate in controls (P = 0.002) and MCI subjects (P = 0.03), and with brain atrophy rate in controls (P = 0.03). The associations with hippocampal atrophy rate remained following adjustment for concurrent brain atrophy rate in controls and MCIs, and for CSF biomarkers in controls (P = 0.007). These novel results suggest that vascular damage alongside AD pathology is associated with disproportionately greater hippocampal atrophy in nondemented older adults. © 2016 The Authors Hippocampus Published by Wiley Periodicals, Inc.},
	language = {eng},
	number = {3},
	journal = {Hippocampus},
	author = {Fiford, Cassidy M. and Manning, Emily N. and Bartlett, Jonathan W. and Cash, David M. and Malone, Ian B. and Ridgway, Gerard R. and Lehmann, Manja and Leung, Kelvin K. and Sudre, Carole H. and Ourselin, Sebastien and Biessels, Geert Jan and Carmichael, Owen T. and Fox, Nick C. and Cardoso, M. Jorge and Barnes, Josephine and {Alzheimer's Disease Neuroimaging Initiative}},
	year = {2017},
	pmid = {27933676},
	pmcid = {PMC5324634},
	keywords = {Aged, Aging, Alzheimer Disease, Alzheimer's disease, Amyloid beta-Peptides, Atrophy, Biomarkers, Cognitive Dysfunction, Disease Progression, Female, Follow-Up Studies, Hippocampus, Humans, Image Processing, Computer-Assisted, Linear Models, Longitudinal Studies, Magnetic Resonance Imaging, Male, Organ Size, Peptide Fragments, White Matter, hippocampus, mild cognitive impairment, vascular disease, white matter disease, white matter hyperintensity (WMH)},
	pages = {249--262},
}

@article{prins_white_2015,
	title = {White matter hyperintensities, cognitive impairment and dementia: an update},
	volume = {11},
	issn = {1759-4766},
	shorttitle = {White matter hyperintensities, cognitive impairment and dementia},
	doi = {10.1038/nrneurol.2015.10},
	abstract = {White matter hyperintensities (WMHs) in the brain are the consequence of cerebral small vessel disease, and can easily be detected on MRI. Over the past three decades, research has shown that the presence and extent of white matter hyperintense signals on MRI are important for clinical outcome, in terms of cognitive and functional impairment. Large, longitudinal population-based and hospital-based studies have confirmed a dose-dependent relationship between WMHs and clinical outcome, and have demonstrated a causal link between large confluent WMHs and dementia and disability. Adequate differential diagnostic assessment and management is of the utmost importance in any patient, but most notably those with incipient cognitive impairment. Novel imaging techniques such as diffusion tensor imaging might reveal subtle damage before it is visible on standard MRI. Even in Alzheimer disease, which is thought to be primarily caused by amyloid, vascular pathology, such as small vessel disease, may be of greater importance than amyloid itself in terms of influencing the disease course, especially in older individuals. Modification of risk factors for small vessel disease could be an important therapeutic goal, although evidence for effective interventions is still lacking. Here, we provide a timely Review on WMHs, including their relationship with cognitive decline and dementia.},
	language = {eng},
	number = {3},
	journal = {Nature Reviews. Neurology},
	author = {Prins, Niels D. and Scheltens, Philip},
	month = mar,
	year = {2015},
	pmid = {25686760},
	keywords = {Brain, Cognition Disorders, Dementia, Humans, Leukoencephalopathies, Magnetic Resonance Imaging},
	pages = {157--165},
}

@article{cabeli_learning_2020,
	title = {Learning clinical networks from medical recordsbased on information estimates in mixed-type data},
	journal = {PLoS Computational Biology},
	author = {Cabeli, Vincent and Verny, Louis and Sella, Nadir and Uguzzoni, Guido and Verny, Marc and Isambert, Hervé},
	year = {2020},
	keywords = {team},
}

@misc{noauthor_paper_nodate,
	title = {paper gradient boosting - {Google} {Search}},
	url = {https://www.google.com/search?client=firefox-b-d&q=paper+gradient+boosting},
	urldate = {2020-01-17},
}

@inproceedings{ke_lightgbm_2017,
	title = {Lightgbm: {A} highly efficient gradient boosting decision tree},
	shorttitle = {Lightgbm},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
	year = {2017},
	pages = {3146--3154},
}

@inproceedings{chen_xgboost_2016,
	title = {Xgboost: {A} scalable tree boosting system},
	shorttitle = {Xgboost},
	booktitle = {Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining},
	publisher = {ACM},
	author = {Chen, Tianqi and Guestrin, Carlos},
	year = {2016},
	pages = {785--794},
}

@article{strobl_approximate_2017,
	title = {Approximate {Kernel}-{Based} {Conditional} {Independence} {Tests} for {Fast} {Non}-{Parametric} {Causal} {Discovery}},
	volume = {7},
	issn = {2193-3685},
	url = {https://www.degruyter.com/view/j/jci.2019.7.issue-1/jci-2018-0017/jci-2018-0017.xml?format=INT},
	doi = {10.1515/jci-2018-0017},
	abstract = {Constraint-based causal discovery (CCD) algorithms require fast and accurate conditional independence (CI) testing. The Kernel Conditional Independence Test (KCIT) is currently one of the most popular CI tests in the non-parametric setting, but many investigators cannot use KCIT with large datasets because the test scales at least quadratically with sample size. We therefore devise two relaxations called the Randomized Conditional Independence Test (RCIT) and the Randomized conditional Correlation Test (RCoT) which both approximate KCIT by utilizing random Fourier features. In practice, both of the proposed tests scale linearly with sample size and return accurate p-values much faster than KCIT in the large sample size context. CCD algorithms run with RCIT or RCoT also return graphs at least as accurate as the same algorithms run with KCIT but with large reductions in run time.},
	number = {1},
	urldate = {2019-05-13},
	journal = {Journal of Causal Inference},
	author = {Strobl, Eric V. and Zhang, Kun and Visweswaran, Shyam},
	year = {2017},
	keywords = {Causal Discovery, Conditional Independence Test, Non-Parametric, Random Fourier Features},
}

@article{strobl_approximate_2019,
	title = {Approximate kernel-based conditional independence tests for fast non-parametric causal discovery},
	volume = {7},
	number = {1},
	journal = {Journal of Causal Inference},
	author = {Strobl, Eric V. and Zhang, Kun and Visweswaran, Shyam},
	year = {2019},
}

@article{zhang_kernel-based_2012,
	title = {Kernel-based conditional independence test and application in causal discovery},
	journal = {arXiv preprint arXiv:1202.3775},
	author = {Zhang, Kun and Peters, Jonas and Janzing, Dominik and Schölkopf, Bernhard},
	year = {2012},
}

@inproceedings{runge_conditional_2018,
	title = {Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information},
	url = {http://proceedings.mlr.press/v84/runge18a.html},
	abstract = {Conditional independence testing is a fundamental problem underlying causal discovery and a particularly challenging task in the presence of nonlinear dependencies. Here a fully non-parametric test...},
	language = {en},
	urldate = {2019-12-31},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Runge, Jakob},
	month = mar,
	year = {2018},
	pages = {938--947},
}

@book{spirtes_causation_2000,
	title = {Causation, prediction, and search},
	publisher = {MIT press},
	author = {Spirtes, Peter and Glymour, Clark N. and Scheines, Richard and Heckerman, David and Meek, Christopher and Cooper, Gregory and Richardson, Thomas},
	year = {2000},
}

@article{rojas-carulla_causal_2017,
	title = {Causal {Discovery} {Using} {Proxy} {Variables}},
	url = {http://arxiv.org/abs/1702.07306},
	abstract = {Discovering causal relations is fundamental to reasoning and intelligence. In particular, observational causal discovery algorithms estimate the cause-effect relation between two random entities \$X\$ and \$Y\$, given \$n\$ samples from \$P(X,Y)\$. In this paper, we develop a framework to estimate the cause-effect relation between two static entities \$x\$ and \$y\$: for instance, an art masterpiece \$x\$ and its fraudulent copy \$y\$. To this end, we introduce the notion of proxy variables, which allow the construction of a pair of random entities \$(A,B)\$ from the pair of static entities \$(x,y)\$. Then, estimating the cause-effect relation between \$A\$ and \$B\$ using an observational causal discovery algorithm leads to an estimation of the cause-effect relation between \$x\$ and \$y\$. For example, our framework detects the causal relation between unprocessed photographs and their modifications, and orders in time a set of shuffled frames from a video. As our main case study, we introduce a human-elicited dataset of 10,000 pairs of casually-linked pairs of words from natural language. Our methods discover 75\% of these causal relations. Finally, we discuss the role of proxy variables in machine learning, as a general tool to incorporate static knowledge into prediction tasks.},
	urldate = {2019-10-01},
	journal = {arXiv:1702.07306 [cs, stat]},
	author = {Rojas-Carulla, Mateo and Baroni, Marco and Lopez-Paz, David},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.07306},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{chattopadhyay_neural_2019,
	title = {Neural {Network} {Attributions}: {A} {Causal} {Perspective}},
	shorttitle = {Neural {Network} {Attributions}},
	url = {http://arxiv.org/abs/1902.02302},
	abstract = {We propose a new attribution method for neural networks developed using first principles of causality (to the best of our knowledge, the first such). The neural network architecture is viewed as a Structural Causal Model, and a methodology to compute the causal effect of each feature on the output is presented. With reasonable assumptions on the causal structure of the input data, we propose algorithms to efficiently compute the causal effects, as well as scale the approach to data with large dimensionality. We also show how this method can be used for recurrent neural networks. We report experimental results on both simulated and real datasets showcasing the promise and usefulness of the proposed algorithm.},
	urldate = {2019-09-10},
	journal = {arXiv:1902.02302 [cs, stat]},
	author = {Chattopadhyay, Aditya and Manupriya, Piyushi and Sarkar, Anirban and Balasubramanian, Vineeth N.},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.02302},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{louppe_understanding_2013,
	title = {Understanding variable importances in forests of randomized trees},
	booktitle = {Advances in neural information processing systems},
	author = {Louppe, Gilles and Wehenkel, Louis and Sutera, Antonio and Geurts, Pierre},
	year = {2013},
	pages = {431--439},
}

@misc{noauthor_understanding_nodate,
	title = {Understanding variable importancesin forests of randomize... - {Google} {Scholar}},
	url = {https://scholar.google.fr/scholar?hl=en&as_sdt=0%2C5&q=Understanding+variable+importancesin+forests+of+randomized+trees&btnG=},
	urldate = {2019-09-06},
}

@article{ross_mutual_2014,
	title = {Mutual {Information} between {Discrete} and {Continuous} {Data} {Sets}},
	volume = {9},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0087357},
	doi = {10.1371/journal.pone.0087357},
	abstract = {Mutual information (MI) is a powerful method for detecting relationships between data sets. There are accurate methods for estimating MI that avoid problems with “binning” when both data sets are discrete or when both data sets are continuous. We present an accurate, non-binning MI estimator for the case of one discrete data set and one continuous data set. This case applies when measuring, for example, the relationship between base sequence and gene expression level, or the effect of a cancer drug on patient survival time. We also show how our method can be adapted to calculate the Jensen–Shannon divergence of two or more data sets.},
	language = {en},
	number = {2},
	urldate = {2019-08-22},
	journal = {PLOS ONE},
	author = {Ross, Brian C.},
	month = feb,
	year = {2014},
	keywords = {Entropy, Gene expression, Information theory, Nucleobases, Probability density, Probability distribution, Square waves, Statistical distributions},
	pages = {e87357},
}

@article{jordon_knockoffgan:_2018,
	title = {{KnockoffGAN}: {Generating} {Knockoffs} for {Feature} {Selection} using {Generative} {Adversarial} {Networks}},
	shorttitle = {{KnockoffGAN}},
	url = {https://openreview.net/forum?id=ByeZ5jC5YQ},
	abstract = {Feature selection is a pervasive problem. The discovery of relevant features can be as important for performing a particular task (such as to avoid overfitting in prediction) as it can be for...},
	urldate = {2019-07-22},
	author = {Jordon, James and Yoon, Jinsung and Schaar, Mihaela van der},
	month = sep,
	year = {2018},
}

@article{huang_relaxing_2019,
	title = {Relaxing the {Assumptions} of {Knockoffs} by {Conditioning}},
	url = {http://arxiv.org/abs/1903.02806},
	abstract = {The recent paper Cand{\textbackslash}`es et al. (2018) introduced model-X knockoffs, a method for variable selection that provably and non-asymptotically controls the false discovery rate with no restrictions or assumptions on the dimensionality of the data or the conditional distribution of the response given the covariates. The one requirement for the procedure is that the covariate samples are drawn independently and identically from a precisely-known (but arbitrary) distribution. The present paper shows that the exact same guarantees can be made without knowing the covariate distribution fully, but instead knowing it only up to a parametric model with as many as \${\textbackslash}Omega(n{\textasciicircum}\{*\}p)\$ parameters, where \$p\$ is the dimension and \$n{\textasciicircum}\{*\}\$ is the number of covariate samples (which may exceed the usual sample size \$n\$ of labeled samples when unlabeled samples are also available). The key is to treat the covariates as if they are drawn conditionally on their observed value for a sufficient statistic of the model. Although this idea is simple, even in Gaussian models conditioning on a sufficient statistic leads to a distribution supported on a set of zero Lebesgue measure, requiring techniques from topological measure theory to establish valid algorithms. We demonstrate how to do this for three models of interest, with simulations showing the new approach remains powerful under the weaker assumptions.},
	urldate = {2019-07-19},
	journal = {arXiv:1903.02806 [stat]},
	author = {Huang, Dongming and Janson, Lucas},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.02806},
	keywords = {Statistics - Methodology},
}

@article{briol_statistical_2019,
	title = {Statistical {Inference} for {Generative} {Models} with {Maximum} {Mean} {Discrepancy}},
	journal = {arXiv preprint arXiv:1906.05944},
	author = {Briol, Francois-Xavier and Barp, Alessandro and Duncan, Andrew B. and Girolami, Mark},
	year = {2019},
}

@article{romano_deep_2018,
	title = {Deep {Knockoffs}},
	url = {http://arxiv.org/abs/1811.06687},
	abstract = {This paper introduces a machine for sampling approximate model-X knockoffs for arbitrary and unspecified data distributions using deep generative models. The main idea is to iteratively refine a knockoff sampling mechanism until a criterion measuring the validity of the produced knockoffs is optimized; this criterion is inspired by the popular maximum mean discrepancy in machine learning and can be thought of as measuring the distance to pairwise exchangeability between original and knockoff features. By building upon the existing model-X framework, we thus obtain a flexible and model-free statistical tool to perform controlled variable selection. Extensive numerical experiments and quantitative tests confirm the generality, effectiveness, and power of our deep knockoff machines. Finally, we apply this new method to a real study of mutations linked to changes in drug resistance in the human immunodeficiency virus.},
	urldate = {2019-07-19},
	journal = {arXiv:1811.06687 [math, stat]},
	author = {Romano, Yaniv and Sesia, Matteo and Candès, Emmanuel J.},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.06687},
	keywords = {Mathematics - Statistics Theory, Statistics - Applications, Statistics - Machine Learning, Statistics - Methodology},
}

@article{jordon_knockoffgan:_2018-1,
	title = {{KnockoffGAN}: {Generating} {Knockoffs} for {Feature} {Selection} using {Generative} {Adversarial} {Networks}},
	shorttitle = {{KnockoffGAN}},
	url = {https://openreview.net/forum?id=ByeZ5jC5YQ},
	abstract = {Feature selection is a pervasive problem. The discovery of relevant features can be as important for performing a particular task (such as to avoid overfitting in prediction) as it can be for...},
	urldate = {2019-07-19},
	author = {Jordon, James and Yoon, Jinsung and Schaar, Mihaela van der},
	month = sep,
	year = {2018},
}

@misc{noauthor_software_nodate,
	title = {Software for {Knockoffs}},
	url = {http://web.stanford.edu/group/candes/knockoffs/software/knockoffs/},
	urldate = {2019-07-19},
}

@article{archer_bayesian_2013,
	title = {Bayesian and quasi-{Bayesian} estimators for mutual information from discrete data},
	volume = {15},
	number = {5},
	journal = {Entropy},
	author = {Archer, Evan and Park, Il and Pillow, Jonathan},
	year = {2013},
	pages = {1738--1755},
}

@article{meinshausen_stability_2010,
	title = {Stability selection},
	volume = {72},
	number = {4},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Meinshausen, Nicolai and Bühlmann, Peter},
	year = {2010},
	pages = {417--473},
}

@misc{noauthor_regression_nodate,
	title = {Regression shrinkage and selection via the lasso: a retrospective - {Tibshirani} - 2017 - {Journal} of the {Royal} {Statistical} {Society}: {Series} {B} ({Statistical} {Methodology}) - {Wiley} {Online} {Library}},
	url = {https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9868.2011.00771.x%4010.1111/%28ISSN%291467-9868.TOP_SERIES_B_RESEARCH},
	urldate = {2019-07-15},
}

@misc{noauthor_wisdom_nodate,
	title = {Wisdom of crowds for robust gene network inference},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3512113/},
	urldate = {2019-07-15},
}

@article{haury_tigress:_2012,
	title = {{TIGRESS}: trustful inference of gene regulation using stability selection},
	volume = {6},
	shorttitle = {{TIGRESS}},
	number = {1},
	journal = {BMC systems biology},
	author = {Haury, Anne-Claire and Mordelet, Fantine and Vera-Licona, Paola and Vert, Jean-Philippe},
	year = {2012},
	pages = {145},
}

@misc{noauthor_wisdom_nodate-1,
	title = {Wisdom of crowds for robust gene network inference},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3512113/},
	urldate = {2019-07-15},
}

@article{meinshausen_stability_2010-1,
	title = {Stability selection},
	volume = {72},
	number = {4},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Meinshausen, Nicolai and Bühlmann, Peter},
	year = {2010},
	pages = {417--473},
}

@article{friedman_sparse_2008,
	title = {Sparse inverse covariance estimation with the graphical lasso},
	volume = {9},
	number = {3},
	journal = {Biostatistics},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	year = {2008},
	pages = {432--441},
}

@misc{noauthor_meinshausen:_nodate,
	title = {Meinshausen: {High}-dimensional graphs and variable... - {Google} {Scholar}},
	url = {https://scholar.google.fr/scholar?cites=3848260762014859214&as_sdt=2005&sciodt=0,5&hl=en},
	urldate = {2019-07-15},
}

@misc{noauthor_meinshausen:_nodate-1,
	title = {Meinshausen: {High}-dimensional graphs and variable... - {Google} {Scholar}},
	url = {https://scholar.google.fr/scholar?cites=3848260762014859214&as_sdt=2005&sciodt=0,5&hl=en},
	urldate = {2019-07-15},
}

@article{yu_false_2019,
	title = {False {Discovery} {Rates} in {Biological} {Networks}},
	url = {http://arxiv.org/abs/1907.03808},
	abstract = {The increasing availability of data has generated unprecedented prospects for network analyses in many biological fields, such as neuroscience (e.g., brain networks), genomics (e.g., gene-gene interaction networks), and ecology (e.g., species interaction networks). A powerful statistical framework for estimating such networks is Gaussian graphical models, but standard estimators for the corresponding graphs are prone to large numbers of false discoveries. In this paper, we introduce a novel graph estimator based on knockoffs that imitate the partial correlation structures of unconnected nodes. We show that this new estimator guarantees accurate control of the false discovery rate in theory, simulations, and biological applications, and we provide easy-to-use R code.},
	urldate = {2019-07-15},
	journal = {arXiv:1907.03808 [q-bio, stat]},
	author = {Yu, Lu and Kaufmann, Tobias and Lederer, Johannes},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.03808},
	keywords = {Quantitative Biology - Quantitative Methods, Statistics - Applications, Statistics - Methodology},
}

@article{meinshausen_high-dimensional_2006,
	title = {High-dimensional graphs and variable selection with the lasso},
	volume = {34},
	number = {3},
	journal = {The annals of statistics},
	author = {Meinshausen, Nicolai and Bühlmann, Peter},
	year = {2006},
	pages = {1436--1462},
}

@misc{noauthor_[1907.03808]_nodate,
	title = {[1907.03808] {False} {Discovery} {Rates} in {Biological} {Networks}},
	url = {https://arxiv.org/abs/1907.03808},
	urldate = {2019-07-15},
}

@article{ince_statistical_2017,
	title = {A statistical framework for neuroimaging data analysis based on mutual information estimated via a gaussian copula},
	volume = {38},
	number = {3},
	journal = {Human brain mapping},
	author = {Ince, Robin AA and Giordano, Bruno L. and Kayser, Christoph and Rousselet, Guillaume A. and Gross, Joachim and Schyns, Philippe G.},
	year = {2017},
	pages = {1541--1573},
}

@article{irrthum_inferring_2010,
	title = {Inferring regulatory networks from expression data using tree-based methods},
	volume = {5},
	number = {9},
	journal = {PloS one},
	author = {Irrthum, Alexandre and Wehenkel, Louis and Geurts, Pierre},
	year = {2010},
	pages = {e12776},
}

@book{cover_elements_2012,
	title = {Elements of information theory},
	publisher = {John Wiley \& Sons},
	author = {Cover, Thomas M. and Thomas, Joy A.},
	year = {2012},
}

@article{gamez_learning_2011,
	title = {Learning {Bayesian} networks by hill climbing: efficient methods based on progressive restriction of the neighborhood},
	volume = {22},
	shorttitle = {Learning {Bayesian} networks by hill climbing},
	number = {1-2},
	journal = {Data Mining and Knowledge Discovery},
	author = {Gámez, José A. and Mateo, Juan L. and Puerta, José M.},
	year = {2011},
	pages = {106--148},
}

@article{tsamardinos_max-min_2006,
	title = {The max-min hill-climbing {Bayesian} network structure learning algorithm},
	volume = {65},
	number = {1},
	journal = {Machine learning},
	author = {Tsamardinos, Ioannis and Brown, Laura E. and Aliferis, Constantin F.},
	year = {2006},
	pages = {31--78},
}

@incollection{pearl_theory_1995,
	title = {A theory of inferred causation},
	volume = {134},
	booktitle = {Studies in {Logic} and the {Foundations} of {Mathematics}},
	publisher = {Elsevier},
	author = {Pearl, Judea and Verma, Thomas S.},
	year = {1995},
	pages = {789--811},
}

@article{spirtes_algorithm_1991,
	title = {An algorithm for fast recovery of sparse causal graphs},
	volume = {9},
	number = {1},
	journal = {Social science computer review},
	author = {Spirtes, Peter and Glymour, Clark},
	year = {1991},
	pages = {62--72},
}

@article{marx_testing_2019,
	title = {Testing {Conditional} {Independence} on {Discrete} {Data} using {Stochastic} {Complexity}},
	journal = {arXiv preprint arXiv:1903.04829},
	author = {Marx, Alexander and Vreeken, Jilles},
	year = {2019},
}

@inproceedings{marx_stochastic_2018,
	title = {Stochastic {Complexity} for {Testing} {Conditional} {Independence} on {Discrete} {Data}},
	booktitle = {{NeurIPS} 2018 {Workshop} on {Causal} {Learning}},
	author = {Marx, Alexander and Vreeken, Jilles},
	year = {2018},
}

@article{veeling_predictive_2018,
	title = {Predictive {Uncertainty} through {Quantization}},
	journal = {arXiv preprint arXiv:1810.05500},
	author = {Veeling, Bastiaan S. and Berg, Rianne van den and Welling, Max},
	year = {2018},
}

@article{frankle_stabilizing_2019,
	title = {Stabilizing the {Lottery} {Ticket} {Hypothesis}},
	url = {http://arxiv.org/abs/1903.01611},
	abstract = {Pruning is a well-established technique for removing unnecessary structure from neural networks after training to improve the performance of inference. Several recent results have explored the possibility of pruning at initialization time to provide similar benefits during training. In particular, the "lottery ticket hypothesis" conjectures that typical neural networks contain small subnetworks that can train to similar accuracy in a commensurate number of steps. The evidence for this claim is that a procedure based on iterative magnitude pruning (IMP) reliably finds such subnetworks retroactively on small vision tasks. However, IMP fails on deeper networks, and proposed methods to prune before training or train pruned networks encounter similar scaling limitations. In this paper, we argue that these efforts have struggled on deeper networks because they have focused on pruning precisely at initialization. We modify IMP to search for subnetworks that could have been obtained by pruning early in training (0.1\% to 7\% through) rather than at iteration 0. With this change, it finds small subnetworks of deeper networks (e.g., 80\% sparsity on Resnet-50) that can complete the training process to match the accuracy of the original network on more challenging tasks (e.g., ImageNet). In situations where IMP fails at iteration 0, the accuracy benefits of delaying pruning accrue rapidly over the earliest iterations of training. To explain these behaviors, we study subnetwork "stability," finding that - as accuracy improves in this fashion - IMP subnetworks train to parameters closer to those of the full network and do so with improved consistency in the face of gradient noise. These results offer new insights into the opportunity to prune large-scale networks early in training and the behaviors underlying the lottery ticket hypothesis.},
	urldate = {2019-06-13},
	journal = {arXiv:1903.01611 [cs, stat]},
	author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M. and Carbin, Michael},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.01611},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{candes_panning_2018,
	title = {Panning for gold:‘model-{X}’knockoffs for high dimensional controlled variable selection},
	volume = {80},
	shorttitle = {Panning for gold},
	number = {3},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Candes, Emmanuel and Fan, Yingying and Janson, Lucas and Lv, Jinchi},
	year = {2018},
	pages = {551--577},
}

@article{li_littles_2013,
	title = {Little's test of missing completely at random},
	volume = {13},
	number = {4},
	journal = {The Stata Journal},
	author = {Li, Cheng},
	year = {2013},
	pages = {795--809},
}

@article{van_erven_renyi_2014,
	title = {Rényi divergence and {Kullback}-{Leibler} divergence},
	volume = {60},
	number = {7},
	journal = {IEEE Transactions on Information Theory},
	author = {Van Erven, Tim and Harremos, Peter},
	year = {2014},
	pages = {3797--3820},
}

@article{nielsen_r$backslash$enyi_2011,
	title = {On {R}\${\textbackslash}backslash\$'enyi and {Tsallis} entropies and divergences for exponential families},
	journal = {arXiv preprint arXiv:1105.3259},
	author = {Nielsen, Frank and Nock, Richard},
	year = {2011},
}

@article{shlens_notes_2014,
	title = {Notes on {Kullback}-{Leibler} divergence and likelihood},
	journal = {arXiv preprint arXiv:1404.2000},
	author = {Shlens, Jonathon},
	year = {2014},
}

@article{little_test_1988,
	title = {A test of missing completely at random for multivariate data with missing values},
	volume = {83},
	number = {404},
	journal = {Journal of the American statistical Association},
	author = {Little, Roderick JA},
	year = {1988},
	pages = {1198--1202},
}

@article{gimenez_discovering_2019,
	title = {Discovering {Conditionally} {Salient} {Features} with {Statistical} {Guarantees}},
	url = {http://arxiv.org/abs/1905.12177},
	abstract = {The goal of feature selection is to identify important features that are relevant to explain an outcome variable. Most of the work in this domain has focused on identifying globally relevant features, which are features that are related to the outcome using evidence across the entire dataset. We study a more fine-grained statistical problem: conditional feature selection, where a feature may be relevant depending on the values of the other features. For example in genetic association studies, variant \$A\$ could be associated with the phenotype in the entire dataset, but conditioned on variant \$B\$ being present it might be independent of the phenotype. In this sense, variant \$A\$ is globally relevant, but conditioned on \$B\$ it is no longer locally relevant in that region of the feature space. We present a generalization of the knockoff procedure that performs conditional feature selection while controlling a generalization of the false discovery rate (FDR) to the conditional setting. By exploiting the feature/response model-free framework of the knockoffs, the quality of the statistical FDR guarantee is not degraded even when we perform conditional feature selections. We implement this method and present an algorithm that automatically partitions the feature space such that it enhances the differences between selected sets in different regions, and validate the statistical theoretical results with experiments.},
	urldate = {2019-06-05},
	journal = {arXiv:1905.12177 [cs, stat]},
	author = {Gimenez, Jaime Roquero and Zou, James},
	month = may,
	year = {2019},
	note = {arXiv: 1905.12177},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_[1905.12177]_nodate,
	title = {[1905.12177] {Discovering} {Conditionally} {Salient} {Features} with {Statistical} {Guarantees}},
	url = {https://arxiv.org/abs/1905.12177},
	urldate = {2019-06-05},
}

@article{shen_false_2019,
	title = {False {Discovery} {Rate} {Control} in {Cancer} {Biomarker} {Selection} {Using} {Knockoffs}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-6694/11/6/744},
	doi = {10.3390/cancers11060744},
	abstract = {The discovery of biomarkers that are informative for cancer risk assessment, diagnosis, prognosis and treatment predictions is crucial. Recent advances in high-throughput genomics make it plausible to select biomarkers from the vast number of human genes in an unbiased manner. Yet, control of false discoveries is challenging given the large number of genes versus the relatively small number of patients in a typical cancer study. To ensure that most of the discoveries are true, we employ a knockoff procedure to control false discoveries. Our method is general and flexible, accommodating arbitrary covariate distributions, linear and nonlinear associations, and survival models. In simulations, our method compares favorably to the alternatives; its utility of identifying important genes in real clinical applications is demonstrated by the identification of seven genes associated with Breslow thickness in skin cutaneous melanoma patients.},
	language = {en},
	number = {6},
	urldate = {2019-06-05},
	journal = {Cancers},
	author = {Shen, Arlina and Fu, Han and He, Kevin and Jiang, Hui},
	month = jun,
	year = {2019},
	keywords = {cancer biomarker, diseases genes, false discovery rate, knockoffs, variable selection},
	pages = {744},
}

@misc{noauthor_false_nodate,
	title = {False {Discovery} {Rate} {Control} in {Cancer} {BiomarkerSelection} {Using} {Knockoffs} - {Google} {Search}},
	url = {https://www.google.com/search?client=ubuntu&hs=fdB&channel=fs&ei=yJn3XKnuHo7UUYOJnNgL&q=False+Discovery+Rate+Control+in+Cancer+BiomarkerSelection+Using+Knockoffs&oq=False+Discovery+Rate+Control+in+Cancer+BiomarkerSelection+Using+Knockoffs&gs_l=psy-ab.3...7950.7950..8139...0.0..0.0.0.......0....1..gws-wiz.6YjwTrOapyI},
	urldate = {2019-06-05},
}

@article{hjelm_learning_2018,
	title = {Learning deep representations by mutual information estimation and maximization},
	url = {https://openreview.net/forum?id=Bklr3j0cKX},
	abstract = {This work investigates unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that...},
	urldate = {2019-05-29},
	author = {Hjelm, R. Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua},
	month = sep,
	year = {2018},
}

@article{gabrie_entropy_2018,
	title = {Entropy and mutual information in models of deep neural networks},
	url = {http://papers.nips.cc/paper/7453-entropy-and-mutual-information-in-models-of-deep-neural-networks.pdf},
	urldate = {2019-05-29},
	journal = {Advances in Neural Information Processing Systems 31},
	author = {Gabrié, Marylou and Manoel, Andre and Luneau, Clément and barbier, jean and Macris, Nicolas and Krzakala, Florent and Zdeborová, Lenka},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {1821--1831},
}

@misc{noauthor_estimation_nodate,
	title = {Estimation of mutual information for real-valued data with error bars and controlled bias {\textbar} {bioRxiv}},
	url = {https://www.biorxiv.org/content/10.1101/589929v2-0},
	urldate = {2019-05-29},
}

@article{hooker_please_2019,
	title = {Please {Stop} {Permuting} {Features}: {An} {Explanation} and {Alternatives}},
	shorttitle = {Please {Stop} {Permuting} {Features}},
	url = {http://arxiv.org/abs/1905.03151},
	abstract = {This paper advocates against permute-and-predict (PaP) methods for interpreting black box functions. Methods such as the variable importance measures proposed for random forests, partial dependence plots, and individual conditional expectation plots remain popular because of their ability to provide model-agnostic measures that depend only on the pre-trained model output. However, numerous studies have found that these tools can produce diagnostics that are highly misleading, particularly when there is strong dependence among features. Rather than simply add to this growing literature by further demonstrating such issues, here we seek to provide an explanation for the observed behavior. In particular, we argue that breaking dependencies between features in hold-out data places undue emphasis on sparse regions of the feature space by forcing the original model to extrapolate to regions where there is little to no data. We explore these effects through various settings where a ground-truth is understood and find support for previous claims in the literature that PaP metrics tend to over-emphasize correlated features both in variable importance and partial dependence plots, even though applying permutation methods to the ground-truth models do not. As an alternative, we recommend more direct approaches that have proven successful in other settings: explicitly removing features, conditional permutations, or model distillation methods.},
	urldate = {2019-05-29},
	journal = {arXiv:1905.03151 [cs, stat]},
	author = {Hooker, Giles and Mentch, Lucas},
	month = may,
	year = {2019},
	note = {arXiv: 1905.03151},
	keywords = {62G08, Computer Science - Machine Learning, I.5.1, Statistics - Machine Learning, Statistics - Methodology},
}

@article{heinze-deml_invariant_2018,
	title = {Invariant {Causal} {Prediction} for {Nonlinear} {Models}},
	volume = {6},
	url = {https://www.degruyter.com/view/j/jci.2018.6.issue-2/jci-2017-0016/jci-2017-0016.xml},
	doi = {10.1515/jci-2017-0016},
	abstract = {An important problem in many domains is to predict how a system will respond to interventions. This task is inherently linked to estimating the system’s underlying causal structure. To this end, Invariant Causal Prediction (ICP) [1] has been proposed which learns a causal model exploiting the invariance of causal relations using data from different environments. When considering linear models, the implementation of ICP is relatively straightforward. However, the nonlinear case is more challenging due to the difficulty of performing nonparametric tests for conditional independence.},
	number = {2},
	urldate = {2019-05-13},
	journal = {Journal of Causal Inference},
	author = {Heinze-Deml, Christina and Peters, Jonas and Meinshausen, Nicolai},
	year = {2018},
	keywords = {causal structure learning, invariance, structural equation models},
}

@article{sun_mutual_2016,
	title = {Mutual {Information} {Based} {Matching} for {Causal} {Inference} with {Observational} {Data}},
	volume = {17},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v17/15-420.html},
	number = {199},
	urldate = {2019-05-10},
	journal = {Journal of Machine Learning Research},
	author = {Sun, Lei and Nikolaev, Alexander G.},
	year = {2016},
	pages = {1--31},
}

@article{kolchinsky_caveats_2018,
	title = {Caveats for information bottleneck in deterministic scenarios},
	url = {https://openreview.net/forum?id=rke4HiAcY7},
	abstract = {Information bottleneck (IB) is a method for extracting information from one random variable X that is relevant for predicting another random variable Y. To do so, IB identifies an intermediate...},
	urldate = {2019-05-10},
	author = {Kolchinsky, Artemy and Tracey, Brendan D. and Kuyk, Steven Van},
	month = sep,
	year = {2018},
}

@article{painsky_information-theoretic_2018,
	title = {An {Information}-{Theoretic} {Framework} for {Non}-linear {Canonical} {Correlation} {Analysis}},
	url = {http://arxiv.org/abs/1810.13259},
	abstract = {Canonical Correlation Analysis (CCA) is a linear representation learning method that seeks maximally correlated variables in multi-view data. Non-linear CCA extends this notion to a broader family of transformations, which are more powerful for many real-world applications. Given the joint probability, the Alternating Conditional Expectation (ACE) provides an optimal solution to the non-linear CCA problem. However, it suffers from limited performance and an increasing computational burden when only a finite number of observations is available. In this work we introduce an information-theoretic framework for the non-linear CCA problem (ITCCA), which extends the classical ACE approach. Our suggested framework seeks compressed representations of the data that allow a maximal level of correlation. This way we control the trade-off between the flexibility and the complexity of the representation. Our approach demonstrates favorable performance at a reduced computational burden, compared to non-linear alternatives, in a finite sample size regime. Further, ITCCA provides theoretical bounds and optimality conditions, as we establish fundamental connections to rate-distortion theory, the information bottleneck and remote source coding. In addition, it implies a "soft" dimensionality reduction, as the compression level is measured (and governed) by the mutual information between the original noisy data and the signals that we extract.},
	urldate = {2019-05-03},
	journal = {arXiv:1810.13259 [cs, stat]},
	author = {Painsky, Amichai and Feder, Meir and Tishby, Naftali},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.13259},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{tjostheim_statistical_2018,
	title = {Statistical dependence: {Beyond} {Pearson}'s \${\textbackslash}rho\$},
	shorttitle = {Statistical dependence},
	url = {http://arxiv.org/abs/1809.10455},
	abstract = {Pearson's \${\textbackslash}rho\$ is the most used measure of statistical dependence. It gives a complete characterization of dependence in the Gaussian case, and it also works well in some non-Gaussian situations. It is well known, however, that it has a number of shortcomings; in particular for heavy tailed distributions and in nonlinear situations, where it may produce misleading, and even disastrous results. In recent years a number of alternatives have been proposed. In this paper, we will survey these developments, especially results obtained in the last couple of decades. Among measures discussed are the copula, distribution-based measures, the distance covariance, the HSIC measure popular in machine learning, and finally the local Gaussian correlation, which is a local version of Pearson's \${\textbackslash}rho\$. Throughout we put the emphasis on conceptual developments and a comparison of these. We point out relevant references to technical details as well as comparative empirical and simulated experiments. There is a broad selection of references under each topic treated.},
	urldate = {2019-05-03},
	journal = {arXiv:1809.10455 [math, stat]},
	author = {Tjøstheim, Dag and Otneim, Håkon and Støve, Bård},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.10455},
	keywords = {Mathematics - Statistics Theory},
}

@article{mcclendon_comparing_2012,
	title = {Comparing {Conformational} {Ensembles} {Using} the {Kullback}-{Leibler} {Divergence} {Expansion}},
	volume = {8},
	issn = {1549-9618},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3538811/},
	doi = {10.1021/ct300008d},
	abstract = {We present a thermodynamical approach to identify changes in macromolecular structure and dynamics in response to perturbations such as mutations or ligand binding, using an expansion of the Kullback-Leibler Divergence that connects local population shifts in torsion angles to changes in the free energy landscape of the protein. While the Kullback-Leibler Divergence is a known formula from information theory, the novelty and power of our implementation lies in its formal developments, connection to thermodynamics, statistical filtering, ease of visualization of results, and extendability by adding higher-order terms. We present a formal derivation of the Kullback-Leibler Divergence expansion and then apply our method at a first-order approximation to molecular dynamics simulations of four protein systems where ligand binding or pH titration is known to cause an effect at a distant site. Our results qualitatively agree with experimental measurements of local changes in structure or dynamics, such as NMR chemical shift perturbations and hydrogen-deuterium exchange mass spectrometry. The approach produces easy-to-analyze results with low background, and as such has the potential to become a routine analysis when molecular dynamics simulations in two or more conditions are available. Our method is implemented in the MutInf code package and is available on the SimTK website at https://simtk.org/home/mutinf.},
	number = {6},
	urldate = {2019-04-16},
	journal = {Journal of chemical theory and computation},
	author = {McClendon, Christopher L. and Hua, Lan and Barreiro, abriela and Jacobson, Matthew P.},
	month = jul,
	year = {2012},
	pmid = {23316121},
	pmcid = {PMC3538811},
	pages = {2115--2126},
}

@article{lundberg_explainable_2018,
	title = {Explainable machine-learning predictions for the prevention of hypoxaemia during surgery},
	volume = {2},
	copyright = {2018 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2157-846X},
	url = {https://www.nature.com/articles/s41551-018-0304-0},
	doi = {10.1038/s41551-018-0304-0},
	abstract = {Although anaesthesiologists strive to avoid hypoxaemia during surgery, reliably predicting future intraoperative hypoxaemia is not possible at present. Here, we report the development and testing of a machine-learning-based system that predicts the risk of hypoxaemia and provides explanations of the risk factors in real time during general anaesthesia. The system, which was trained on minute-by-minute data from the electronic medical records of over 50,000 surgeries, improved the performance of anaesthesiologists by providing interpretable hypoxaemia risks and contributing factors. The explanations for the predictions are broadly consistent with the literature and with prior knowledge from anaesthesiologists. Our results suggest that if anaesthesiologists currently anticipate 15\% of hypoxaemia events, with the assistance of this system they could anticipate 30\%, a large portion of which may benefit from early intervention because they are associated with modifiable factors. The system can help improve the clinical understanding of hypoxaemia risk during anaesthesia care by providing general insights into the exact changes in risk induced by certain characteristics of the patient or procedure.},
	language = {en},
	number = {10},
	urldate = {2019-04-16},
	journal = {Nature Biomedical Engineering},
	author = {Lundberg, Scott M. and Nair, Bala and Vavilala, Monica S. and Horibe, Mayumi and Eisses, Michael J. and Adams, Trevor and Liston, David E. and Low, Daniel King-Wai and Newman, Shu-Fang and Kim, Jerry and Lee, Su-In},
	month = oct,
	year = {2018},
	pages = {749},
}

@article{lundberg_consistent_2018,
	title = {Consistent {Individualized} {Feature} {Attribution} for {Tree} {Ensembles}},
	url = {http://arxiv.org/abs/1802.03888},
	abstract = {Interpreting predictions from tree ensemble methods such as gradient boosting machines and random forests is important, yet feature attribution for trees is often heuristic and not individualized for each prediction. Here we show that popular feature attribution methods are inconsistent, meaning they can lower a feature's assigned importance when the true impact of that feature actually increases. This is a fundamental problem that casts doubt on any comparison between features. To address it we turn to recent applications of game theory and develop fast exact tree solutions for SHAP (SHapley Additive exPlanation) values, which are the unique consistent and locally accurate attribution values. We then extend SHAP values to interaction effects and define SHAP interaction values. We propose a rich visualization of individualized feature attributions that improves over classic attribution summaries and partial dependence plots, and a unique "supervised" clustering (clustering based on feature attributions). We demonstrate better agreement with human intuition through a user study, exponential improvements in run time, improved clustering performance, and better identification of influential features. An implementation of our algorithm has also been merged into XGBoost and LightGBM, see http://github.com/slundberg/shap for details.},
	urldate = {2019-04-16},
	journal = {arXiv:1802.03888 [cs, stat]},
	author = {Lundberg, Scott M. and Erion, Gabriel G. and Lee, Su-In},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.03888},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{lundberg_consistent_2018-1,
	title = {Consistent {Individualized} {Feature} {Attribution} for {Tree} {Ensembles}},
	url = {http://arxiv.org/abs/1802.03888},
	abstract = {Interpreting predictions from tree ensemble methods such as gradient boosting machines and random forests is important, yet feature attribution for trees is often heuristic and not individualized for each prediction. Here we show that popular feature attribution methods are inconsistent, meaning they can lower a feature's assigned importance when the true impact of that feature actually increases. This is a fundamental problem that casts doubt on any comparison between features. To address it we turn to recent applications of game theory and develop fast exact tree solutions for SHAP (SHapley Additive exPlanation) values, which are the unique consistent and locally accurate attribution values. We then extend SHAP values to interaction effects and define SHAP interaction values. We propose a rich visualization of individualized feature attributions that improves over classic attribution summaries and partial dependence plots, and a unique "supervised" clustering (clustering based on feature attributions). We demonstrate better agreement with human intuition through a user study, exponential improvements in run time, improved clustering performance, and better identification of influential features. An implementation of our algorithm has also been merged into XGBoost and LightGBM, see http://github.com/slundberg/shap for details.},
	urldate = {2019-04-16},
	journal = {arXiv:1802.03888 [cs, stat]},
	author = {Lundberg, Scott M. and Erion, Gabriel G. and Lee, Su-In},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.03888},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{lundberg_unified_2017,
	title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	url = {http://arxiv.org/abs/1705.07874},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	urldate = {2019-04-16},
	journal = {arXiv:1705.07874 [cs, stat]},
	author = {Lundberg, Scott and Lee, Su-In},
	month = may,
	year = {2017},
	note = {arXiv: 1705.07874},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{burnham_kullback-leibler_2001,
	title = {Kullback-{Leibler} information as a basis for strong inference in ecological studies},
	volume = {28},
	number = {2},
	journal = {Wildlife research},
	author = {Burnham, Kenneth P. and Anderson, David R.},
	year = {2001},
	pages = {111--119},
}

@article{burnham_kullback-leibler_2001-1,
	title = {Kullback-{Leibler} information as a basis for strong inference in ecological studies},
	volume = {28},
	number = {2},
	journal = {Wildlife research},
	author = {Burnham, Kenneth P. and Anderson, David R.},
	year = {2001},
	pages = {111--119},
}

@article{wang_divergence_2005,
	title = {Divergence estimation of continuous distributions based on data-dependent partitions},
	volume = {51},
	number = {9},
	journal = {IEEE Transactions on Information Theory},
	author = {Wang, Qing and Kulkarni, Sanjeev R. and Verdú, Sergio},
	year = {2005},
	pages = {3064--3074},
}

@inproceedings{perez-cruz_kullback-leibler_2008,
	title = {Kullback-{Leibler} divergence estimation of continuous distributions},
	booktitle = {2008 {IEEE} international symposium on information theory},
	publisher = {IEEE},
	author = {Pérez-Cruz, Fernando},
	year = {2008},
	pages = {1666--1670},
}

@article{drysdale_false_2019,
	title = {The {False} {Positive} {Control} {Lasso}},
	url = {http://arxiv.org/abs/1903.12584},
	abstract = {In high dimensional settings where a small number of regressors are expected to be important, the Lasso estimator can be used to obtain a sparse solution vector with the expectation that most of the non-zero coefficients are associated with true signals. While several approaches have been developed to control the inclusion of false predictors with the Lasso, these approaches are limited by relying on asymptotic theory, having to empirically estimate terms based on theoretical quantities, assuming a continuous response class with Gaussian noise and design matrices, or high computation costs. In this paper we show how: (1) an existing model (the SQRT-Lasso) can be recast as a method of controlling the number of expected false positives, (2) how a similar estimator can used for all other generalized linear model classes, and (3) this approach can be fit with existing fast Lasso optimization solvers. Our justification for false positive control using randomly weighted self-normalized sum theory is to our knowledge novel. Moreover, our estimator's properties hold in finite samples up to some approximation error which we find in practical settings to be negligible under a strict mutual incoherence condition.},
	urldate = {2019-04-08},
	journal = {arXiv:1903.12584 [cs, stat]},
	author = {Drysdale, Erik and Peng, Yingwei and Hanna, Timothy P. and Nguyen, Paul and Goldenberg, Anna},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.12584},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{drysdale_false_2019-1,
	title = {The {False} {Positive} {Control} {Lasso}},
	url = {http://arxiv.org/abs/1903.12584},
	abstract = {In high dimensional settings where a small number of regressors are expected to be important, the Lasso estimator can be used to obtain a sparse solution vector with the expectation that most of the non-zero coefficients are associated with true signals. While several approaches have been developed to control the inclusion of false predictors with the Lasso, these approaches are limited by relying on asymptotic theory, having to empirically estimate terms based on theoretical quantities, assuming a continuous response class with Gaussian noise and design matrices, or high computation costs. In this paper we show how: (1) an existing model (the SQRT-Lasso) can be recast as a method of controlling the number of expected false positives, (2) how a similar estimator can used for all other generalized linear model classes, and (3) this approach can be fit with existing fast Lasso optimization solvers. Our justification for false positive control using randomly weighted self-normalized sum theory is to our knowledge novel. Moreover, our estimator's properties hold in finite samples up to some approximation error which we find in practical settings to be negligible under a strict mutual incoherence condition.},
	urldate = {2019-04-08},
	journal = {arXiv:1903.12584 [cs, stat]},
	author = {Drysdale, Erik and Peng, Yingwei and Hanna, Timothy P. and Nguyen, Paul and Goldenberg, Anna},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.12584},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_[1903.12584]_nodate,
	title = {[1903.12584] {The} {False} {Positive} {Control} {Lasso}},
	url = {https://arxiv.org/abs/1903.12584},
	urldate = {2019-04-08},
}

@article{miannay_constraints_2018,
	title = {Constraints on signaling network logic reveal functional subgraphs on {Multiple} {Myeloma} {OMIC} data},
	volume = {12},
	number = {3},
	journal = {BMC systems biology},
	author = {Miannay, Bertrand and Minvielle, Stéphane and Magrangeas, Florence and Guziolowski, Carito},
	year = {2018},
	pages = {32},
}

@article{walters-williams_estimation_nodate,
	title = {Estimation of {Mutual} {Information}: {A} {Survey}},
	shorttitle = {Estimation of {Mutual} {Information}},
	author = {Walters-Williams, Janett and Li, Yan},
}

@inproceedings{walters-williams_estimation_2009,
	title = {Estimation of mutual information: {A} survey},
	shorttitle = {Estimation of mutual information},
	booktitle = {International {Conference} on {Rough} {Sets} and {Knowledge} {Technology}},
	publisher = {Springer},
	author = {Walters-Williams, Janett and Li, Yan},
	year = {2009},
	pages = {389--396},
}

@article{barber_controlling_2015,
	title = {Controlling the false discovery rate via knockoffs},
	volume = {43},
	number = {5},
	journal = {The Annals of Statistics},
	author = {Barber, Rina Foygel and Candès, Emmanuel J.},
	year = {2015},
	pages = {2055--2085},
}

@article{gimenez_knockoffs_2018,
	title = {Knockoffs for the mass: new feature importance statistics with false discovery guarantees},
	shorttitle = {Knockoffs for the mass},
	url = {http://arxiv.org/abs/1807.06214},
	abstract = {An important problem in machine learning and statistics is to identify features that causally affect the outcome. This is often impossible to do from purely observational data, and a natural relaxation is to identify features that are correlated with the outcome even conditioned on all other observed features. For example, we want to identify that smoking really is correlated with cancer conditioned on demographics. The knockoff procedure is a recent breakthrough in statistics that, in theory, can identify truly correlated features while guaranteeing that the false discovery is limited. The idea is to create synthetic data -knockoffs- that captures correlations amongst the features. However there are substantial computational and practical challenges to generating and using knockoffs. This paper makes several key advances that enable knockoff application to be more efficient and powerful. We develop an efficient algorithm to generate valid knockoffs from Bayesian Networks. Then we systematically evaluate knockoff test statistics and develop new statistics with improved power. The paper combines new mathematical guarantees with systematic experiments on real and synthetic data.},
	urldate = {2019-04-03},
	journal = {arXiv:1807.06214 [cs, stat]},
	author = {Gimenez, Jaime Roquero and Ghorbani, Amirata and Zou, James},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.06214},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bates_metropolized_2019,
	title = {Metropolized {Knockoff} {Sampling}},
	url = {http://arxiv.org/abs/1903.00434},
	abstract = {Model-X knockoffs is a wrapper that transforms essentially any feature importance measure into a variable selection algorithm, which discovers true effects while rigorously controlling the expected fraction of false positives. A frequently discussed challenge to apply this method is to construct knockoff variables, which are synthetic variables obeying a crucial exchangeability property with the explanatory variables under study. This paper introduces techniques for knockoff generation in great generality: we provide a sequential characterization of all possible knockoff distributions, which leads to a Metropolis-Hastingsformulation of an exact knockoff sampler. We further show how to use conditional independence structure to speed up computations. Combining these two threads, we introduce an explicit set of sequential algorithms and empirically demonstrate their effectiveness. Our theoretical analysis proves that our algorithms achieve near-optimal computational complexity in certain cases. The techniques we develop are sufficiently rich to enable knockoff sampling in challenging models including cases where the covariates are continuous and heavy-tailed, and follow a graphical model such as the Ising model.},
	urldate = {2019-04-03},
	journal = {arXiv:1903.00434 [stat]},
	author = {Bates, Stephen and Candès, Emmanuel and Janson, Lucas and Wang, Wenshuo},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.00434},
	keywords = {Statistics - Methodology},
}

@misc{noauthor_control_nodate,
	title = {Control of {Confounding} and {Reporting} of {Results} in... - {Google} {Scholar}},
	url = {https://scholar.google.fr/scholar?hl=en&as_sdt=0%2C5&q=Control+of+Confounding+and+Reporting+of+Results+in+Causal+Inference+Studies.+Guidance+for+Authors+from+Editors+of+Respiratory%2C+Sleep%2C+and+Critical+Care+Journals.&btnG=},
	urldate = {2019-03-29},
}

@article{zeng_jackknife_2018,
	title = {Jackknife approach to the estimation of mutual information},
	volume = {115},
	number = {40},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Zeng, Xianli and Xia, Yingcun and Tong, Howell},
	year = {2018},
	pages = {9956--9961},
}

@article{basu_iterative_2018,
	title = {Iterative random forests to discover predictive and stable high-order interactions},
	volume = {115},
	number = {8},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Basu, Sumanta and Kumbier, Karl and Brown, James B. and Yu, Bin},
	year = {2018},
	pages = {1943--1948},
}

@article{hansen_model_2001,
	title = {Model selection and the principle of minimum description length},
	volume = {96},
	number = {454},
	journal = {Journal of the American Statistical Association},
	author = {Hansen, Mark H. and Yu, Bin},
	year = {2001},
	pages = {746--774},
}

@misc{kratzer_varrank:_2018,
	title = {varrank: {Heuristics} {Tools} {Based} on {Mutual} {Information} for {Variable} {Ranking}},
	copyright = {GPL-3},
	shorttitle = {varrank},
	url = {https://CRAN.R-project.org/package=varrank},
	abstract = {A computational toolbox of heuristics approaches for performing variable ranking and feature selection based on mutual information well adapted for multivariate system epidemiology datasets. The core function is a general implementation of the minimum redundancy maximum relevance model. R. Battiti (1994) {\textless}doi:10.1109/72.298224{\textgreater}. Continuous variables are discretized using a large choice of rule. Variables ranking can be learned with a sequential forward/backward search algorithm. The two main problems that can be addressed by this package is the selection of the most representative variable within a group of variables of interest (i.e. dimension reduction) and variable ranking with respect to a set of features of interest.},
	urldate = {2019-03-19},
	author = {Kratzer, Gilles and Furrer, Reinhard},
	month = dec,
	year = {2018},
}

@misc{kursa_praznik:_2018,
	title = {praznik: {Collection} of {Information}-{Based} {Feature} {Selection} {Filters}},
	copyright = {GPL-3},
	shorttitle = {praznik},
	url = {https://CRAN.R-project.org/package=praznik},
	abstract = {A collection of feature selection filters performing greedy optimisation of mutual information-based usefulness criteria, inspired by the overview by Brown, Pocock, Zhao and Lujan (2012) {\textless}http://www.jmlr.org/papers/v13/brown12a.html{\textgreater}. Implements, among other, minimum redundancy maximal relevancy ('mRMR') method by Peng, Long and Ding (2005) {\textless}doi:10.1109/TPAMI.2005.159{\textgreater}; joint mutual information ('JMI') method by Yang and Moody (1999) {\textless}http://papers.nips.cc/paper/1779-data-visualization-and-feature-selection-new-algorithms-for-nongaussian-data{\textgreater}; double input symmetrical relevance ('DISR') method by Meyer and Bontempi (2006) {\textless}doi:10.1007/11732242\_9{\textgreater} as well as joint mutual information maximisation ('JMIM') method by Bennasar, Hicks and Setchi (2015) {\textless}doi:10.1016/j.eswa.2015.07.007{\textgreater}.},
	urldate = {2019-03-19},
	author = {Kursa, Miron B.},
	month = may,
	year = {2018},
}

@article{folch-fortuny_enabling_2015,
	title = {Enabling network inference methods to handle missing data and outliers},
	volume = {16},
	issn = {1471-2105},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4559359/},
	doi = {10.1186/s12859-015-0717-7},
	abstract = {Background
The inference of complex networks from data is a challenging problem in biological sciences, as well as in a wide range of disciplines such as chemistry, technology, economics, or sociology. The quantity and quality of the data greatly affect the results. While many methodologies have been developed for this task, they seldom take into account issues such as missing data or outlier detection and correction, which need to be properly addressed before network inference.

Results
Here we present an approach to (i) handle missing data and (ii) detect and correct outliers based on multivariate projection to latent structures. The method, called trimmed scores regression (TSR), enables network inference methods to analyse incomplete datasets by imputing the missing values coherently with the latent data structure. Furthermore, it substitutes the faulty values in a dataset by proper estimations. We provide an implementation of this approach, and show how it can be integrated with any network inference method as a preliminary data curation step. This functionality is demonstrated with a state of the art network inference method based on mutual information distance and entropy reduction, MIDER.

Conclusion
The methodology presented here enables network inference methods to analyse a large number of incomplete and faulty datasets that could not be reliably analysed so far. Our comparative studies show the superiority of TSR over other missing data approaches used by practitioners. Furthermore, the method allows for outlier detection and correction.

Electronic supplementary material
The online version of this article (doi:10.1186/s12859-015-0717-7) contains supplementary material, which is available to authorized users.},
	urldate = {2019-03-19},
	journal = {BMC Bioinformatics},
	author = {Folch-Fortuny, Abel and Villaverde, Alejandro F. and Ferrer, Alberto and Banga, Julio R.},
	month = sep,
	year = {2015},
	pmid = {26335628},
	pmcid = {PMC4559359},
}

@article{climente-gonzalez_block_2019,
	title = {Block {HSIC} {Lasso}: model-free biomarker detection for ultra-high dimensional data},
	shorttitle = {Block {HSIC} {Lasso}},
	journal = {bioRxiv},
	author = {Climente-Gonzalez, Hector and Azencott, Chloe-Agathe and Kaski, Samuel and Yamada, Makoto},
	year = {2019},
	pages = {532192},
}

@article{szekely_brownian_2009,
	title = {Brownian distance covariance},
	volume = {3},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/euclid.aoas/1267453933},
	doi = {10.1214/09-AOAS312},
	abstract = {Distance correlation is a new class of multivariate dependence coefficients applicable to random vectors of arbitrary and not necessarily equal dimension. Distance covariance and distance correlation are analogous to product-moment covariance and correlation, but generalize and extend these classical bivariate measures of dependence. Distance correlation characterizes independence: it is zero if and only if the random vectors are independent. The notion of covariance with respect to a stochastic process is introduced, and it is shown that population distance covariance coincides with the covariance with respect to Brownian motion; thus, both can be called Brownian distance covariance. In the bivariate case, Brownian covariance is the natural extension of product-moment covariance, as we obtain Pearson product-moment covariance by replacing the Brownian motion in the definition with identity. The corresponding statistic has an elegantly simple computing formula. Advantages of applying Brownian covariance and correlation vs the classical Pearson covariance and correlation are discussed and illustrated.},
	language = {EN},
	number = {4},
	urldate = {2019-02-18},
	journal = {The Annals of Applied Statistics},
	author = {Székely, Gábor J. and Rizzo, Maria L.},
	month = dec,
	year = {2009},
	mrnumber = {MR2752127},
	zmnumber = {1196.62077},
	keywords = {Brownian covariance, Distance correlation, dcor, independence, multivariate},
	pages = {1236--1265},
}

@article{lachmann_aracne-ap:_2016,
	title = {{ARACNe}-{AP}: gene network reverse engineering through adaptive partitioning inference of mutual information},
	volume = {32},
	issn = {1367-4803},
	shorttitle = {{ARACNe}-{AP}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4937200/},
	doi = {10.1093/bioinformatics/btw216},
	abstract = {Summary: The accurate reconstruction of gene regulatory networks from large scale molecular profile datasets represents one of the grand challenges of Systems Biology. The Algorithm for the Reconstruction of Accurate Cellular Networks (ARACNe) represents one of the most effective tools to accomplish this goal. However, the initial Fixed Bandwidth (FB) implementation is both inefficient and unable to deal with sample sets providing largely uneven coverage of the probability density space. Here, we present a completely new implementation of the algorithm, based on an Adaptive Partitioning strategy (AP) for estimating the Mutual Information. The new AP implementation (ARACNe-AP) achieves a dramatic improvement in computational performance (200× on average) over the previous methodology, while preserving the Mutual Information estimator and the Network inference accuracy of the original algorithm. Given that the previous version of ARACNe is extremely demanding, the new version of the algorithm will allow even researchers with modest computational resources to build complex regulatory networks from hundreds of gene expression profiles., Availability and Implementation: A JAVA cross-platform command line executable of ARACNe, together with all source code and a detailed usage guide are freely available on Sourceforge (http://sourceforge.net/projects/aracne-ap). JAVA version 8 or higher is required., Contact:
califano@c2b2.columbia.edu, Supplementary information:
Supplementary data are available at Bioinformatics online.},
	number = {14},
	urldate = {2019-02-05},
	journal = {Bioinformatics},
	author = {Lachmann, Alexander and Giorgi, Federico M. and Lopez, Gonzalo and Califano, Andrea},
	month = jul,
	year = {2016},
	pmid = {27153652},
	pmcid = {PMC4937200},
	pages = {2233--2235},
}

@misc{noauthor_quel_nodate,
	title = {Quel est le statut du doctorant en {France}? {\textbar} {Collectif} {Doctoral}},
	shorttitle = {Quel est le statut du doctorant en {France}?},
	url = {https://doctoratp4.hypotheses.org/category/faire-sa-these-a-paris-sorbonne/quest-ce-que-le-doctorat/quel-est-le-statut-du-doctorant-en-france},
	language = {fr-FR},
	urldate = {2019-01-29},
}

@article{lei_geometric_2018,
	title = {Geometric {Understanding} of {Deep} {Learning}},
	url = {http://arxiv.org/abs/1805.10451},
	abstract = {Deep learning is the mainstream technique for many machine learning tasks, including image recognition, machine translation, speech recognition, and so on. It has outperformed conventional methods in various fields and achieved great successes. Unfortunately, the understanding on how it works remains unclear. It has the central importance to lay down the theoretic foundation for deep learning. In this work, we give a geometric view to understand deep learning: we show that the fundamental principle attributing to the success is the manifold structure in data, namely natural high dimensional data concentrates close to a low-dimensional manifold, deep learning learns the manifold and the probability distribution on it. We further introduce the concepts of rectified linear complexity for deep neural network measuring its learning capability, rectified linear complexity of an embedding manifold describing the difficulty to be learned. Then we show for any deep neural network with fixed architecture, there exists a manifold that cannot be learned by the network. Finally, we propose to apply optimal mass transportation theory to control the probability distribution in the latent space.},
	urldate = {2019-01-24},
	journal = {arXiv:1805.10451 [cs, stat]},
	author = {Lei, Na and Luo, Zhongxuan and Yau, Shing-Tung and Gu, David Xianfeng},
	month = may,
	year = {2018},
	note = {arXiv: 1805.10451},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{huynh-thu_gene_2018,
	title = {Gene regulatory network inference: an introductory survey},
	shorttitle = {Gene regulatory network inference},
	url = {http://arxiv.org/abs/1801.04087},
	abstract = {Gene regulatory networks are powerful abstractions of biological systems. Since the advent of high-throughput measurement technologies in biology in the late 90s, reconstructing the structure of such networks has been a central computational problem in systems biology. While the problem is certainly not solved in its entirety, considerable progress has been made in the last two decades, with mature tools now available. This chapter aims to provide an introduction to the basic concepts underpinning network inference tools, attempting a categorisation which highlights commonalities and relative strengths. While the chapter is meant to be self-contained, the material presented should provide a useful background to the later, more specialised chapters of this book.},
	urldate = {2018-03-28},
	journal = {arXiv:1801.04087 [q-bio]},
	author = {Huynh-Thu, Vân Anh and Sanguinetti, Guido},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.04087},
	keywords = {Quantitative Biology - Molecular Networks, Quantitative Biology - Quantitative Methods},
}

@article{noauthor_identifying_2018,
	title = {Identifying {Causal} {Effects} with the {R} {Package} causaleffect {\textbar} {Tikka} {\textbar} {Journal} of {Statistical} {Software}},
	url = {https://www.jstatsoft.org/article/view/v076i12},
	doi = {10.18637/jss.v076.i12},
	language = {en-US},
	urldate = {2018-02-02},
	month = feb,
	year = {2018},
	keywords = {C-component, DAG, causal model, causality, d-separation, do-calculus, graph, hedge, identifiability},
}

@misc{noauthor_mode_nodate,
	title = {A {Mode} l for {Measuremen} t {Erro} r fo r {Gen} e {Expressio}... - {Google} {Scholar}},
	url = {https://scholar.google.fr/scholar?hl=en&as_sdt=0%2C5&q=A+Mode+l+for+Measuremen+t+Erro+++r+fo+r+Gen+e+Expressio+n+Array+s&btnG=},
	urldate = {2018-02-01},
}

@misc{noauthor_estimating_nodate,
	title = {Estimating psychological networks and their accuracy: {A} tutorial paper {\textbar} {SpringerLink}},
	url = {https://link.springer.com/article/10.3758/s13428-017-0862-1},
	urldate = {2019-01-15},
}

@article{hastie_learning_2015,
	title = {Learning the {Structure} of {Mixed} {Graphical} {Models} {AU}  - {Lee}, {Jason} {D}.},
	volume = {24},
	issn = {1061-8600},
	url = {https://amstat.tandfonline.com/doi/abs/10.1080/10618600.2014.900500},
	doi = {10.1080/10618600.2014.900500},
	abstract = {We consider the problem of learning the structure of a pairwise graphical model over continuous and discrete variables. We present a new pairwise model for graphical models with both continuous and discrete variables that is amenable to structure learning. In previous work, authors have considered structure learning of Gaussian graphical models and structure learning of discrete models. Our approach is a natural generalization of these two lines of work to the mixed case. The penalization scheme involves a novel symmetric use of the group-lasso norm and follows naturally from a particular parameterization of the model. Supplementary materials for this article are available online.},
	number = {1},
	urldate = {2019-01-22},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Hastie, Trevor J.},
	month = jan,
	year = {2015},
	pages = {230--253},
}

@article{lam_fused_2016,
	title = {Fused {Regression} for {Multi}-source {Gene} {Regulatory} {Network} {Inference}},
	volume = {12},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005157},
	doi = {10.1371/journal.pcbi.1005157},
	abstract = {Understanding gene regulatory networks is critical to understanding cellular differentiation and response to external stimuli. Methods for global network inference have been developed and applied to a variety of species. Most approaches consider the problem of network inference independently in each species, despite evidence that gene regulation can be conserved even in distantly related species. Further, network inference is often confined to single data-types (single platforms) and single cell types. We introduce a method for multi-source network inference that allows simultaneous estimation of gene regulatory networks in multiple species or biological processes through the introduction of priors based on known gene relationships such as orthology incorporated using fused regression. This approach improves network inference performance even when orthology mapping and conservation are incomplete. We refine this method by presenting an algorithm that extracts the true conserved subnetwork from a larger set of potentially conserved interactions and demonstrate the utility of our method in cross species network inference. Last, we demonstrate our method’s utility in learning from data collected on different experimental platforms.},
	language = {en},
	number = {12},
	urldate = {2019-01-18},
	journal = {PLOS Computational Biology},
	author = {Lam, Kari Y. and Westrick, Zachary M. and Müller, Christian L. and Christiaen, Lionel and Bonneau, Richard},
	month = dec,
	year = {2016},
	keywords = {Bacillus anthracis, Bacillus subtilis, Gene expression, Gene regulation, Gene regulatory networks, Genetic networks, Operons, Transcription factors},
	pages = {e1005157},
}

@article{bonneau_inferelator:_2006,
	title = {The {Inferelator}: an algorithm for learning parsimonious regulatory networks from systems-biology data sets de novo},
	volume = {7},
	issn = {1474-760X},
	shorttitle = {The {Inferelator}},
	url = {https://doi.org/10.1186/gb-2006-7-5-r36},
	doi = {10.1186/gb-2006-7-5-r36},
	abstract = {We present a method (the Inferelator) for deriving genome-wide transcriptional regulatory interactions, and apply the method to predict a large portion of the regulatory network of the archaeon Halobacterium NRC-1. The Inferelator uses regression and variable selection to identify transcriptional influences on genes based on the integration of genome annotation and expression data. The learned network successfully predicted Halobacterium's global expression under novel perturbations with predictive power similar to that seen over training data. Several specific regulatory predictions were experimentally tested and verified.},
	number = {5},
	urldate = {2019-01-18},
	journal = {Genome Biology},
	author = {Bonneau, Richard and Reiss, David J. and Shannon, Paul and Facciotti, Marc and Hood, Leroy and Baliga, Nitin S. and Thorsson, Vesteinn},
	month = may,
	year = {2006},
	pages = {R36},
}

@article{cheng_high-dimensional_2017,
	title = {High-dimensional mixed graphical models},
	volume = {26},
	number = {2},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Cheng, Jie and Li, Tianxi and Levina, Elizaveta and Zhu, Ji},
	year = {2017},
	pages = {367--378},
}

@article{haslbeck_structure_2015,
	title = {Structure estimation for mixed graphical models in high-dimensional data},
	journal = {arXiv preprint arXiv:1510.05677},
	author = {Haslbeck, Jonas and Waldorp, Lourens J.},
	year = {2015},
}

@article{sedgewick_mixed_2018,
	title = {Mixed graphical models for integrative causal analysis with application to chronic lung disease diagnosis and prognosis},
	journal = {Bioinformatics},
	author = {Sedgewick, Andrew J. and Buschur, Kristina and Shi, Ivy and Ramsey, Joseph D. and Raghu, Vineet K. and Manatakis, Dimitris V. and Zhang, Yingze and Bon, Jessica and Chandra, Divay and Karoleski, Chad},
	year = {2018},
}

@article{simonsohn_specification_2015,
	title = {Specification curve: {Descriptive} and inferential statistics on all reasonable specifications},
	shorttitle = {Specification curve},
	author = {Simonsohn, Uri and Simmons, Joseph P. and Nelson, Leif D.},
	year = {2015},
}

@article{epskamp_estimating_2018,
	title = {Estimating psychological networks and their accuracy: {A} tutorial paper},
	volume = {50},
	shorttitle = {Estimating psychological networks and their accuracy},
	number = {1},
	journal = {Behavior Research Methods},
	author = {Epskamp, Sacha and Borsboom, Denny and Fried, Eiko I.},
	year = {2018},
	pages = {195--212},
}

@misc{noauthor_google_nodate,
	title = {Google {Scholar}},
	url = {https://scholar.google.fr/scholar?oi=bibs&hl=en&cites=14857737826931418003,7189110465048893631},
	urldate = {2019-01-15},
}

@article{epskamp_tutorial_2018,
	title = {A {Tutorial} on {Regularized} {Partial} {Correlation} {Networks}},
	volume = {23},
	issn = {1939-1463, 1082-989X},
	url = {http://arxiv.org/abs/1607.01367},
	doi = {10.1037/met0000167},
	abstract = {Recent years have seen an emergence of network modeling applied to moods, attitudes, and problems in the realm of psychology. In this framework, psychological variables are understood to directly affect each other rather than being caused by an unobserved latent entity. In this tutorial, we introduce the reader to estimating the most popular network model for psychological data: the partial correlation network. We describe how regularization techniques can be used to efficiently estimate a parsimonious and interpretable network structure in psychological data. We show how to perform these analyses in R and demonstrate the method in an empirical example on post-traumatic stress disorder data. In addition, we discuss the effect of the hyperparameter that needs to be manually set by the researcher, how to handle non-normal data, how to determine the required sample size for a network analysis, and provide a checklist with potential solutions for problems that can arise when estimating regularized partial correlation networks.},
	number = {4},
	urldate = {2019-01-15},
	journal = {Psychological Methods},
	author = {Epskamp, Sacha and Fried, Eiko I.},
	month = dec,
	year = {2018},
	note = {arXiv: 1607.01367},
	keywords = {Statistics - Applications, Statistics - Methodology},
	pages = {617--634},
}

@article{van_borkulo_new_2014,
	title = {A new method for constructing networks from binary data},
	volume = {4},
	copyright = {2014 Nature Publishing Group},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/srep05918},
	doi = {10.1038/srep05918},
	abstract = {Network analysis is entering fields where network structures are unknown, such as psychology and the educational sciences. A crucial step in the application of network models lies in the assessment of network structure. Current methods either have serious drawbacks or are only suitable for Gaussian data. In the present paper, we present a method for assessing network structures from binary data. Although models for binary data are infamous for their computational intractability, we present a computationally efficient model for estimating network structures. The approach, which is based on Ising models as used in physics, combines logistic regression with model selection based on a Goodness-of-Fit measure to identify relevant relationships between variables that define connections in a network. A validation study shows that this method succeeds in revealing the most relevant features of a network for realistic sample sizes. We apply our proposed method to estimate the network of depression and anxiety symptoms from symptom scores of 1108 subjects. Possible extensions of the model are discussed.},
	language = {en},
	urldate = {2019-01-15},
	journal = {Scientific Reports},
	author = {van Borkulo, Claudia D. and Borsboom, Denny and Epskamp, Sacha and Blanken, Tessa F. and Boschloo, Lynn and Schoevers, Robert A. and Waldorp, Lourens J.},
	month = aug,
	year = {2014},
	pages = {5918},
}

@article{mcallester_formal_2018,
	title = {Formal {Limitations} on the {Measurement} of {Mutual} {Information}},
	journal = {arXiv preprint arXiv:1811.04251},
	author = {McAllester, David and Statos, Karl},
	year = {2018},
}

@article{belghazi_mine:_2018,
	title = {{MINE}: mutual information neural estimation},
	shorttitle = {{MINE}},
	journal = {arXiv preprint arXiv:1801.04062},
	author = {Belghazi, Ishmael and Rajeswar, Sai and Baratin, Aristide and Hjelm, R. Devon and Courville, Aaron},
	year = {2018},
}

@article{basu_iterative_2018-1,
	title = {Iterative random forests to discover predictive and stable high-order interactions},
	volume = {115},
	copyright = {Copyright © 2018 the Author(s). Published by PNAS.. This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/8/1943},
	doi = {10.1073/pnas.1711236115},
	abstract = {Genomics has revolutionized biology, enabling the interrogation of whole transcriptomes, genome-wide binding sites for proteins, and many other molecular processes. However, individual genomic assays measure elements that interact in vivo as components of larger molecular machines. Understanding how these high-order interactions drive gene expression presents a substantial statistical challenge. Building on random forests (RFs) and random intersection trees (RITs) and through extensive, biologically inspired simulations, we developed the iterative random forest algorithm (iRF). iRF trains a feature-weighted ensemble of decision trees to detect stable, high-order interactions with the same order of computational cost as the RF. We demonstrate the utility of iRF for high-order interaction discovery in two prediction problems: enhancer activity in the early Drosophila embryo and alternative splicing of primary transcripts in human-derived cell lines. In Drosophila, among the 20 pairwise transcription factor interactions iRF identifies as stable (returned in more than half of bootstrap replicates), 80\% have been previously reported as physical interactions. Moreover, third-order interactions, e.g., between Zelda (Zld), Giant (Gt), and Twist (Twi), suggest high-order relationships that are candidates for follow-up experiments. In human-derived cells, iRF rediscovered a central role of H3K36me3 in chromatin-mediated splicing regulation and identified interesting fifth- and sixth-order interactions, indicative of multivalent nucleosomes with specific roles in splicing regulation. By decoupling the order of interactions from the computational cost of identification, iRF opens additional avenues of inquiry into the molecular mechanisms underlying genome biology.},
	language = {en},
	number = {8},
	urldate = {2019-01-10},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Basu, Sumanta and Kumbier, Karl and Brown, James B. and Yu, Bin},
	month = feb,
	year = {2018},
	pmid = {29351989},
	keywords = {genomics, high-order interaction, interpretable machine learning, random forests, stability},
	pages = {1943--1948},
}

@inproceedings{raghu_evaluation_2018,
	title = {Evaluation of {Causal} {Structure} {Learning} {Methods} on {Mixed} {Data} {Types}},
	url = {http://proceedings.mlr.press/v92/raghu18a.html},
	abstract = {Causal structure learning algorithms are very important in many fields, including biomedical sciences, because they can uncover the underlying causal network structure from observational data. Seve...},
	language = {en},
	urldate = {2019-01-10},
	booktitle = {Proceedings of 2018 {ACM} {SIGKDD} {Workshop} on {Causal} {Discovery}},
	author = {Raghu, Vineet K. and Poon, Allen and Benos, Panayiotis V.},
	month = aug,
	year = {2018},
	pages = {48--65},
}

@article{oconnor_distinguishing_2018,
	title = {Distinguishing genetic correlation from causation across 52 diseases and complex traits},
	journal = {bioRxiv},
	author = {O'Connor, Luke J. and Price, Alkes L.},
	year = {2018},
	pages = {205435},
}

@article{zenil_causal_2019,
	title = {Causal deconvolution by algorithmic generative models},
	volume = {1},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-018-0005-0},
	doi = {10.1038/s42256-018-0005-0},
	abstract = {Most machine learning approaches extract statistical features from data, rather than the underlying causal mechanisms. A different approach analyses information in a general way by extracting recursive patterns from data using generative models under the paradigm of computability and algorithmic information theory.},
	language = {En},
	number = {1},
	urldate = {2019-01-08},
	journal = {Nature Machine Intelligence},
	author = {Zenil, Hector and Kiani, Narsis A. and Zea, Allan A. and Tegnér, Jesper},
	month = jan,
	year = {2019},
	pages = {58},
}

@article{williams_nonnegative_2010,
	title = {Nonnegative decomposition of multivariate information},
	journal = {arXiv preprint arXiv:1004.2515},
	author = {Williams, Paul L. and Beer, Randall D.},
	year = {2010},
}

@book{pearl_book_2018,
	title = {The {Book} of {Why}: {The} {New} {Science} of {Cause} and {Effect}},
	shorttitle = {The {Book} of {Why}},
	publisher = {Basic Books},
	author = {Pearl, Judea and Mackenzie, Dana},
	year = {2018},
}

@article{runge_escaping_2012,
	title = {Escaping the curse of dimensionality in estimating multivariate transfer entropy},
	volume = {108},
	number = {25},
	journal = {Physical review letters},
	author = {Runge, Jakob and Heitzig, Jobst and Petoukhov, Vladimir and Kurths, Jürgen},
	year = {2012},
	pages = {258701},
}

@article{petersen_causal_2014,
	title = {Causal {Models} and {Learning} from {Data}},
	volume = {25},
	issn = {1044-3983},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4077670/},
	doi = {10.1097/EDE.0000000000000078},
	abstract = {The practice of epidemiology requires asking causal questions. Formal frameworks for causal inference developed over the past decades have the potential to improve the rigor of this process. However, the appropriate role for formal causal thinking in applied epidemiology remains a matter of debate. We argue that a formal causal framework can help in designing a statistical analysis that comes as close as possible to answering the motivating causal question, while making clear what assumptions are required to endow the resulting estimates with a causal interpretation. A systematic approach for the integration of causal modeling with statistical estimation is presented. We highlight some common points of confusion that occur when causal modeling techniques are applied in practice and provide a broad overview on the types of questions that a causal framework can help to address. Our aims are to argue for the utility of formal causal thinking, to clarify what causal models can and cannot do, and to provide an accessible introduction to the flexible and powerful tools provided by causal models.},
	number = {3},
	urldate = {2018-09-25},
	journal = {Epidemiology (Cambridge, Mass.)},
	author = {Petersen, Maya L. and van der Laan, Mark J.},
	month = may,
	year = {2014},
	pmid = {24713881},
	pmcid = {PMC4077670},
	pages = {418--426},
}

@article{gessen_translating_2018,
	title = {Translating “{The} {Americans},” and {Seeing} a {Mirror} of {My} {Own} {American} {Experience}},
	issn = {0028-792X},
	url = {https://www.newyorker.com/news/our-columnists/translating-the-americans-and-seeing-a-mirror-of-my-own-american-experience},
	abstract = {The underlying assumption of the main characters’ lives is that nothing is true except that the U.S.S.R. will last forever. My parents based their decision to emigrate on the same premise.},
	language = {en},
	urldate = {2018-09-18},
	journal = {The New Yorker},
	author = {Gessen, Masha},
	month = jun,
	year = {2018},
	keywords = {languages, russian, soviet union russia, television, the americans, translation},
}

@article{strobl_approximate_2017,
	title = {Approximate {Kernel}-based {Conditional} {Independence} {Tests} for {Fast} {Non}-{Parametric} {Causal} {Discovery}},
	journal = {arXiv preprint arXiv:1702.03877},
	author = {Strobl, Eric V. and Zhang, Kun and Visweswaran, Shyam},
	year = {2017},
}

@misc{noauthor_laposte.net:_nodate,
	title = {Laposte.net: {Réception} (2535)},
	url = {https://webmail.laposte.net/mail#2},
	urldate = {2018-09-13},
}

@misc{noauthor_embolie_nodate,
	title = {L'embolie pulmonaire},
	url = {http://www.doctissimo.fr/html/sante/encyclopedie/sa_1575_embolie_pulmon.htm},
	abstract = {L'embolie pulmonaire signifie qu'un caillot circulant dans le sang va obstruer l'artère pulmonaire. Entre 70 000 et 100 000 cas d'embolie pulmonaire surviennent chaque année en France, dont 5\% sont potentiellement mortels. Des examens complémentaires permettront de confirmer le diagnostic avant d'initier en urgence des traitements anticoagulants, voire un traitement thrombolytique ou une embolectomie chirurgicale. On peut limiter les récidives en agissant sur les facteurs de risques et grâce à un traitement anticoagulant au long cours.},
	language = {fr},
	urldate = {2018-09-05},
}

@inproceedings{papaxanthos_finding_2016,
	title = {Finding significant combinations of features in the presence of categorical covariates},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Papaxanthos, Laetitia and Llinares-López, Felipe and Bodenham, Dean and Borgwardt, Karsten},
	year = {2016},
	pages = {2279--2287},
}

@article{sugiyama_finding_2017,
	title = {Finding {Significant} {Combinations} of {Continuous} {Features}},
	journal = {arXiv preprint arXiv:1702.08694},
	author = {Sugiyama, Mahito and Borgwardt, Karsten M.},
	year = {2017},
}

@article{gregorova_structured_2018,
	title = {Structured nonlinear variable selection},
	journal = {arXiv preprint arXiv:1805.06258},
	author = {Gregorová, Magda and Kalousis, Alexandros and Marchand-Maillet, Stéphane},
	year = {2018},
}

@article{runge_conditional_2017,
	title = {Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information},
	journal = {arXiv preprint arXiv:1709.01447},
	author = {Runge, Jakob},
	year = {2017},
}

@article{tsagris_constraint-based_2018,
	title = {Constraint-based causal discovery with mixed data},
	journal = {International Journal of Data Science and Analytics},
	author = {Tsagris, Michail and Borboudakis, Giorgos and Lagani, Vincenzo and Tsamardinos, Ioannis},
	year = {2018},
	keywords = {MXM},
	pages = {1--12},
}

@article{sedgewick_learning_2016,
	title = {Learning mixed graphical models with separate sparsity parameters and stability-based model selection},
	volume = {17},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/s12859-016-1039-0},
	doi = {10.1186/s12859-016-1039-0},
	abstract = {Mixed graphical models (MGMs) are graphical models learned over a combination of continuous and discrete variables. Mixed variable types are common in biomedical datasets. MGMs consist of a parameterized joint probability density, which implies a network structure over these heterogeneous variables. The network structure reveals direct associations between the variables and the joint probability density allows one to ask arbitrary probabilistic questions on the data. This information can be used for feature selection, classification and other important tasks.},
	number = {5},
	urldate = {2018-08-27},
	journal = {BMC Bioinformatics},
	author = {Sedgewick, Andrew J. and Shi, Ivy and Donovan, Rory M. and Benos, Panayiotis V.},
	month = jun,
	year = {2016},
	keywords = {MGM},
	pages = {S175},
}

@article{bhadra_inferring_2018,
	title = {Inferring network structure in non-normal and mixed discrete-continuous genomic data},
	volume = {74},
	number = {1},
	journal = {Biometrics},
	author = {Bhadra, Anindya and Rao, Arvind and Baladandayuthapani, Veerabhadran},
	year = {2018},
	pages = {185--195},
}

@article{kensert_transfer_2018,
	title = {Transfer learning with deep convolutional neural networks for classifying cellular morphological changes},
	copyright = {© 2018, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/early/2018/07/24/345728},
	doi = {10.1101/345728},
	abstract = {Quantification and identification of cellular phenotypes from high content microscopy images have proven to be very useful for understanding biological activity in response to different drug treatments. The traditional approach has been to use classical image analysis to quantify changes in cell morphology, which requires several non-trivial and independent analysis steps. Recently convolutional neural networks have emerged as a compelling alternative, offering good predictive performance and the possibility to replace traditional workflows with a single network architecture. In this study we applied the pre-trained deep convolutional neural networks ResNet50, InceptionV3 and InceptionResnetV2 to predict cell mechanisms of action in response to chemical perturbations for two cell profiling datasets from the Broad Bioimage Benchmark Collection. These networks were pre-trained on ImageNet enabling much quicker model training. We obtain higher predictive accuracy than previously reported, between 95 and 97\% based on 'leave-one-compound-out' cross-validation. The ability to quickly and accurately distinguish between different cell morphologies from a scarce amount of labelled data illustrates the combined benefit of transfer learning and deep convolutional neural networks for interrogating cell-based images.},
	language = {en},
	urldate = {2018-07-25},
	journal = {bioRxiv},
	author = {Kensert, Alexander and Harrison, Philip J. and Spjuth, Ola},
	month = jul,
	year = {2018},
	pages = {345728},
}

@article{colombo_order-independent_2014,
	title = {Order-independent constraint-based causal structure learning},
	volume = {15},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Colombo, Diego and Maathuis, Marloes H.},
	year = {2014},
	pages = {3741--3782},
}

@article{sun_causal_2015,
	title = {Causal {Network} {Inference} by {Optimal} {Causation} {Entropy}},
	volume = {14},
	url = {https://epubs.siam.org/doi/abs/10.1137/140956166},
	doi = {10.1137/140956166},
	abstract = {The broad abundance of time series data, which is in sharp contrast to limited knowledge of the underlying network dynamic processes that produce such observations, calls for a rigorous and efficient method of causal network inference. Here we develop mathematical theory of causation entropy, an information-theoretic statistic designed for model-free causality inference. For stationary Markov processes, we prove that for a given node in the network, its causal parents form the minimal set of nodes that maximizes causation entropy, a result we refer to as the optimal causation entropy principle. Furthermore, this principle guides us in developing computational and data efficient algorithms for causal network inference based on a two-step discovery and removal algorithm for time series data for a network-coupled dynamical system. Validation in terms of analytical and numerical results for Gaussian processes on large random networks highlights that inference by our algorithm outperforms previous leading methods, including conditional Granger causality and transfer entropy. Interestingly, our numerical results suggest that the number of samples required for accurate inference depends strongly on network characteristics such as the density of links and information diffusion rate and not necessarily on the number of nodes.},
	number = {1},
	urldate = {2018-07-05},
	journal = {SIAM Journal on Applied Dynamical Systems},
	author = {Sun, J. and Taylor, D. and Bollt, E.},
	month = jan,
	year = {2015},
	pages = {73--106},
}

@article{amjad_how_2018,
	title = {How ({Not}) {To} {Train} {Your} {Neural} {Network} {Using} the {Information} {Bottleneck} {Principle}},
	url = {http://arxiv.org/abs/1802.09766},
	abstract = {In this theory paper, we investigate training deep neural networks (DNNs) for classification via minimizing the information bottleneck (IB) functional. We show that, even if the joint distribution between continuous feature variables and the discrete class variable is known, the resulting optimization problem suffers from two severe issues: First, for deterministic DNNs, the IB functional is infinite for almost all weight matrices, making the optimization problem ill-posed. Second, the invariance of the IB functional under bijections prevents it from capturing desirable properties for classification, such as robustness, architectural simplicity, and simplicity of the learned representation. We argue that these issues are partly resolved for stochastic DNNs, DNNs that include a (hard or soft) decision rule, or by replacing the IB functional with related, but more well-behaved cost functions. We conclude that recent successes reported about training DNNs using the IB framework must be attributed to such solutions. As a side effect, our results imply limitations of the IB framework for the analysis of DNNs.},
	urldate = {2018-07-05},
	journal = {arXiv:1802.09766 [cs, math]},
	author = {Amjad, Rana Ali and Geiger, Bernhard C.},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.09766},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Theory, Computer Science - Machine Learning},
}

@article{sedgewick_mixed_2017,
	title = {Mixed {Graphical} {Models} for {Causal} {Analysis} of {Multi}-modal {Variables}},
	url = {http://arxiv.org/abs/1704.02621},
	abstract = {Graphical causal models are an important tool for knowledge discovery because they can represent both the causal relations between variables and the multivariate probability distributions over the data. Once learned, causal graphs can be used for classification, feature selection and hypothesis generation, while revealing the underlying causal network structure and thus allowing for arbitrary likelihood queries over the data. However, current algorithms for learning sparse directed graphs are generally designed to handle only one type of data (continuous-only or discrete-only), which limits their applicability to a large class of multi-modal biological datasets that include mixed type variables. To address this issue, we developed new methods that modify and combine existing methods for finding undirected graphs with methods for finding directed graphs. These hybrid methods are not only faster, but also perform better than the directed graph estimation methods alone for a variety of parameter settings and data set sizes. Here, we describe a new conditional independence test for learning directed graphs over mixed data types and we compare performances of different graph learning strategies on synthetic data.},
	urldate = {2018-07-05},
	journal = {arXiv:1704.02621 [cs, stat]},
	author = {Sedgewick, Andrew J. and Ramsey, Joseph D. and Spirtes, Peter and Glymour, Clark and Benos, Panayiotis V.},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.02621},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Machine Learning},
}

@article{ish-horowicz_mutual_2017,
	title = {Mutual {Information} {Estimation} {For} {Transcriptional} {Regulatory} {Network} {Inference}},
	copyright = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/early/2017/07/26/132647},
	doi = {10.1101/132647},
	abstract = {Mutual information-based network inference algorithms are an important tool in the reverse-engineering of transcriptional regulatory networks, but all rely on estimates of the mutual information between the expression of pairs of genes. Various methods exist to compute estimates of the mutual information, but none have been firmly established as optimal for network inference. The performance of 9 mutual information estimation methods are compared using three popular network inference algorithms: CLR, MRNET and ARACNE. The performance of the estimators is compared on one synthetic and two real datasets. For estimators that discretise data, the effect of discretisation parameters are also studied in detail. Implementations of 5 estimators are provided in parallelised C++ with an R interface. These are faster than alternative implementations, with reductions in computation time up to a factor of 3,500. The B-spline estimator consistently performs well on real and synthetic datasets. CLR was found to be the best performing inference algorithm, corroborating previous results indicating that it is the state of the art mutual inference algorithm. It is also found to be robust to the mutual information estimation method and their parameters. Furthermore, when using an estimator that discretises expression data, using N 1/3 bins for N samples gives the most accurate inferred network. This contradicts previous findings that suggested using N 1/2 bins.},
	language = {en},
	urldate = {2018-07-05},
	journal = {bioRxiv},
	author = {Ish-Horowicz, Jonathan and Reid, John},
	month = jul,
	year = {2017},
	pages = {132647},
}

@article{zhu_causal_2018,
	title = {Causal associations between risk factors and common diseases inferred from {GWAS} summary data},
	volume = {9},
	copyright = {2018 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-017-02317-2},
	doi = {10.1038/s41467-017-02317-2},
	abstract = {Genetic methods are useful to test whether risk factors are causal for or consequence of disease. Here, Zhu et al. develop a generalized summary-based Mendelian Randomization (GSMR) method which uses summary-level data from GWAS to test for causal associations of health risk factors with common diseases.},
	language = {en},
	number = {1},
	urldate = {2018-07-05},
	journal = {Nature Communications},
	author = {Zhu, Zhihong and Zheng, Zhili and Zhang, Futao and Wu, Yang and Trzaskowski, Maciej and Maier, Robert and Robinson, Matthew R. and McGrath, John J. and Visscher, Peter M. and Wray, Naomi R. and Yang, Jian},
	month = jan,
	year = {2018},
	pages = {224},
}

@inproceedings{poczos_nonparametric_2012,
	title = {Nonparametric {Estimation} of {Conditional} {Information} and {Divergences}.},
	booktitle = {{AISTATS}},
	author = {Póczos, Barnabás and Schneider, Jeff G.},
	year = {2012},
	pages = {914--923},
}

@article{sun_causal_2015-1,
	title = {Causal network inference by optimal causation entropy},
	volume = {14},
	number = {1},
	journal = {SIAM Journal on Applied Dynamical Systems},
	author = {Sun, Jie and Taylor, Dane and Bollt, Erik M.},
	year = {2015},
	pages = {73--106},
}

@article{bhadra_inferring_2018-1,
	title = {Inferring network structure in non-normal and mixed discrete-continuous genomic data},
	volume = {74},
	number = {1},
	journal = {Biometrics},
	author = {Bhadra, Anindya and Rao, Arvind and Baladandayuthapani, Veerabhadran},
	year = {2018},
	pages = {185--195},
}

@inproceedings{cui_copula_2016,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Copula {PC} {Algorithm} for {Causal} {Discovery} from {Mixed} {Data}},
	isbn = {978-3-319-46226-4 978-3-319-46227-1},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-46227-1_24},
	doi = {10.1007/978-3-319-46227-1_24},
	abstract = {We propose the ‘Copula PC’ algorithm for causal discovery from a combination of continuous and discrete data, assumed to be drawn from a Gaussian copula model. It is based on a two-step approach. The first step applies Gibbs sampling on rank-based data to obtain samples of correlation matrices. These are then translated into an average correlation matrix and an effective number of data points, which in the second step are input to the standard PC algorithm for causal discovery. A stable version naturally arises when rerunning the PC algorithm on different Gibbs samples. Our ‘Copula PC’ algorithm extends the ‘Rank PC’ algorithm, which has been designed for Gaussian copula models for purely continuous data. In simulations, ‘Copula PC’ indeed outperforms ‘Rank PC’ in cases with mixed variables, in particular for larger numbers of data points, at the expense of a slight increase in computation time.},
	language = {en},
	urldate = {2018-06-26},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer, Cham},
	author = {Cui, Ruifei and Groot, Perry and Heskes, Tom},
	month = sep,
	year = {2016},
	pages = {377--392},
}

@article{reshef_empirical_2018,
	title = {An empirical study of the maximal and total information coefficients and leading measures of dependence},
	volume = {12},
	number = {1},
	journal = {The Annals of Applied Statistics},
	author = {Reshef, David N. and Reshef, Yakir A. and Sabeti, Pardis C. and Mitzenmacher, Michael},
	year = {2018},
	pages = {123--155},
}

@article{wray_common_2018,
	title = {Common {Disease} {Is} {More} {Complex} {Than} {Implied} by the {Core} {Gene} {Omnigenic} {Model}},
	volume = {173},
	issn = {0092-8674},
	url = {https://www.sciencedirect.com/science/article/pii/S0092867418307141},
	doi = {10.1016/j.cell.2018.05.051},
	abstract = {The evidence that most adult-onset common diseases have a polygenic genetic architecture fully consistent with robust biological systems supported by multiple back-up mechanisms is now overwhelming. In this context, we consider the recent “omnigenic” or “core genes” model. A key assumption of the model is that there is a relatively small number of core genes relevant to any disease. While intuitively appealing, this model may underestimate the biological complexity of common disease, and therefore, the goal to discover core genes should not guide experimental design. We consider other implications of polygenicity, concluding that a focus on patient stratification is needed to achieve the goals of precision medicine.},
	number = {7},
	urldate = {2018-06-21},
	journal = {Cell},
	author = {Wray, Naomi R. and Wijmenga, Cisca and Sullivan, Patrick F. and Yang, Jian and Visscher, Peter M.},
	month = jun,
	year = {2018},
	pages = {1573--1580},
}

@article{moignard_decoding_2015,
	title = {Decoding the regulatory network of early blood development from single-cell gene expression measurements},
	volume = {33},
	number = {3},
	journal = {Nature biotechnology},
	author = {Moignard, Victoria and Woodhouse, Steven and Haghverdi, Laleh and Lilly, Andrew J. and Tanaka, Yosuke and Wilkinson, Adam C. and Buettner, Florian and Macaulay, Iain C. and Jawaid, Wajid and Diamanti, Evangelia},
	year = {2015},
	pages = {269},
}

@article{cui_learning_2018,
	title = {Learning causal structure from mixed data with missing values using {Gaussian} copula models},
	issn = {0960-3174, 1573-1375},
	url = {https://link.springer.com/article/10.1007/s11222-018-9810-x},
	doi = {10.1007/s11222-018-9810-x},
	abstract = {We consider the problem of causal structure learning from data with missing values, assumed to be drawn from a Gaussian copula model. First, we extend the ‘Rank PC’ algorithm, designed for Gaussian copula models with purely continuous data (so-called nonparanormal models), to incomplete data by applying rank correlation to pairwise complete observations and replacing the sample size with an effective sample size in the conditional independence tests to account for the information loss from missing values. When the data are missing completely at random (MCAR), we provide an error bound on the accuracy of ‘Rank PC’ and show its high-dimensional consistency. However, when the data are missing at random (MAR), ‘Rank PC’ fails dramatically. Therefore, we propose a Gibbs sampling procedure to draw correlation matrix samples from mixed data that still works correctly under MAR. These samples are translated into an average correlation matrix and an effective sample size, resulting in the ‘Copula PC’ algorithm for incomplete data. Simulation study shows that: (1) ‘Copula PC’ estimates a more accurate correlation matrix and causal structure than ‘Rank PC’ under MCAR and, even more so, under MAR and (2) the usage of the effective sample size significantly improves the performance of ‘Rank PC’ and ‘Copula PC.’ We illustrate our methods on two real-world datasets: riboflavin production data and chronic fatigue syndrome data.},
	language = {en},
	urldate = {2018-06-13},
	journal = {Statistics and Computing},
	author = {Cui, Ruifei and Groot, Perry and Heskes, Tom},
	month = mar,
	year = {2018},
	pages = {1--23},
}

@misc{noauthor_hacker_nodate,
	title = {Hacker {News}},
	url = {https://news.ycombinator.com/},
	urldate = {2018-06-12},
}

@incollection{venelli_efficient_2010,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Efficient {Entropy} {Estimation} for {Mutual} {Information} {Analysis} {Using} {B}-{Splines}},
	isbn = {978-3-642-12367-2 978-3-642-12368-9},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-12368-9_2},
	abstract = {The Correlation Power Analysis (CPA) is probably the most used side-channel attack because it seems to fit the power model of most standard CMOS devices and is very efficiently computed. However, the Pearson correlation coefficient used in the CPA measures only linear statistical dependences where the Mutual Information (MI) takes into account both linear and nonlinear dependences. Even if there can be simultaneously large correlation coefficients quantified by the correlation coefficient and weak dependences quantified by the MI, we can expect to get a more profound understanding about interactions from an MI Analysis (MIA). We study methods that improve the non-parametric Probability Density Functions (PDF) in the estimation of the entropies and, in particular, the use of B-spline basis functions as pdf estimators. Our results indicate an improvement of two fold in the number of required samples compared to a classic MI estimation. The B-spline smoothing technique can also be applied to the rencently introduced Cramér-von-Mises test.},
	language = {en},
	urldate = {2018-06-08},
	booktitle = {Information {Security} {Theory} and {Practices}. {Security} and {Privacy} of {Pervasive} {Systems} and {Smart} {Devices}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Venelli, Alexandre},
	month = apr,
	year = {2010},
	doi = {10.1007/978-3-642-12368-9_2},
	pages = {17--30},
}

@article{daub_estimating_2004,
	title = {Estimating mutual information using {B}-spline functions–an improved similarity measure for analysing gene expression data},
	volume = {5},
	number = {1},
	journal = {BMC bioinformatics},
	author = {Daub, Carsten O. and Steuer, Ralf and Selbig, Joachim and Kloska, Sebastian},
	year = {2004},
	pages = {118},
}

@article{cellucci_statistical_2005,
	title = {Statistical validation of mutual information calculations: {Comparison} of alternative numerical algorithms},
	volume = {71},
	shorttitle = {Statistical validation of mutual information calculations},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.71.066208},
	doi = {10.1103/PhysRevE.71.066208},
	abstract = {Given two time series X and Y, their mutual information, I(X,Y)=I(Y,X), is the average number of bits of X that can be predicted by measuring Y and vice versa. In the analysis of observational data, calculation of mutual information occurs in three contexts: identification of nonlinear correlation, determination of an optimal sampling interval, particularly when embedding data, and in the investigation of causal relationships with directed mutual information. In this contribution a minimum description length argument is used to determine the optimal number of elements to use when characterizing the distributions of X and Y. However, even when using partitions of the X and Y axis indicated by minimum description length, mutual information calculations performed with a uniform partition of the XY plane can give misleading results. This motivated the construction of an algorithm for calculating mutual information that uses an adaptive partition. This algorithm also incorporates an explicit test of the statistical independence of X and Y in a calculation that returns an assessment of the corresponding null hypothesis. The previously published Fraser-Swinney algorithm for calculating mutual information includes a sophisticated procedure for local adaptive control of the partitioning process. When the Fraser and Swinney algorithm and the algorithm constructed here are compared, they give very similar numerical results (less than 4\% difference in a typical application). Detailed comparisons are possible when X and Y are correlated jointly Gaussian distributed because an analytic expression for I(X,Y) can be derived for that case. Based on these tests, three conclusions can be drawn. First, the algorithm constructed here has an advantage over the Fraser-Swinney algorithm in providing an explicit calculation of the probability of the null hypothesis that X and Y are independent. Second, the Fraser-Swinney algorithm is marginally the more accurate of the two algorithms when large data sets are used. With smaller data sets, however, the Fraser-Swinney algorithm reports structures that disappear when more data are available. Third, the algorithm constructed here requires about 0.5\% of the computation time required by the Fraser-Swinney algorithm.},
	number = {6},
	urldate = {2018-06-07},
	journal = {Physical Review E},
	author = {Cellucci, C. J. and Albano, A. M. and Rapp, P. E.},
	month = jun,
	year = {2005},
	pages = {066208},
}

@article{vlachos_nonuniform_2010,
	title = {Nonuniform state-space reconstruction and coupling detection},
	volume = {82},
	number = {1},
	journal = {Physical Review E},
	author = {Vlachos, Ioannis and Kugiumtzis, Dimitris},
	year = {2010},
	pages = {016207},
}

@article{budden_information_2016,
	title = {Information theoretic approaches for inference of biological networks from continuous-valued data},
	volume = {10},
	issn = {1752-0509},
	url = {https://doi.org/10.1186/s12918-016-0331-y},
	doi = {10.1186/s12918-016-0331-y},
	abstract = {Characterising programs of gene regulation by studying individual protein-DNA and protein-protein interactions would require a large volume of high-resolution proteomics data, and such data are not yet available. Instead, many gene regulatory network (GRN) techniques have been developed, which leverage the wealth of transcriptomic data generated by recent consortia to study indirect, gene-level relationships between transcriptional regulators. Despite the popularity of such methods, previous methods of GRN inference exhibit limitations that we highlight and address through the lens of information theory.},
	urldate = {2018-06-06},
	journal = {BMC Systems Biology},
	author = {Budden, David M. and Crampin, Edmund J.},
	month = sep,
	year = {2016},
	keywords = {Gene expression, Gene regulatory network, Transcriptional regulation},
	pages = {89},
}

@article{jo_measuring_2017,
	title = {Measuring the tendency of {CNNs} to {Learn} {Surface} {Statistical} {Regularities}},
	url = {http://arxiv.org/abs/1711.11561},
	abstract = {Deep CNNs are known to exhibit the following peculiarity: on the one hand they generalize extremely well to a test set, while on the other hand they are extremely sensitive to so-called adversarial perturbations. The extreme sensitivity of high performance CNNs to adversarial examples casts serious doubt that these networks are learning high level abstractions in the dataset. We are concerned with the following question: How can a deep CNN that does not learn any high level semantics of the dataset manage to generalize so well? The goal of this article is to measure the tendency of CNNs to learn surface statistical regularities of the dataset. To this end, we use Fourier filtering to construct datasets which share the exact same high level abstractions but exhibit qualitatively different surface statistical regularities. For the SVHN and CIFAR-10 datasets, we present two Fourier filtered variants: a low frequency variant and a randomly filtered variant. Each of the Fourier filtering schemes is tuned to preserve the recognizability of the objects. Our main finding is that CNNs exhibit a tendency to latch onto the Fourier image statistics of the training dataset, sometimes exhibiting up to a 28\% generalization gap across the various test sets. Moreover, we observe that significantly increasing the depth of a network has a very marginal impact on closing the aforementioned generalization gap. Thus we provide quantitative evidence supporting the hypothesis that deep CNNs tend to learn surface statistical regularities in the dataset rather than higher-level abstract concepts.},
	urldate = {2018-05-30},
	journal = {arXiv:1711.11561 [cs, stat]},
	author = {Jo, Jason and Bengio, Yoshua},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.11561},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{vejmelka_inferring_2008,
	title = {Inferring the directionality of coupling with conditional mutual information},
	volume = {77},
	number = {2},
	journal = {Physical Review E},
	author = {Vejmelka, Martin and Paluš, Milan},
	year = {2008},
	pages = {026214},
}

@article{kontkanen_linear-time_2007,
	title = {A linear-time algorithm for computing the multinomial stochastic complexity},
	volume = {103},
	number = {6},
	journal = {Information Processing Letters},
	author = {Kontkanen, Petri and Myllymäki, Petri},
	year = {2007},
	pages = {227--233},
}

@article{moon_estimation_1995,
	title = {Estimation of mutual information using kernel density estimators},
	volume = {52},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.52.2318},
	doi = {10.1103/PhysRevE.52.2318},
	abstract = {Mutual information is useful for investigating the dependence between two experimental time series. It is often used to establish an appropriate time delay in phase-portrait reconstruction from time-series data. A histogram based approach has been used so far to estimate the probabilities. It is shown here that kernel density estimation of the probability density functions needed in estimating the average mutual information across two coordinates can be more effective than the histogram method of Fraser and Swinney [Phys. Rev. A 33, 1134 (1986)].},
	number = {3},
	urldate = {2018-06-04},
	journal = {Physical Review E},
	author = {Moon, Young-Il and Rajagopalan, Balaji and Lall, Upmanu},
	month = sep,
	year = {1995},
	pages = {2318--2321},
}

@article{janzing_quantifying_2013,
	title = {Quantifying causal influences},
	journal = {The Annals of Statistics},
	author = {Janzing, Dominik and Balduzzi, David and Grosse-Wentrup, Moritz and Schölkopf, Bernhard},
	year = {2013},
	pages = {2324--2358},
}

@article{janzing_quantifying_2013-1,
	title = {Quantifying causal influences},
	journal = {The Annals of Statistics},
	author = {Janzing, Dominik and Balduzzi, David and Grosse-Wentrup, Moritz and Schölkopf, Bernhard},
	year = {2013},
	pages = {2324--2358},
}

@article{janzing_quantifying_2013-2,
	title = {Quantifying causal influences},
	journal = {The Annals of Statistics},
	author = {Janzing, Dominik and Balduzzi, David and Grosse-Wentrup, Moritz and Schölkopf, Bernhard},
	year = {2013},
	pages = {2324--2358},
}

@article{liu_inference_2016,
	title = {Inference of {Gene} {Regulatory} {Network} {Based} on {Local} {Bayesian} {Networks}},
	volume = {12},
	issn = {1553-7358},
	url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005024},
	doi = {10.1371/journal.pcbi.1005024},
	abstract = {The inference of gene regulatory networks (GRNs) from expression data can mine the direct regulations among genes and gain deep insights into biological processes at a network level. During past decades, numerous computational approaches have been introduced for inferring the GRNs. However, many of them still suffer from various problems, e.g., Bayesian network (BN) methods cannot handle large-scale networks due to their high computational complexity, while information theory-based methods cannot identify the directions of regulatory interactions and also suffer from false positive/negative problems. To overcome the limitations, in this work we present a novel algorithm, namely local Bayesian network (LBN), to infer GRNs from gene expression data by using the network decomposition strategy and false-positive edge elimination scheme. Specifically, LBN algorithm first uses conditional mutual information (CMI) to construct an initial network or GRN, which is decomposed into a number of local networks or GRNs. Then, BN method is employed to generate a series of local BNs by selecting the k-nearest neighbors of each gene as its candidate regulatory genes, which significantly reduces the exponential search space from all possible GRN structures. Integrating these local BNs forms a tentative network or GRN by performing CMI, which reduces redundant regulations in the GRN and thus alleviates the false positive problem. The final network or GRN can be obtained by iteratively performing CMI and local BN on the tentative network. In the iterative process, the false or redundant regulations are gradually removed. When tested on the benchmark GRN datasets from DREAM challenge as well as the SOS DNA repair network in E.coli, our results suggest that LBN outperforms other state-of-the-art methods (ARACNE, GENIE3 and NARROMI) significantly, with more accurate and robust performance. In particular, the decomposition strategy with local Bayesian networks not only effectively reduce the computational cost of BN due to much smaller sizes of local GRNs, but also identify the directions of the regulations.},
	language = {en},
	number = {8},
	urldate = {2018-05-30},
	journal = {PLOS Computational Biology},
	author = {Liu, Fei and Zhang, Shao-Wu and Guo, Wei-Feng and Wei, Ze-Gang and Chen, Luonan},
	month = aug,
	year = {2016},
	keywords = {Algorithms, Gene expression, Gene regulation, Gene regulatory networks, Genetic networks, Graphs, Probability distribution, Regulator genes},
	pages = {e1005024},
}

@article{liu_inference_2016-1,
	title = {Inference of gene regulatory network based on local bayesian networks},
	volume = {12},
	number = {8},
	journal = {PLoS computational biology},
	author = {Liu, Fei and Zhang, Shao-Wu and Guo, Wei-Feng and Wei, Ze-Gang and Chen, Luonan},
	year = {2016},
	pages = {e1005024},
}

@article{zhang_conditional_2015,
	title = {Conditional mutual inclusive information enables accurate quantification of associations in gene regulatory networks},
	volume = {43},
	issn = {0305-1048},
	url = {https://academic.oup.com/nar/article/43/5/e31/2453158},
	doi = {10.1093/nar/gku1315},
	abstract = {Abstract.  Mutual information (MI), a quantity describing the nonlinear dependence between two random variables, has been widely used to construct gene regulato},
	language = {en},
	number = {5},
	urldate = {2018-05-30},
	journal = {Nucleic Acids Research},
	author = {Zhang, Xiujun and Zhao, Juan and Hao, Jin-Kao and Zhao, Xing-Ming and Chen, Luonan},
	month = mar,
	year = {2015},
	pages = {e31--e31},
}

@article{zhang_inferring_2012,
	title = {Inferring gene regulatory networks from gene expression data by path consistency algorithm based on conditional mutual information},
	volume = {28},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/28/1/98/221936},
	doi = {10.1093/bioinformatics/btr626},
	abstract = {Abstract.  Motivation: Reconstruction of gene regulatory networks (GRNs), which explicitly represent the causality of developmental or regulatory process, is of},
	language = {en},
	number = {1},
	urldate = {2018-05-30},
	journal = {Bioinformatics},
	author = {Zhang, Xiujun and Zhao, Xing-Ming and He, Kun and Lu, Le and Cao, Yongwei and Liu, Jingdong and Hao, Jin-Kao and Liu, Zhi-Ping and Chen, Luonan},
	month = jan,
	year = {2012},
	pages = {98--104},
}

@article{arbel_gradient_2018,
	title = {On gradient regularizers for {MMD} {GANs}},
	url = {http://arxiv.org/abs/1805.11565},
	abstract = {We propose a principled method for gradient-based regularization of the critic of GAN-like models trained by adversarially optimizing the kernel of a Maximum Mean Discrepancy (MMD). Our method is based on studying the behavior of the optimized MMD, and constrains the gradient based on analytical results rather than an optimization penalty. Experimental results show that the proposed regularization leads to stable training and outperforms state-of-the art methods on image generation, including on \$160 {\textbackslash}times 160\$ CelebA and \$64 {\textbackslash}times 64\$ ImageNet.},
	urldate = {2018-05-30},
	journal = {arXiv:1805.11565 [cs, stat]},
	author = {Arbel, Michael and Sutherland, Dougal J. and Bińkowski, Mikołaj and Gretton, Arthur},
	month = may,
	year = {2018},
	note = {arXiv: 1805.11565},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{tibshirani_regression_1996,
	title = {Regression shrinkage and selection via the lasso},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	pages = {267--288},
}

@article{tibshirani_regression_1996-1,
	title = {Regression shrinkage and selection via the lasso},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	pages = {267--288},
}

@article{tibshirani_sparsity_2005,
	title = {Sparsity and smoothness via the fused lasso},
	volume = {67},
	number = {1},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Tibshirani, Robert and Saunders, Michael and Rosset, Saharon and Zhu, Ji and Knight, Keith},
	year = {2005},
	pages = {91--108},
}

@article{xu_robust_2018,
	title = {Robust {GANs} against {Dishonest} {Adversaries}},
	url = {http://arxiv.org/abs/1802.09700},
	abstract = {Robustness of deep learning models is a property that has recently gained increasing attention. We formally define a notion of robustness for generative adversarial models, and show that, perhaps surprisingly, the GAN in its original form is not robust. Indeed, the discriminator in GANs may be viewed as merely offering "teaching feedback". Our notion of robustness relies on a dishonest discriminator, or noisy, adversarial interference with its feedback. We explore, theoretically and empirically, the effect of model and training properties on this robustness. In particular, we show theoretical conditions for robustness that are supported by empirical evidence. We also test the effect of regularization. Our results suggest variations of GANs that are indeed more robust to noisy attacks, and have overall more stable training behavior.},
	urldate = {2018-05-30},
	journal = {arXiv:1802.09700 [cs, stat]},
	author = {Xu, Zhi and Li, Chengtao and Jegelka, Stefanie},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.09700},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{lachmann_aracne-ap:_2016,
	title = {{ARACNe}-{AP}: gene network reverse engineering through adaptive partitioning inference of mutual information},
	volume = {32},
	issn = {1367-4803},
	shorttitle = {{ARACNe}-{AP}},
	url = {https://academic.oup.com/bioinformatics/article/32/14/2233/1743418},
	doi = {10.1093/bioinformatics/btw216},
	abstract = {Abstract.  Summary: The accurate reconstruction of gene regulatory networks from large scale molecular profile datasets represents one of the grand challenges o},
	language = {en},
	number = {14},
	urldate = {2018-05-29},
	journal = {Bioinformatics},
	author = {Lachmann, Alexander and Giorgi, Federico M. and Lopez, Gonzalo and Califano, Andrea},
	month = jul,
	year = {2016},
	pages = {2233--2235},
}

@article{tee_is_2018,
	title = {Is {Information} in the {Brain} {Represented} in {Continuous} or {Discrete} {Form}?},
	url = {http://arxiv.org/abs/1805.01631},
	abstract = {The question of continuous-versus-discrete information representation in the brain is a fundamental yet unresolved physiological question. Historically, most analyses assume a continuous representation without considering the alternative possibility of a discrete representation. Our work explores the plausibility of both representations, and answers the question from a communications engineering perspective. Drawing on the well-established Shannon's communications theory, we posit that information in the brain is represented in a discrete form. Using a computer simulation, we show that information cannot be communicated reliably between neurons using a continuous representation, due to the presence of noise; neural information has to be in a discrete form. In addition, we designed 3 (human) behavioral experiments on probability estimation and analyzed the data using a novel discrete (quantized) model of probability. Under a discrete model of probability, two distinct probabilities (say, 0.57 and 0.58) are treated indifferently. We found that data from all participants were better fit to discrete models than continuous ones. Furthermore, we re-analyzed the data from a published (human) behavioral study on intertemporal choice using a novel discrete (quantized) model of intertemporal choice. Under such a model, two distinct time delays (say, 16 days and 17 days) are treated indifferently. We found corroborating results, showing that data from all participants were better fit to discrete models than continuous ones. In summary, all results reported here support our discrete hypothesis of information representation in the brain, which signifies a major demarcation from the current understanding of the brain's physiology.},
	urldate = {2018-05-28},
	journal = {arXiv:1805.01631 [cs, math, q-bio]},
	author = {Tee, James and Taylor, Desmond P.},
	month = may,
	year = {2018},
	note = {arXiv: 1805.01631},
	keywords = {Computer Science - Information Theory, Quantitative Biology - Neurons and Cognition},
}

@article{li_network-constrained_2008,
	title = {Network-constrained regularization and variable selection for analysis of genomic data},
	volume = {24},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/24/9/1175/206444},
	doi = {10.1093/bioinformatics/btn081},
	abstract = {Abstract.  Motivation: Graphs or networks are common ways of depicting information. In biology in particular, many different biological processes are represente},
	language = {en},
	number = {9},
	urldate = {2018-05-28},
	journal = {Bioinformatics},
	author = {Li, Caiyan and Li, Hongzhe},
	month = may,
	year = {2008},
	pages = {1175--1182},
}

@inproceedings{feldman_coresets_2010,
	title = {Coresets and sketches for high dimensional subspace approximation problems},
	booktitle = {Proceedings of the twenty-first annual {ACM}-{SIAM} symposium on {Discrete} {Algorithms}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Feldman, Dan and Monemizadeh, Morteza and Sohler, Christian and Woodruff, David P.},
	year = {2010},
	pages = {630--649},
}

@misc{noauthor_coresets_nodate,
	title = {Coresets and {Sketches} for {High} {Dimensional} {SubspaceApproximation} {Problems} - {Google} {Search}},
	url = {https://www.google.com/search?client=ubuntu&channel=fs&q=Coresets+and+Sketches+for+High+Dimensional+SubspaceApproximation+Problems&ie=utf-8&oe=utf-8},
	urldate = {2018-05-28},
}

@article{marinazzo_kernel_2008,
	title = {Kernel {Method} for {Nonlinear} {Granger} {Causality}},
	volume = {100},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.100.144103},
	doi = {10.1103/PhysRevLett.100.144103},
	abstract = {Important information on the structure of complex systems can be obtained by measuring to what extent the individual components exchange information among each other. The linear Granger approach, to detect cause-effect relationships between time series, has emerged in recent years as a leading statistical technique to accomplish this task. Here we generalize Granger causality to the nonlinear case using the theory of reproducing kernel Hilbert spaces. Our method performs linear Granger causality in the feature space of suitable kernel functions, assuming arbitrary degree of nonlinearity. We develop a new strategy to cope with the problem of overfitting, based on the geometry of reproducing kernel Hilbert spaces. Applications to coupled chaotic maps and physiological data sets are presented.},
	number = {14},
	urldate = {2018-05-28},
	journal = {Physical Review Letters},
	author = {Marinazzo, Daniele and Pellicoro, Mario and Stramaglia, Sebastiano},
	month = apr,
	year = {2008},
	pages = {144103},
}

@inproceedings{kontkanen_mdl_2007,
	title = {{MDL} histogram density estimation},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	author = {Kontkanen, Petri and Myllymäki, Petri},
	year = {2007},
	pages = {219--226},
}

@article{li_application_2017,
	title = {Application of t-{SNE} to {Human} {Genetic} {Data}},
	copyright = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/early/2017/03/08/114884},
	doi = {10.1101/114884},
	abstract = {{\textless}p{\textgreater}The t-SNE (t-distributed stochastic neighbor embedding) is a new dimension reduction and visualization technique for high-dimensional data. t-SNE is rarely applied to human genetic data, even though it is commonly used in other data-intensive biological fields, such as single-cell genomics. We explore the applicability of t-SNE to human genetic data and make these observations: (i) similar to previously used dimension reduction techniques such as principal component analysis (PCA), t-SNE is able to separate samples from different continents; (ii) unlike PCA, t-SNE is more robust with respect to the presence of outliers; (iii) t-SNE is able to display both continental and sub-continental patterns in a single plot. We conclude that the ability for t-SNE to reveal population stratification at different scales could be useful for human genetic association studies.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2018-05-22},
	journal = {bioRxiv},
	author = {Li, Wentian and Cerise, Jane E. and Yang, Yaning and Han, Henry},
	month = mar,
	year = {2017},
	pages = {114884},
}

@article{kunkle_meta-analysis_2018,
	title = {Meta-analysis of genetic association with diagnosed {Alzheimer}'s disease identifies novel risk loci and implicates {Abeta}, {Tau}, immunity and lipid processing},
	copyright = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/early/2018/04/05/294629},
	doi = {10.1101/294629},
	abstract = {{\textless}p{\textgreater}Late-onset Alzheimers disease (LOAD, onset age \&gt; 60 years) is the most prevalent dementia in the elderly, and risk is partially driven by genetics. Many of the loci responsible for this genetic risk were identified by genome-wide association studies (GWAS). To identify additional LOAD risk loci, we performed the largest GWAS to date (89,769 individuals), analyzing both common and rare variants. We confirm 20 previous LOAD risk loci and identify four new genome-wide loci (\textit{IQCK}, \textit{ACE}, \textit{ADAM10}, and \textit{ADAMTS1}). Pathway analysis of these data implicates the immune system and lipid metabolism, and for the first time tau binding proteins and APP metabolism. These findings show that genetic variants affecting APP and Abeta processing are not only associated with early-onset autosomal dominant AD but also with LOAD. Analysis of AD risk genes and pathways show enrichment for rare variants (P = 1.32 x 10$^{\textrm{-7}}$) indicating that additional rare variants remain to be identified.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2018-05-22},
	journal = {bioRxiv},
	author = {Kunkle, Brian W. and Grenier-Boley, Benjamin and Sims, Rebecca and Bis, Joshua C. and Naj, Adam C. and Boland, Anne and Vronskaya, Maria and Lee, Sven J. van der and Amlie-Wolf, Alex and Bellenguez, Celine and Frizatti, Aura and Chouraki, Vincent and Consortium (ADGC), Alzheimer's Disease Genetics and Initiative (EADI), European Alzheimer's Disease and Consortium (CHARGE), Cohorts for Heart {and} Aging Research in Genomic Epidemiology and Consortium (GERAD/PERADES), Genetic {and} Environmental Risk in Alzheimer's Disease and Schmidt, Helena and Hakonarson, Hakon and Munger, Ron and Schmidt, Reinhold and Farrer, Lindsay A. and Broeckhoven, Christine Van and O'Donovan, Michael C. and Destefano, Anita L. and Jones, Lesley and Haines, Jonathan L. and Deleuze, Jean-Francois and Owen, Michael J. and Gudnason, Vilmundur and Mayeux, Richard P. and Escott-Price, Valentina and Psaty, Bruce M. and Ruiz, Agustin and Ramirez, Alfredo and Wang, Li-San and Duijn, Cornelia M. van and Holmans, Peter A. and Seshadri, Sudha and Williams, Julie and Amouyel, Philippe and Schellenberg, Gerard D. and Lambert, Jean-Charles and Pericak-Vance, Margaret A.},
	month = apr,
	year = {2018},
	pages = {294629},
}

@article{fortune_simgwas:_2018,
	title = {{simGWAS}: a fast method for simulation of large scale case-control {GWAS} summary statistics},
	copyright = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {{simGWAS}},
	url = {https://www.biorxiv.org/content/early/2018/05/02/313023},
	doi = {10.1101/313023},
	abstract = {{\textless}p{\textgreater}Methods for analysis of GWAS summary statistics have encouraged data sharing and democratised the analysis of different diseases. Ideal validation for such methods is application to simulated data, where some "truth" is known. As GWAS increase in size, so does the computational complexity of such evaluations; standard practice repeatedly simulates and analyses genotype data for all individuals in an example study. We have developed a novel method based on an alternative approach, directly simulating GWAS summary data, without individual data as an intermediate step. We mathematically derive the expected statistics for any set of causal variants and their effect sizes, conditional upon control haplotype frequencies (available from public reference datasets). Simulation of GWAS summary output can be conducted independently of sample size by simulating random variates about these expected values. Across a range of scenarios, our method, available as an open source R package, produces very similar output to that from simulating individual genotypes with a substantial gain in speed even for modest sample sizes. Fast simulation of GWAS summary statistics will enable more complete and rapid evaluation of summary statistic methods as well as opening new potential avenues of research in fine mapping and gene set enrichment analysis.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2018-05-22},
	journal = {bioRxiv},
	author = {Fortune, Mary D. and Wallace, Chris},
	month = may,
	year = {2018},
	pages = {313023},
}

@article{speed_exposing_2018,
	title = {Exposing flaws in {S}-{LDSC}; reply to {Gazal} et al.},
	copyright = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/early/2018/03/27/280784},
	doi = {10.1101/280784},
	abstract = {{\textless}p{\textgreater}In our recent publication, we examined the two heritability models most widely used when estimating SNP heritability: the GCTA Model, which is used by the software GCTA and upon which LD Score regression (LDSC) is based, and the LDAK Model, which is used by our software LDAK. First we demonstrated the importance of choosing an appropriate heritability model, by showing that estimates of SNP heritability can be highly sensitive to which model is assumed. Then we empirically tested the GCTA and LDAK Models on GWAS data for a wide variety of complex traits. We found that the LDAK Model fits real data both significantly and substantially better than the GCTA Model, indicating that LDAK estimates more accurately describe the genetic architecture of complex traits than those from GCTA or LDSC. Some of our most striking results were our revised estimates of functional enrichments (the heritability enrichments of SNP categories defined by functional annotations). In general, estimates from LDAK were substantially more modest than previous estimates based on the GCTA Model. For example, we estimated that DNase I hypersensitive sites (DHS) were 1.4-fold (SD 0.1) enriched, whereas a study using GCTA had found they were 5.1-fold (SD 0.5) enriched, and we estimated that conserved SNPs were 1.3-fold (SD 0.3) enriched, whereas a study using S-LDSC (stratified LDSC) had found they were 13.3-fold (SD 1.5) enriched. In their correspondence, Gazal et al. dispute our findings. They assert that the heritability model assumed by LDSC is more realistic than the LDAK Model, and that estimates of enrichment from S-LDSC are more accurate than those from LDAK. Here, we explain why their justification for preferring the model used by LDSC is incorrect, and provide a simple demonstration that S-LDSC produces unreliable estimates of enrichment.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2018-05-22},
	journal = {bioRxiv},
	author = {Speed, Doug and Balding, David},
	month = mar,
	year = {2018},
	pages = {280784},
}

@inproceedings{kontkanen_mdl_2007-1,
	title = {{MDL} histogram density estimation},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	author = {Kontkanen, Petri and Myllymäki, Petri},
	year = {2007},
	pages = {219--226},
}

@article{tsimpiris_nearest_2012,
	title = {Nearest neighbor estimate of conditional mutual information in feature selection},
	volume = {39},
	number = {16},
	journal = {Expert Systems with Applications},
	author = {Tsimpiris, Alkiviadis and Vlachos, Ioannis and Kugiumtzis, Dimitris},
	year = {2012},
	pages = {12697--12708},
}

@article{ju_somatic_2017,
	title = {Somatic mutations reveal asymmetric cellular dynamics in the early human embryo},
	volume = {543},
	copyright = {2017 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature21703},
	doi = {10.1038/nature21703},
	abstract = {Somatic cells acquire mutations throughout the course of an individual’s life. Mutations occurring early in embryogenesis are often present in a substantial proportion of, but not all, cells in postnatal humans and thus have particular characteristics and effects1. Depending on their location in the genome and the proportion of cells they are present in, these mosaic mutations can cause a wide range of genetic disease syndromes2 and predispose carriers to cancer3,4. They have a high chance of being transmitted to offspring as de novo germline mutations and, in principle, can provide insights into early human embryonic cell lineages and their contributions to adult tissues5. Although it is known that gross chromosomal abnormalities are remarkably common in early human embryos6, our understanding of early embryonic somatic mutations is very limited. Here we use whole-genome sequences of normal blood from 241 adults to identify 163 early embryonic mutations. We estimate that approximately three base substitution mutations occur per cell per cell-doubling event in early human embryogenesis and these are mainly attributable to two known mutational signatures7. We used the mutations to reconstruct developmental lineages of adult cells and demonstrate that the two daughter cells of many early embryonic cell-doubling events contribute asymmetrically to adult blood at an approximately 2:1 ratio. This study therefore provides insights into the mutation rates, mutational processes and developmental outcomes of cell dynamics that operate during early human embryogenesis.},
	language = {en},
	number = {7647},
	urldate = {2018-05-14},
	journal = {Nature},
	author = {Ju, Young Seok and Martincorena, Inigo and Gerstung, Moritz and Petljak, Mia and Alexandrov, Ludmil B. and Rahbari, Raheleh and Wedge, David C. and Davies, Helen R. and Ramakrishna, Manasa and Fullam, Anthony and Martin, Sancha and Alder, Christopher and Patel, Nikita and Gamble, Steve and O’Meara, Sarah and Giri, Dilip D. and Sauer, Torril and Pinder, Sarah E. and Purdie, Colin A. and Borg, Åke and Stunnenberg, Henk and Vijver, Marc van de and Tan, Benita K. T. and Caldas, Carlos and Tutt, Andrew and Ueno, Naoto T. and Veer, Laura J. van ’t and Martens, John W. M. and Sotiriou, Christos and Knappskog, Stian and Span, Paul N. and Lakhani, Sunil R. and Eyfjörd, Jórunn Erla and Børresen-Dale, Anne-Lise and Richardson, Andrea and Thompson, Alastair M. and Viari, Alain and Hurles, Matthew E. and Nik-Zainal, Serena and Campbell, Peter J. and Stratton, Michael R.},
	month = mar,
	year = {2017},
	pages = {714--718},
}

@article{julia_sincell:_2015,
	title = {Sincell: an {R}/{Bioconductor} package for statistical assessment of cell-state hierarchies from single-cell {RNA}-seq},
	volume = {31},
	issn = {1367-4803},
	shorttitle = {Sincell},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4595899/},
	doi = {10.1093/bioinformatics/btv368},
	abstract = {Summary: Cell differentiation processes are achieved through a continuum of hierarchical intermediate cell states that might be captured by single-cell RNA seq. Existing computational approaches for the assessment of cell-state hierarchies from single-cell data can be formalized under a general framework composed of (i) a metric to assess cell-to-cell similarities (with or without a dimensionality reduction step) and (ii) a graph-building algorithm (optionally making use of a cell clustering step). The Sincell R package implements a methodological toolbox allowing flexible workflows under such a framework. Furthermore, Sincell contributes new algorithms to provide cell-state hierarchies with statistical support while accounting for stochastic factors in single-cell RNA seq. Graphical representations and functional association tests are provided to interpret hierarchies. The functionalities of Sincell are illustrated in a real case study, which demonstrates its ability to discriminate noisy from stable cell-state hierarchies., Availability and implementation:
Sincell is an open-source R/Bioconductor package available at http://bioconductor.org/packages/sincell. A detailed manual and a vignette are provided with the package., Contact:
antonio.rausell@isb-sib.ch, Supplementary information: Supplementary data are available at Bioinformatics online.},
	number = {20},
	urldate = {2018-05-09},
	journal = {Bioinformatics},
	author = {Juliá, Miguel and Telenti, Amalio and Rausell, Antonio},
	month = oct,
	year = {2015},
	pmid = {26099264},
	pmcid = {PMC4595899},
	pages = {3380--3382},
}

@article{binkowski_demystifying_2018,
	title = {Demystifying {MMD} {GANs}},
	url = {http://arxiv.org/abs/1801.01401},
	abstract = {We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs. As our main theoretical contribution, we clarify the situation with bias in GAN loss functions raised by recent work: we show that gradient estimators used in the optimization process for both MMD GANs and Wasserstein GANs are unbiased, but learning a discriminator based on samples leads to biased gradients for the generator parameters. We also discuss the issue of kernel choice for the MMD critic, and characterize the kernel corresponding to the energy distance used for the Cramer GAN critic. Being an integral probability metric, the MMD benefits from training strategies recently developed for Wasserstein GANs. In experiments, the MMD GAN is able to employ a smaller critic network than the Wasserstein GAN, resulting in a simpler and faster-training algorithm with matching performance. We also propose an improved measure of GAN convergence, the Kernel Inception Distance, and show how to use it to dynamically adapt learning rates during GAN training.},
	urldate = {2018-05-04},
	journal = {arXiv:1801.01401 [cs, stat]},
	author = {Bińkowski, Mikołaj and Sutherland, Dougal J. and Arbel, Michael and Gretton, Arthur},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.01401},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{tran_implicit_2018,
	title = {Implicit {Causal} {Models} for {Genome}-wide {Association} {Studies}},
	url = {https://openreview.net/forum?id=SyELrEeAb},
	abstract = {Progress in probabilistic generative models has accelerated, developing richer models with neural architectures, implicit densities, and with scalable algorithms for their Bayesian inference....},
	urldate = {2018-05-04},
	author = {Tran, Dustin and Blei, David M.},
	month = feb,
	year = {2018},
}

@article{arjovsky_towards_2017,
	title = {Towards principled methods for training generative adversarial networks},
	journal = {arXiv preprint arXiv:1701.04862},
	author = {Arjovsky, Martin and Bottou, Léon},
	year = {2017},
}

@article{saxe_information_2018,
	title = {On the {Information} {Bottleneck} {Theory} of {Deep} {Learning}},
	url = {https://openreview.net/forum?id=ry_WPG-A-&noteId=ry_WPG-A-},
	abstract = {The practical successes of deep neural networks have not been matched by theoretical progress that satisfyingly explains their behavior. In this work, we study the information bottleneck (IB)...},
	urldate = {2018-04-16},
	author = {Saxe, Andrew Michael and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan Daniel and Cox, David Daniel},
	month = feb,
	year = {2018},
}

@article{tishby_deep_2015,
	title = {Deep {Learning} and the {Information} {Bottleneck} {Principle}},
	url = {http://arxiv.org/abs/1503.02406},
	abstract = {Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
	urldate = {2018-04-16},
	journal = {arXiv:1503.02406 [cs]},
	author = {Tishby, Naftali and Zaslavsky, Noga},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02406},
	keywords = {Computer Science - Learning, Information bottleneck},
}

@article{shwartz-ziv_opening_2017,
	title = {Opening the {Black} {Box} of {Deep} {Neural} {Networks} via {Information}},
	url = {http://arxiv.org/abs/1703.00810},
	abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the {\textbackslash}textit\{Information Plane\}; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on \{{\textbackslash}emph compression\} of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.},
	urldate = {2018-04-16},
	journal = {arXiv:1703.00810 [cs]},
	author = {Shwartz-Ziv, Ravid and Tishby, Naftali},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.00810},
	keywords = {Computer Science - Learning, Information bottleneck},
}

@article{kolchinsky_nonlinear_2017,
	title = {Nonlinear {Information} {Bottleneck}},
	url = {http://arxiv.org/abs/1705.02436},
	abstract = {Information bottleneck [IB] is a technique for extracting information in some `input' random variable that is relevant for predicting some different 'output' random variable. IB works by encoding the input in a compressed 'bottleneck variable' from which the output can then be accurately decoded. IB can be difficult to compute in practice, and has been mainly developed for two limited cases: (1) discrete random variables with small state spaces, and (2) continuous random variables that are jointly Gaussian distributed (in which case the encoding and decoding maps are linear). We propose a method to perform IB in more general domains. Our approach can be applied to discrete or continuous inputs and outputs, and allows for nonlinear encoding and decoding maps. The method uses a novel upper bound on the IB objective, derived using a non-parametric estimator of mutual information and a variational approximation. We show how to implement the method using neural networks and gradient-based optimization, and demonstrate its performance on the MNIST dataset.},
	urldate = {2018-04-16},
	journal = {arXiv:1705.02436 [cs, math, stat]},
	author = {Kolchinsky, Artemy and Tracey, Brendan D. and Wolpert, David H.},
	month = may,
	year = {2017},
	note = {arXiv: 1705.02436},
	keywords = {Computer Science - Information Theory, Computer Science - Learning, Information bottleneck, Statistics - Machine Learning},
}

@incollection{dougherty_supervised_1995,
	title = {Supervised and unsupervised discretization of continuous features},
	booktitle = {Machine {Learning} {Proceedings} 1995},
	publisher = {Elsevier},
	author = {Dougherty, James and Kohavi, Ron and Sahami, Mehran},
	year = {1995},
	keywords = {Discretization},
	pages = {194--202},
}

@article{fayyad_multi-interval_1993,
	title = {Multi-interval discretization of continuous-valued attributes for classification learning},
	author = {Fayyad, Usama and Irani, Keki},
	year = {1993},
	keywords = {Discretization},
}

@article{wang_evolutionary_2018,
	title = {Evolutionary {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1803.00657},
	abstract = {Generative adversarial networks (GAN) have been effective for learning generative models for real-world data. However, existing GANs (GAN and its variants) tend to suffer from training problems such as instability and mode collapse. In this paper, we propose a novel GAN framework called evolutionary generative adversarial networks (E-GAN) for stable GAN training and improved generative performance. Unlike existing GANs, which employ a pre-defined adversarial objective function alternately training a generator and a discriminator, we utilize different adversarial training objectives as mutation operations and evolve a population of generators to adapt to the environment (i.e., the discriminator). We also utilize an evaluation mechanism to measure the quality and diversity of generated samples, such that only well-performing generator(s) are preserved and used for further training. In this way, E-GAN overcomes the limitations of an individual adversarial training objective and always preserves the best offspring, contributing to progress in and the success of GANs. Experiments on several datasets demonstrate that E-GAN achieves convincing generative performance and reduces the training problems inherent in existing GANs.},
	urldate = {2018-04-13},
	journal = {arXiv:1803.00657 [cs, stat]},
	author = {Wang, Chaoyue and Xu, Chang and Yao, Xin and Tao, Dacheng},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.00657},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{arora_gans_2017,
	title = {Do {GANs} actually learn the distribution? {An} empirical study},
	shorttitle = {Do {GANs} actually learn the distribution?},
	url = {http://arxiv.org/abs/1706.08224},
	abstract = {Do GANS (Generative Adversarial Nets) actually learn the target distribution? The foundational paper of (Goodfellow et al 2014) suggested they do, if they were given sufficiently large deep nets, sample size, and computation time. A recent theoretical analysis in Arora et al (to appear at ICML 2017) raised doubts whether the same holds when discriminator has finite size. It showed that the training objective can approach its optimum value even if the generated distribution has very low support ---in other words, the training objective is unable to prevent mode collapse. The current note reports experiments suggesting that such problems are not merely theoretical. It presents empirical evidence that well-known GANs approaches do learn distributions of fairly low support, and thus presumably are not learning the target distribution. The main technical contribution is a new proposed test, based upon the famous birthday paradox, for estimating the support size of the generated distribution.},
	urldate = {2018-04-12},
	journal = {arXiv:1706.08224 [cs]},
	author = {Arora, Sanjeev and Zhang, Yi},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.08224},
	keywords = {Computer Science - Learning},
}

@article{butler_integrated_2017,
	title = {Integrated analysis of single cell transcriptomic data across conditions, technologies, and species},
	journal = {bioRxiv},
	author = {Butler, Andrew and Satija, Rahul},
	year = {2017},
	pages = {164889},
}

@article{tirosh_single-cell_2016,
	title = {Single-cell {RNA}-seq supports a developmental hierarchy in human oligodendroglioma., {Single}-cell {RNA}-seq supports a developmental hierarchy in human oligodendroglioma},
	volume = {539, 539},
	issn = {0028-0836},
	url = {http://europepmc.org/abstract/MED/27806376, http://europepmc.org/articles/PMC5465819/?report=abstract},
	doi = {10.1038/nature20123, 10.1038/nature20123},
	abstract = {FULL TEXT Abstract: Although human tumours are shaped by the genetic evolution of cancer cells, evidence also suggests that they display hierarchies related to...},
	language = {eng},
	number = {7628, 7628},
	urldate = {2018-04-05},
	journal = {Nature, Nature},
	author = {Tirosh, I. and Venteicher, A. S. and Hebert, C. and Escalante, L. E. and Patel, A. P. and Yizhak, K. and Fisher, J. M. and Rodman, C. and Mount, C. and Filbin, M. G. and Neftel, C. and Desai, N. and Nyman, J. and Izar, B. and Luo, C. C. and Francis, J. M. and Patel, A. A. and Onozato, M. L. and Riggi, N. and Livak, K. J. and Gennert, D. and Satija, R. and Nahed, B. V. and Curry, W. T. and Martuza, R. L. and Mylvaganam, R. and Iafrate, A. J. and Frosch, M. P. and Golub, T. R. and Rivera, M. N. and Getz, G. and Rozenblatt-Rosen, O. and Cahill, D. P. and Monje, M. and Bernstein, B. E. and Louis, D. N. and Regev, A. and Suvà, M. L.},
	month = nov,
	year = {2016},
	pmid = {27806376},
	pages = {309, 309--313},
}

@article{villani_single-cell_2017,
	title = {Single-cell {RNA}-seq reveals new types of human blood dendritic cells, monocytes and progenitors},
	volume = {356},
	issn = {0036-8075},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5775029/},
	doi = {10.1126/science.aah4573},
	abstract = {Dendritic cells (DCs) and monocytes play a central role in pathogen sensing, phagocytosis and antigen presentation and consist of multiple specialized subtypes. However, their identities and interrelationships are not fully understood. Using unbiased single-cell RNA sequencing (RNA-seq) of {\textasciitilde}2400 cells, we identified six human DCs and four monocyte subtypes in human blood. Our study reveals: a new DC subset that shares properties with plasmacytoid DCs (pDCs) but potently activates T cells, thus redefining pDCs; a new subdivision within the CD1C+ subset of DCs; the relationship between blastic plasmacytoid DC neoplasia cells and healthy DCs; and circulating progenitor of conventional DCs (cDCs). Our revised taxonomy will enable more accurate functional and developmental analyses as well as immune monitoring in health and disease.},
	number = {6335},
	urldate = {2018-04-05},
	journal = {Science (New York, N.Y.)},
	author = {Villani, Alexandra-Chloé and Satija, Rahul and Reynolds, Gary and Sarkizova, Siranush and Shekhar, Karthik and Fletcher, James and Griesbeck, Morgane and Butler, Andrew and Zheng, Shiwei and Lazo, Suzan and Jardine, Laura and Dixon, David and Stephenson, Emily and Nilsson, Emil and Grundberg, Ida and McDonald, David and Filby, Andrew and Li, Weibo and De Jager, Philip L. and Rozenblatt-Rosen, Orit and Lane, Andrew A. and Haniffa, Muzlifah and Regev, Aviv and Hacohen, Nir},
	month = apr,
	year = {2017},
	pmid = {28428369},
	pmcid = {PMC5775029},
}

@article{villani_single-cell_2017-1,
	title = {Single-cell {RNA}-seq reveals new types of human blood dendritic cells, monocytes and progenitors},
	volume = {356},
	issn = {0036-8075},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5775029/},
	doi = {10.1126/science.aah4573},
	abstract = {Dendritic cells (DCs) and monocytes play a central role in pathogen sensing, phagocytosis and antigen presentation and consist of multiple specialized subtypes. However, their identities and interrelationships are not fully understood. Using unbiased single-cell RNA sequencing (RNA-seq) of {\textasciitilde}2400 cells, we identified six human DCs and four monocyte subtypes in human blood. Our study reveals: a new DC subset that shares properties with plasmacytoid DCs (pDCs) but potently activates T cells, thus redefining pDCs; a new subdivision within the CD1C+ subset of DCs; the relationship between blastic plasmacytoid DC neoplasia cells and healthy DCs; and circulating progenitor of conventional DCs (cDCs). Our revised taxonomy will enable more accurate functional and developmental analyses as well as immune monitoring in health and disease.},
	number = {6335},
	urldate = {2018-04-05},
	journal = {Science (New York, N.Y.)},
	author = {Villani, Alexandra-Chloé and Satija, Rahul and Reynolds, Gary and Sarkizova, Siranush and Shekhar, Karthik and Fletcher, James and Griesbeck, Morgane and Butler, Andrew and Zheng, Shiwei and Lazo, Suzan and Jardine, Laura and Dixon, David and Stephenson, Emily and Nilsson, Emil and Grundberg, Ida and McDonald, David and Filby, Andrew and Li, Weibo and De Jager, Philip L. and Rozenblatt-Rosen, Orit and Lane, Andrew A. and Haniffa, Muzlifah and Regev, Aviv and Hacohen, Nir},
	month = apr,
	year = {2017},
	pmid = {28428369},
	pmcid = {PMC5775029},
}

@misc{chintala_ganhacks:_2018,
	title = {ganhacks: starter from "{How} to {Train} a {GAN}?" at {NIPS2016}},
	shorttitle = {ganhacks},
	url = {https://github.com/soumith/ganhacks},
	urldate = {2018-04-05},
	author = {Chintala, Soumith},
	month = apr,
	year = {2018},
	note = {original-date: 2016-12-09T16:09:27Z},
}

@inproceedings{gulrajani_improved_2017,
	title = {Improved training of wasserstein gans},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C.},
	year = {2017},
	pages = {5769--5779},
}

@inproceedings{salimans_improved_2016,
	title = {Improved techniques for training gans},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
	year = {2016},
	pages = {2234--2242},
}

@article{goodfellow_nips_2016,
	title = {{NIPS} 2016 tutorial: {Generative} adversarial networks},
	shorttitle = {{NIPS} 2016 tutorial},
	journal = {arXiv preprint arXiv:1701.00160},
	author = {Goodfellow, Ian},
	year = {2016},
}

@article{garcia_tutorial_2016,
	title = {Tutorial on practical tips of the most influential data preprocessing algorithms in data mining},
	volume = {98},
	issn = {0950-7051},
	url = {http://www.sciencedirect.com/science/article/pii/S0950705115004785},
	doi = {10.1016/j.knosys.2015.12.006},
	abstract = {Data preprocessing is a major and essential stage whose main goal is to obtain final data sets that can be considered correct and useful for further data mining algorithms. This paper summarizes the most influential data preprocessing algorithms according to their usage, popularity and extensions proposed in the specialized literature. For each algorithm, we provide a description, a discussion on its impact, and a review of current and further research on it. These most influential algorithms cover missing values imputation, noise filtering, dimensionality reduction (including feature selection and space transformations), instance reduction (including selection and generation), discretization and treatment of data for imbalanced preprocessing. They constitute all among the most important topics in data preprocessing research and development. This paper emphasizes on the most well-known preprocessing methods and their practical study, selected after a recent, generic book on data preprocessing that does not deepen on them. This manuscript also presents an illustrative study in two sections with different data sets that provide useful tips for the use of preprocessing algorithms. In the first place, we graphically present the effects on two benchmark data sets for the preprocessing methods. The reader may find useful insights on the different characteristics and outcomes generated by them. Secondly, we use a real world problem presented in the ECDBL’2014 Big Data competition to provide a thorough analysis on the application of some preprocessing techniques, their combination and their performance. As a result, five different cases are analyzed, providing tips that may be useful for readers.},
	urldate = {2018-04-03},
	journal = {Knowledge-Based Systems},
	author = {García, Salvador and Luengo, Julián and Herrera, Francisco},
	month = apr,
	year = {2016},
	keywords = {Data mining, Data preprocessing, Data reduction, Dimensionality reduction, Discretization, Instance reduction, Missing values imputation, Noise filtering},
	pages = {1--29},
}

@article{rissanen_modeling_1978,
	title = {Modeling by shortest data description},
	volume = {14},
	number = {5},
	journal = {Automatica},
	author = {Rissanen, Jorma},
	year = {1978},
	pages = {465--471},
}

@article{nestorowa_single-cell_2016,
	title = {A single-cell resolution map of mouse hematopoietic stem and progenitor cell differentiation},
	volume = {128},
	number = {8},
	journal = {Blood},
	author = {Nestorowa, Sonia and Hamey, Fiona K. and Sala, Blanca Pijuan and Diamanti, Evangelia and Shepherd, Mairi and Laurenti, Elisa and Wilson, Nicola K. and Kent, David G. and Göttgens, Berthold},
	year = {2016},
	pages = {e20--e31},
}

@article{brillinger_data_2004,
	title = {Some data analyses using mutual information},
	journal = {Brazilian Journal of Probability and Statistics},
	author = {Brillinger, David R.},
	year = {2004},
	pages = {163--182},
}

@article{brillinger_data_2004-1,
	title = {Some data analyses using mutual information},
	journal = {Brazilian Journal of Probability and Statistics},
	author = {Brillinger, David R.},
	year = {2004},
	pages = {163--182},
}

@article{brillinger_data_2004-2,
	title = {Some data analyses using mutual information},
	journal = {Brazilian Journal of Probability and Statistics},
	author = {Brillinger, David R.},
	year = {2004},
	pages = {163--182},
}

@misc{noauthor_data_nodate,
	title = {Some data analyses using mutu- al information - {Google} {Scholar}},
	url = {https://scholar.google.fr/scholar?hl=en&as_sdt=0%2C5&q=Some+data+analyses+using+mutu-+al+information&btnG=},
	urldate = {2018-03-28},
}

@article{reshef_detecting_2011,
	title = {Detecting {Novel} {Associations} in {Large} {Datasets}},
	volume = {334},
	issn = {0036-8075},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3325791/},
	doi = {10.1126/science.1205438},
	abstract = {Identifying interesting relationships between pairs of variables in large datasets is increasingly important. Here, we present a measure of dependence for two-variable relationships: the maximal information coefficient (MIC). MIC captures a wide range of associations both functional and not, and for functional relationships provides a score that roughly equals the coefficient of determination (R2) of the data relative to the regression function. MIC belongs to a larger class of maximal information-based nonparametric exploration (MINE) statistics for identifying and classifying relationships. We apply MIC and MINE to datasets in global health, gene expression, major-league baseball, and the human gut microbiota, and identify known and novel relationships.},
	number = {6062},
	urldate = {2018-03-06},
	journal = {Science (New York, N.y.)},
	author = {Reshef, David N. and Reshef, Yakir A. and Finucane, Hilary K. and Grossman, Sharon R. and McVean, Gilean and Turnbaugh, Peter J. and Lander, Eric S. and Mitzenmacher, Michael and Sabeti, Pardis C.},
	month = dec,
	year = {2011},
	pmid = {22174245},
	pmcid = {PMC3325791},
	keywords = {BIC, binning},
	pages = {1518--1524},
}

@article{kinney_equitability_2014,
	title = {Equitability, mutual information, and the maximal information coefficient},
	volume = {111},
	copyright = {©  . Freely available online through the PNAS open access option.},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/111/9/3354},
	doi = {10.1073/pnas.1309933111},
	abstract = {How should one quantify the strength of association between two random variables without bias for relationships of a specific form? Despite its conceptual simplicity, this notion of statistical “equitability” has yet to receive a definitive mathematical formalization. Here we argue that equitability is properly formalized by a self-consistency condition closely related to Data Processing Inequality. Mutual information, a fundamental quantity in information theory, is shown to satisfy this equitability criterion. These findings are at odds with the recent work of Reshef et al. [Reshef DN, et al. (2011) Science 334(6062):1518–1524], which proposed an alternative definition of equitability and introduced a new statistic, the “maximal information coefficient” (MIC), said to satisfy equitability in contradistinction to mutual information. These conclusions, however, were supported only with limited simulation evidence, not with mathematical arguments. Upon revisiting these claims, we prove that the mathematical definition of equitability proposed by Reshef et al. cannot be satisfied by any (nontrivial) dependence measure. We also identify artifacts in the reported simulation evidence. When these artifacts are removed, estimates of mutual information are found to be more equitable than estimates of MIC. Mutual information is also observed to have consistently higher statistical power than MIC. We conclude that estimating mutual information provides a natural (and often practical) way to equitably quantify statistical associations in large datasets.},
	language = {en},
	number = {9},
	urldate = {2018-03-05},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kinney, Justin B. and Atwal, Gurinder S.},
	month = mar,
	year = {2014},
	pmid = {24550517},
	keywords = {BIC},
	pages = {3354--3359},
}

@article{reshef_cleaning_2014,
	title = {Cleaning up the record on the maximal information coefficient and equitability},
	volume = {111},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/111/33/E3362},
	doi = {10.1073/pnas.1408920111},
	abstract = {Although we appreciate Kinney and Atwal’s interest in equitability and maximal information coefficient (MIC), we believe they misrepresent our work. We highlight a few of our main objections below.



Fig. 1. 
Equitability of MIC and mutual information under a range of noise models. The equitability of MIC and mutual information across a subset of noise models analyzed in refs. 1 and 4. For each noise model, the relationships tested are as in ref. 4. In each plot in A , each shaded region denotes 90\% probability intervals based on 500 trials of a given relationship at each of 40 noise levels. In the noise models in A , {\textless}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"{\textgreater}{\textless}mml:mrow{\textgreater}{\textless}mml:msub{\textgreater}{\textless}mml:mi{\textgreater}N{\textless}/mml:mi{\textgreater}{\textless}mml:mi{\textgreater}x{\textless}/mml:mi{\textgreater}{\textless}/mml:msub{\textgreater}{\textless}/mml:mrow{\textgreater}{\textless}/mml:math{\textgreater} and {\textless}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"{\textgreater}{\textless}mml:mrow{\textgreater}{\textless}mml:msub{\textgreater}{\textless}mml:mi{\textgreater}N{\textless}/mml:mi{\textgreater}{\textless}mml:mi{\textgreater}y{\textless}/mml:mi{\textgreater}{\textless}/mml:msub{\textgreater}{\textless}/mml:mrow{\textgreater}{\textless}/mml:math{\textgreater} represent Gaussians, and {\textless}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"{\textgreater}{\textless}mml:mi{\textgreater}X{\textless}/mml:mi{\textgreater}{\textless}/mml:math{\textgreater}-values are chosen so that the noiseless data points are spaced uniformly along the graph of {\textless}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"{\textgreater}{\textless}mml:mrow{\textgreater}{\textless}mml:mi{\textgreater}f{\textless}/mml:mi{\textgreater}{\textless}mml:mrow{\textgreater}{\textless}mml:mrow{\textgreater}{\textless}mml:mo{\textgreater}({\textless}/mml:mo{\textgreater}{\textless}mml:mi{\textgreater}X{\textless}/mml:mi{\textgreater}{\textless}mml:mo{\textgreater}){\textless}/mml:mo{\textgreater}{\textless}/mml:mrow{\textgreater}{\textless}/mml:mrow{\textgreater}{\textless}/mml:mrow{\textgreater}{\textless}/mml:math{\textgreater}. The intervals plotted in red for each noise model in A represent the largest range of R 2 values that correspond to a single value of the statistic in question. This provides a quantitative measure of the equitability of each statistic (the … 



[↵][1]2To whom correspondence may be addressed. Email: dnreshef\{at\}mit.edu or yakirr\{at\}mit.edu.

 [1]: \#xref-corresp-1-1},
	language = {en},
	number = {33},
	urldate = {2018-03-06},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Reshef, David N. and Reshef, Yakir A. and Mitzenmacher, Michael and Sabeti, Pardis C.},
	month = aug,
	year = {2014},
	pmid = {25139972},
	keywords = {BIC},
	pages = {E3362--E3363},
}

@article{reshef_measuring_2016,
	title = {Measuring dependence powerfully and equitably},
	volume = {17},
	number = {212},
	journal = {Journal of Machine Learning Research},
	author = {Reshef, Yakir A. and Reshef, David N. and Finucane, Hilary K. and Sabeti, Pardis C. and Mitzenmacher, Michael},
	year = {2016},
	keywords = {BIC, binning},
	pages = {1--63},
}

@inproceedings{kontkanen_mdl_2007-2,
	title = {{MDL} histogram density estimation},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	author = {Kontkanen, Petri and MyllymÃ, Petri},
	year = {2007},
	keywords = {binning},
	pages = {219--226},
}

@article{zografos_expressions_2005,
	title = {Expressions for {Rényi} and {Shannon} entropies for multivariate distributions},
	volume = {71},
	issn = {0167-7152},
	url = {http://www.sciencedirect.com/science/article/pii/S016771520400286X},
	doi = {10.1016/j.spl.2004.10.023},
	abstract = {Exact forms of Rényi and Shannon entropies are determined for several multivariate distributions, including multivariate t, multivariate Cauchy, multivariate Pearson type VII, multivariate Pearson type II, multivariate symmetric Kotz type, multivariate logistic, multivariate Burr, multivariate Pareto type I, multivariate Pareto type II, multivariate Pareto type III, multivariate Pareto type IV, Dirichlet, inverted Dirichlet, multivariate Liouville, multivariate exponential, multivariate Weinman exponential, multivariate ordered Weinman exponential, bivariate gamma exponential, bivariate conditionally specified exponential, multivariate Weibull and multivariate log-normal. Monotonicity properties of Rényi and Shannon entropies for these distributions are also studied. We believe that the results presented here will serve as an important reference for scientists and engineers in many areas.},
	number = {1},
	urldate = {2018-03-15},
	journal = {Statistics \& Probability Letters},
	author = {Zografos, K. and Nadarajah, S.},
	month = jan,
	year = {2005},
	keywords = {Elliptically contoured distributions, Rényi entropy, Shannon entropy, expressions},
	pages = {71--84},
}

@article{darbellay_entropy_2000,
	title = {Entropy expressions for multivariate continuous distributions},
	volume = {46},
	issn = {0018-9448},
	doi = {10.1109/18.825848},
	abstract = {Analytical formulas for the entropy and the mutual information of multivariate continuous probability distributions are presented},
	number = {2},
	journal = {IEEE Transactions on Information Theory},
	author = {Darbellay, G. A. and Vajda, I.},
	month = mar,
	year = {2000},
	keywords = {Closed-form solution, Computational complexity, Convolutional codes, Decoding, Entropy, Network address translation, Probability distribution, Tail, Throughput, Timing, analytical formulas, continuous probability distributions, entropy, entropy expressions, expressions, information theory, multivariate continuous distributions, mutual information, probability},
	pages = {709--712},
}

@article{ahmed_entropy_1989,
	title = {Entropy expressions and their estimators for multivariate distributions},
	volume = {35},
	issn = {0018-9448},
	doi = {10.1109/18.30996},
	abstract = {Entropy expressions for several continuous multivariate distributions are derived. Point estimation of entropy for the multinormal distribution and for the distribution of order statistics from D.G. Weinman's (Ph.D dissertation, Ariz. State Univ., Tempe, AZ, 1966) exponential distribution is considered. The asymptotic distribution of the uniformly minimum variance unbiased estimator for multinormal entropy is obtained. Simulation results on convergence of the means and variances of these estimators are provided},
	number = {3},
	journal = {IEEE Transactions on Information Theory},
	author = {Ahmed, N. A. and Gokhale, D. V.},
	month = may,
	year = {1989},
	keywords = {Convergence, Density functional theory, Density measurement, Entropy, Exponential distribution, Gaussian distribution, Random variables, Statistical analysis, Statistical distributions, Testing, asymptotic distribution, continuous multivariate distributions, convergence, entropy, exponential distribution, expressions, information theory, multinormal distribution, multinormal entropy, order statistics distribution, parameter estimation, point estimation, statistical analysis, statistical inference, uniformly minimum variance unbiased estimator},
	pages = {688--692},
}

@article{knuth_optimal_2006,
	title = {Optimal data-based binning for histograms},
	journal = {arXiv preprint physics/0605197},
	author = {Knuth, Kevin H.},
	year = {2006},
	keywords = {binning},
}

@article{hudson_signal_2006,
	title = {Signal processing using mutual information},
	volume = {23},
	number = {6},
	journal = {IEEE Signal Processing Magazine},
	author = {Hudson, John E.},
	year = {2006},
	keywords = {binning},
	pages = {50--54},
}

@article{bojanowski_optimizing_2017,
	title = {Optimizing the latent space of generative networks},
	journal = {arXiv preprint arXiv:1707.05776},
	author = {Bojanowski, Piotr and Joulin, Armand and Lopez-Paz, David and Szlam, Arthur},
	year = {2017},
}

@article{beirlant_nonparametric_1997,
	title = {Nonparametric entropy estimation: {An} overview},
	volume = {6},
	shorttitle = {Nonparametric entropy estimation},
	number = {1},
	journal = {International Journal of Mathematical and Statistical Sciences},
	author = {Beirlant, Jan and Dudewicz, Edward J. and Györfi, László and Van der Meulen, Edward C.},
	year = {1997},
	pages = {17--39},
}

@article{zografos_expressions_2005,
	title = {Expressions for {Rényi} and {Shannon} entropies for multivariate distributions},
	volume = {71},
	issn = {0167-7152},
	url = {http://www.sciencedirect.com/science/article/pii/S016771520400286X},
	doi = {10.1016/j.spl.2004.10.023},
	abstract = {Exact forms of Rényi and Shannon entropies are determined for several multivariate distributions, including multivariate t, multivariate Cauchy, multivariate Pearson type VII, multivariate Pearson type II, multivariate symmetric Kotz type, multivariate logistic, multivariate Burr, multivariate Pareto type I, multivariate Pareto type II, multivariate Pareto type III, multivariate Pareto type IV, Dirichlet, inverted Dirichlet, multivariate Liouville, multivariate exponential, multivariate Weinman exponential, multivariate ordered Weinman exponential, bivariate gamma exponential, bivariate conditionally specified exponential, multivariate Weibull and multivariate log-normal. Monotonicity properties of Rényi and Shannon entropies for these distributions are also studied. We believe that the results presented here will serve as an important reference for scientists and engineers in many areas.},
	number = {1},
	urldate = {2018-03-28},
	journal = {Statistics \& Probability Letters},
	author = {Zografos, K. and Nadarajah, S.},
	month = jan,
	year = {2005},
	keywords = {Elliptically contoured distributions, Rényi entropy, Shannon entropy},
	pages = {71--84},
}

@article{cho_neural_2018,
	title = {Neural {Data} {Visualization} for {Scalable} and {Generalizable} {Single} {Cell} {Analysis}},
	copyright = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/early/2018/03/27/289223},
	doi = {10.1101/289223},
	abstract = {Single-cell RNA sequencing is becoming effective and accessible as emerging technologies push its scale to millions of cells and beyond. Visualizing the landscape of single cell expression has been a fundamental tool in single cell analysis. However, standard methods for visualization, such as t-stochastic neighbor embedding (t-SNE), not only lack scalability to data sets with millions of cells, but also are unable to generalize to new cells, an important ability for transferring knowledge across fast-accumulating data sets. We introduce net-SNE, which trains a neural network to learn a high quality visualization of single cells that newly generalizes to unseen data. While matching the visualization quality of t-SNE on 14 benchmark data sets of varying sizes, from hundreds to 1.3 million cells, net-SNE also effectively positions previously unseen cells, even when an entire subtype is missing from the initial data set or when the new cells are from a different sequencing experiment. Furthermore, given a "reference" visualization, net-SNE can vastly reduce the computational burden of visualizing millions of single cells from multiple days to just a few minutes of runtime. Our work provides a general framework for newly bootstrapping single cell analysis from existing data sets.},
	language = {en},
	urldate = {2018-03-28},
	journal = {bioRxiv},
	author = {Cho, Hyunghoon and Berger, Bonnie and Peng, Jian},
	month = mar,
	year = {2018},
	pages = {289223},
}

@article{huang_brie:_2017,
	title = {{BRIE}: transcriptome-wide splicing quantification in single cells},
	volume = {18},
	issn = {1474-760X},
	shorttitle = {{BRIE}},
	url = {https://doi.org/10.1186/s13059-017-1248-5},
	doi = {10.1186/s13059-017-1248-5},
	abstract = {Single-cell RNA-seq (scRNA-seq) provides a comprehensive measurement of stochasticity in transcription, but the limitations of the technology have prevented its application to dissect variability in RNA processing events such as splicing. Here, we present BRIE (Bayesian regression for isoform estimation), a Bayesian hierarchical model that resolves these problems by learning an informative prior distribution from sequence features. We show that BRIE yields reproducible estimates of exon inclusion ratios in single cells and provides an effective tool for differential isoform quantification between scRNA-seq data sets. BRIE, therefore, expands the scope of scRNA-seq experiments to probe the stochasticity of RNA processing.},
	urldate = {2018-03-28},
	journal = {Genome Biology},
	author = {Huang, Yuanhua and Sanguinetti, Guido},
	month = jun,
	year = {2017},
	keywords = {Differential splicing, Isoform estimate, Single-cell RNA-seq},
	pages = {123},
}

@article{knuth_optimal_2006,
	title = {Optimal data-based binning for histograms},
	journal = {arXiv preprint physics/0605197},
	author = {Knuth, Kevin H.},
	year = {2006},
}

@article{knuth_optimal_2006-1,
	title = {Optimal data-based binning for histograms},
	journal = {arXiv preprint physics/0605197},
	author = {Knuth, Kevin H.},
	year = {2006},
}

@article{wang_review_2014,
	series = {Network-based biomarkers for complex diseases},
	title = {Review on statistical methods for gene network reconstruction using expression data},
	volume = {362},
	issn = {0022-5193},
	url = {http://www.sciencedirect.com/science/article/pii/S0022519314001969},
	doi = {10.1016/j.jtbi.2014.03.040},
	abstract = {Network modeling has proven to be a fundamental tool in analyzing the inner workings of a cell. It has revolutionized our understanding of biological processes and made significant contributions to the discovery of disease biomarkers. Much effort has been devoted to reconstruct various types of biochemical networks using functional genomic datasets generated by high-throughput technologies. This paper discusses statistical methods used to reconstruct gene regulatory networks using gene expression data. In particular, we highlight progress made and challenges yet to be met in the problems involved in estimating gene interactions, inferring causality and modeling temporal changes of regulation behaviors. As rapid advances in technologies have made available diverse, large-scale genomic data, we also survey methods of incorporating all these additional data to achieve better, more accurate inference of gene networks.},
	urldate = {2018-03-28},
	journal = {Journal of Theoretical Biology},
	author = {Wang, Y. X. Rachel and Huang, Haiyan},
	month = dec,
	year = {2014},
	keywords = {Bayesian networks, Coexpression networks, Community detection, Dynamic networks, Genomic data integration},
	pages = {53--61},
}

@article{wang_review_2014-1,
	series = {Network-based biomarkers for complex diseases},
	title = {Review on statistical methods for gene network reconstruction using expression data},
	volume = {362},
	issn = {0022-5193},
	url = {http://www.sciencedirect.com/science/article/pii/S0022519314001969},
	doi = {10.1016/j.jtbi.2014.03.040},
	abstract = {Network modeling has proven to be a fundamental tool in analyzing the inner workings of a cell. It has revolutionized our understanding of biological processes and made significant contributions to the discovery of disease biomarkers. Much effort has been devoted to reconstruct various types of biochemical networks using functional genomic datasets generated by high-throughput technologies. This paper discusses statistical methods used to reconstruct gene regulatory networks using gene expression data. In particular, we highlight progress made and challenges yet to be met in the problems involved in estimating gene interactions, inferring causality and modeling temporal changes of regulation behaviors. As rapid advances in technologies have made available diverse, large-scale genomic data, we also survey methods of incorporating all these additional data to achieve better, more accurate inference of gene networks.},
	urldate = {2018-03-28},
	journal = {Journal of Theoretical Biology},
	author = {Wang, Y. X. Rachel and Huang, Haiyan},
	month = dec,
	year = {2014},
	keywords = {Bayesian networks, Coexpression networks, Community detection, Dynamic networks, Genomic data integration},
	pages = {53--61},
}

@article{gao_latent_2013,
	title = {A latent factor model with a mixture of sparse and dense factors to model gene expression data with confounding effects},
	url = {http://arxiv.org/abs/1310.4792},
	abstract = {One important problem in genome science is to determine sets of co-regulated genes based on measurements of gene expression levels across samples, where the quantification of expression levels includes substantial technical and biological noise. To address this problem, we developed a Bayesian sparse latent factor model that uses a three parameter beta prior to flexibly model shrinkage in the loading matrix. By applying three layers of shrinkage to the loading matrix (global, factor-specific, and element-wise), this model has non-parametric properties in that it estimates the appropriate number of factors from the data. We added a two-component mixture to model each factor loading as being generated from either a sparse or a dense mixture component; this allows dense factors that capture confounding noise, and sparse factors that capture local gene interactions. We developed two statistics to quantify the stability of the recovered matrices for both sparse and dense matrices. We tested our model on simulated data and found that we successfully recovered the true latent structure as compared to related models. We applied our model to a large gene expression study and found that we recovered known covariates and small groups of co-regulated genes. We validated these gene subsets by testing for associations between genotype data and these latent factors, and we found a substantial number of biologically important genetic regulators for the recovered gene subsets.},
	urldate = {2018-03-28},
	journal = {arXiv:1310.4792 [q-bio, stat]},
	author = {Gao, Chuan and Brown, Christopher D. and Engelhardt, Barbara E.},
	month = oct,
	year = {2013},
	note = {arXiv: 1310.4792},
	keywords = {Quantitative Biology - Genomics, Statistics - Applications},
}

@article{szabo_information_2014,
	title = {Information theoretical estimators toolbox},
	volume = {15},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Szabó, Zoltán},
	year = {2014},
	pages = {283--287},
}

@inproceedings{huber_entropy_2008,
	title = {On entropy approximation for {Gaussian} mixture random vectors},
	booktitle = {Multisensor {Fusion} and {Integration} for {Intelligent} {Systems}, 2008. {MFI} 2008. {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Huber, Marco F. and Bailey, Tim and Durrant-Whyte, Hugh and Hanebeck, Uwe D.},
	year = {2008},
	pages = {181--188},
}

@article{torkkola_feature_2003,
	title = {Feature extraction by non-parametric mutual information maximization},
	volume = {3},
	number = {Mar},
	journal = {Journal of machine learning research},
	author = {Torkkola, Kari},
	year = {2003},
	pages = {1415--1438},
}

@misc{noauthor_eature_nodate,
	title = {eature {Extraction} by {Non}-{Parametric} {Mutual} {Information}... - {Google} {Scholar}},
	url = {https://scholar.google.fr/scholar?hl=en&as_sdt=0%2C5&q=eature+Extraction+by+Non-Parametric+Mutual+Information+Maximization&btnG=},
	urldate = {2018-03-16},
}

@article{seok_mutual_2015,
	title = {Mutual information between discrete variables with many categories using recursive adaptive partitioning},
	volume = {5},
	journal = {Scientific Reports},
	author = {Seok, Junhee and Kang, Yeong Seon},
	year = {2015},
	pages = {10981},
}

@article{ross_mutual_2014,
	title = {Mutual {Information} between {Discrete} and {Continuous} {Data} {Sets}},
	volume = {9},
	issn = {1932-6203},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0087357},
	doi = {10.1371/journal.pone.0087357},
	abstract = {Mutual information (MI) is a powerful method for detecting relationships between data sets. There are accurate methods for estimating MI that avoid problems with “binning” when both data sets are discrete or when both data sets are continuous. We present an accurate, non-binning MI estimator for the case of one discrete data set and one continuous data set. This case applies when measuring, for example, the relationship between base sequence and gene expression level, or the effect of a cancer drug on patient survival time. We also show how our method can be adapted to calculate the Jensen–Shannon divergence of two or more data sets.},
	language = {en},
	number = {2},
	urldate = {2018-03-15},
	journal = {PLOS ONE},
	author = {Ross, Brian C.},
	month = feb,
	year = {2014},
	keywords = {Entropy, Gene expression, Information theory, Nucleobases, Probability density, Probability distribution, Square waves, Statistical distributions},
	pages = {e87357},
}

@misc{noauthor_mutual_nodate,
	title = {Mutual {Information} between {Discrete} and {Continuous} {Data} {Sets}},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0087357},
	urldate = {2018-03-15},
}

@article{darbellay_estimation_1999,
	title = {Estimation of the information by an adaptive partitioning of the observation space},
	volume = {45},
	number = {4},
	journal = {IEEE Transactions on Information Theory},
	author = {Darbellay, Georges A. and Vajda, Igor},
	year = {1999},
	pages = {1315--1321},
}

@article{bojanowski_optimizing_2017,
	title = {Optimizing the {Latent} {Space} of {Generative} {Networks}},
	url = {http://arxiv.org/abs/1707.05776},
	abstract = {Generative Adversarial Networks (GANs) have been shown to be able to sample impressively realistic images. GAN training consists of a saddle point optimization problem that can be thought of as an adversarial game between a generator which produces the images, and a discriminator, which judges if the images are real. Both the generator and the discriminator are commonly parametrized as deep convolutional neural networks. The goal of this paper is to disentangle the contribution of the optimization procedure and the network parametrization to the success of GANs. To this end we introduce and study Generative Latent Optimization (GLO), a framework to train a generator without the need to learn a discriminator, thus avoiding challenging adversarial optimization problems. We show experimentally that GLO enjoys many of the desirable properties of GANs: learning from large data, synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors.},
	urldate = {2018-03-15},
	journal = {arXiv:1707.05776 [cs, stat]},
	author = {Bojanowski, Piotr and Joulin, Armand and Lopez-Paz, David and Szlam, Arthur},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.05776},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Statistics - Machine Learning},
}

@article{gao_estimating_2017,
	title = {Estimating {Mutual} {Information} for {Discrete}-{Continuous} {Mixtures}},
	url = {http://arxiv.org/abs/1709.06212},
	abstract = {Estimating mutual information from observed samples is a basic primitive, useful in several machine learning tasks including correlation mining, information bottleneck clustering, learning a Chow-Liu tree, and conditional independence testing in (causal) graphical models. While mutual information is a well-defined quantity in general probability spaces, existing estimators can only handle two special cases of purely discrete or purely continuous pairs of random variables. The main challenge is that these methods first estimate the (differential) entropies of X, Y and the pair (X;Y) and add them up with appropriate signs to get an estimate of the mutual information. These 3H-estimators cannot be applied in general mixture spaces, where entropy is not well-defined. In this paper, we design a novel estimator for mutual information of discrete-continuous mixtures. We prove that the proposed estimator is consistent. We provide numerical experiments suggesting superiority of the proposed estimator compared to other heuristics of adding small continuous noise to all the samples and applying standard estimators tailored for purely continuous variables, and quantizing the samples and applying standard estimators tailored for purely discrete variables. This significantly widens the applicability of mutual information estimation in real-world applications, where some variables are discrete, some continuous, and others are a mixture between continuous and discrete components.},
	urldate = {2018-03-15},
	journal = {arXiv:1709.06212 [cs, math]},
	author = {Gao, Weihao and Kannan, Sreeram and Oh, Sewoong and Viswanath, Pramod},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.06212},
	keywords = {Computer Science - Information Theory, Computer Science - Learning},
}

@article{roulston_estimating_1999,
	title = {Estimating the errors on measured entropy and mutual information},
	volume = {125},
	number = {3-4},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Roulston, Mark S.},
	year = {1999},
	pages = {285--294},
}

@article{radford_unsupervised_2015,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	urldate = {2018-03-08},
	journal = {arXiv:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06434},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
}

@article{ghahramani_generative_2018,
	title = {Generative adversarial networks uncover epidermal regulators and predict single cell perturbations},
	copyright = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/early/2018/02/08/262501},
	doi = {10.1101/262501},
	abstract = {Recent advances have enabled gene expression profiling of single cells at lower cost. As more data is produced there is an increasing need to integrate diverse datasets and better analyse underutilised data to gain biological insights. However, analysis of single cell RNA-seq data is challenging due to biological and technical noise which not only varies between laboratories but also between batches. Here for the first time, we apply a new generative deep learning approach called Generative Adversarial Networks (GAN) to biological data. We show that it is possible to integrate diverse skin (epidermal) datasets and in doing so, our generative model is able to simulate realistic scRNA-seq data that covers the full diversity of cell types. In contrast to many machine-learning approaches, we are able to interpret internal parameters in a biologically meaningful manner. These include using internal GAN learned features for improved dimensionality reduction. Using our generative model we are able to obtain a universal representation of epidermal differentiation and use this to predict the effect of cell state perturbations on gene expression at high time-resolution. We show that our trained neural networks identify biological state-determining genes and through analysis of these networks we can obtain inferred gene regulatory relationships. In combination these attributes provide a powerful framework to progress the analysis of scRNA-seq data beyond exploratory analysis of cell clusters and towards integration of multiple datasets regardless of origin.},
	language = {en},
	urldate = {2018-03-08},
	journal = {bioRxiv},
	author = {Ghahramani, Arsham and Watt, Fiona M. and Luscombe, Nicholas M.},
	month = feb,
	year = {2018},
	pages = {262501},
}

@article{kraskov_estimating_2004,
	title = {Estimating mutual information},
	volume = {69},
	number = {6},
	journal = {Physical review E},
	author = {Kraskov, Alexander and Stögbauer, Harald and Grassberger, Peter},
	year = {2004},
	pages = {066138},
}

@article{paninski_estimation_2003,
	title = {Estimation of entropy and mutual information},
	volume = {15},
	number = {6},
	journal = {Neural computation},
	author = {Paninski, Liam},
	year = {2003},
	pages = {1191--1253},
}

@article{kraskov_estimating_2004-1,
	title = {Estimating mutual information},
	volume = {69},
	number = {6},
	journal = {Physical review E},
	author = {Kraskov, Alexander and Stögbauer, Harald and Grassberger, Peter},
	year = {2004},
	pages = {066138},
}

@inproceedings{gao_efficient_2015,
	title = {Efficient estimation of mutual information for strongly dependent variables},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	author = {Gao, Shuyang and Ver Steeg, Greg and Galstyan, Aram},
	year = {2015},
	pages = {277--286},
}

@article{gao_estimating_2015,
	title = {Estimating {Mutual} {Information} by {Local} {Gaussian} {Approximation}},
	url = {http://arxiv.org/abs/1508.00536},
	abstract = {Estimating mutual information (MI) from samples is a fundamental problem in statistics, machine learning, and data analysis. Recently it was shown that a popular class of non-parametric MI estimators perform very poorly for strongly dependent variables and have sample complexity that scales exponentially with the true MI. This undesired behavior was attributed to the reliance of those estimators on local uniformity of the underlying (and unknown) probability density function. Here we present a novel semi-parametric estimator of mutual information, where at each sample point, densities are \{{\textbackslash}em locally\} approximated by a Gaussians distribution. We demonstrate that the estimator is asymptotically unbiased. We also show that the proposed estimator has a superior performance compared to several baselines, and is able to accurately measure relationship strengths over many orders of magnitude.},
	urldate = {2018-03-06},
	journal = {arXiv:1508.00536 [physics]},
	author = {Gao, Shuyang and Steeg, Greg Ver and Galstyan, Aram},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.00536},
	keywords = {Computer Science - Information Theory, Physics - Data Analysis, Statistics and Probability},
}

@article{reshef_measuring_2016,
	title = {Measuring dependence powerfully and equitably},
	volume = {17},
	number = {212},
	journal = {Journal of Machine Learning Research},
	author = {Reshef, Yakir A. and Reshef, David N. and Finucane, Hilary K. and Sabeti, Pardis C. and Mitzenmacher, Michael},
	year = {2016},
	pages = {1--63},
}

@article{reshef_equitability_2015,
	title = {Equitability, interval estimation, and statistical power},
	url = {http://arxiv.org/abs/1505.02212},
	abstract = {For analysis of a high-dimensional dataset, a common approach is to test a null hypothesis of statistical independence on all variable pairs using a non-parametric measure of dependence. However, because this approach attempts to identify any non-trivial relationship no matter how weak, it often identifies too many relationships to be useful. What is needed is a way of identifying a smaller set of relationships that merit detailed further analysis. Here we formally present and characterize equitability, a property of measures of dependence that aims to overcome this challenge. Notionally, an equitable statistic is a statistic that, given some measure of noise, assigns similar scores to equally noisy relationships of different types [Reshef et al. 2011]. We begin by formalizing this idea via a new object called the interpretable interval, which functions as an interval estimate of the amount of noise in a relationship of unknown type. We define an equitable statistic as one with small interpretable intervals. We then draw on the equivalence of interval estimation and hypothesis testing to show that under moderate assumptions an equitable statistic is one that yields well powered tests for distinguishing not only between trivial and non-trivial relationships of all kinds but also between non-trivial relationships of different strengths. This means that equitability allows us to specify a threshold relationship strength \$x\_0\$ and to search for relationships of all kinds with strength greater than \$x\_0\$. Thus, equitability can be thought of as a strengthening of power against independence that enables fruitful analysis of data sets with a small number of strong, interesting relationships and a large number of weaker ones. We conclude with a demonstration of how our two equivalent characterizations of equitability can be used to evaluate the equitability of a statistic in practice.},
	urldate = {2018-03-06},
	journal = {arXiv:1505.02212 [cs, math, q-bio, stat]},
	author = {Reshef, Yakir A. and Reshef, David N. and Sabeti, Pardis C. and Mitzenmacher, Michael M.},
	month = may,
	year = {2015},
	note = {arXiv: 1505.02212},
	keywords = {Computer Science - Learning, Mathematics - Statistics Theory, Quantitative Biology - Quantitative Methods, Statistics - Machine Learning, Statistics - Methodology},
}

@article{zhao_part_2016,
	title = {Part mutual information for quantifying direct associations in networks},
	volume = {113},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/113/18/5130},
	doi = {10.1073/pnas.1522586113},
	abstract = {Quantitatively identifying direct dependencies between variables is an important task in data analysis, in particular for reconstructing various types of networks and causal relations in science and engineering. One of the most widely used criteria is partial correlation, but it can only measure linearly direct association and miss nonlinear associations. However, based on conditional independence, conditional mutual information (CMI) is able to quantify nonlinearly direct relationships among variables from the observed data, superior to linear measures, but suffers from a serious problem of underestimation, in particular for those variables with tight associations in a network, which severely limits its applications. In this work, we propose a new concept, “partial independence,” with a new measure, “part mutual information” (PMI), which not only can overcome the problem of CMI but also retains the quantification properties of both mutual information (MI) and CMI. Specifically, we first defined PMI to measure nonlinearly direct dependencies between variables and then derived its relations with MI and CMI. Finally, we used a number of simulated data as benchmark examples to numerically demonstrate PMI features and further real gene expression data from Escherichia coli and yeast to reconstruct gene regulatory networks, which all validated the advantages of PMI for accurately quantifying nonlinearly direct associations in networks.},
	language = {en},
	number = {18},
	urldate = {2018-03-06},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Zhao, Juan and Zhou, Yiwei and Zhang, Xiujun and Chen, Luonan},
	month = may,
	year = {2016},
	pmid = {27092000},
	keywords = {conditional independence, conditional mutual information, network inference, systems biology},
	pages = {5130--5135},
}

@article{moddemeijer_estimation_1989,
	title = {On estimation of entropy and mutual information of continuous distributions},
	volume = {16},
	number = {3},
	journal = {Signal processing},
	author = {Moddemeijer, Rudy},
	year = {1989},
	pages = {233--248},
}

@article{garcia-albeniz_value_2017,
	title = {The value of explicitly emulating a target trial when using real world evidence: an application to colorectal cancer screening},
	volume = {32},
	issn = {0393-2990, 1573-7284},
	shorttitle = {The value of explicitly emulating a target trial when using real world evidence},
	url = {https://link.springer.com/article/10.1007/s10654-017-0287-2},
	doi = {10.1007/s10654-017-0287-2},
	abstract = {Observational analyses for causal inference often rely on real world data collected for purposes other than research. A frequent goal of these observational analyses is to use the data to emulate a hypothetical randomized experiment, i.e., the target trial, that mimics the design features of a true experiment, including a clear definition of time zero with synchronization of treatment assignment and determination of eligibility. We review a recent observational analysis that explicitly emulated a target trial of screening colonoscopy using insurance claims from U.S. Medicare. We then compare this explicit emulation with alternative, simpler observational analyses that do not synchronize treatment assignment and eligibility determination at time zero and/or do not allow for repeated eligibility. This empirical comparison suggests that lack of an explicit emulation of the target trial leads to biased estimates, and shows that allowing for repeated eligibility increases the statistical efficiency of the estimates.},
	language = {en},
	number = {6},
	urldate = {2018-03-05},
	journal = {European Journal of Epidemiology},
	author = {García-Albéniz, Xabier and Hsu, John and Hernán, Miguel A.},
	month = jun,
	year = {2017},
	pages = {495--500},
}

@inproceedings{kpotufe_consistency_2014,
	title = {Consistency of causal inference under the additive noise model},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Kpotufe, Samory and Sgouritsa, Eleni and Janzing, Dominik and Schölkopf, Bernhard},
	year = {2014},
	pages = {478--486},
}

@article{buhlmann_cam:_2014,
	title = {{CAM}: {Causal} additive models, high-dimensional order search and penalized regression},
	volume = {42},
	issn = {0090-5364},
	shorttitle = {{CAM}},
	url = {http://arxiv.org/abs/1310.1533},
	doi = {10.1214/14-AOS1260},
	abstract = {We develop estimation for potentially high-dimensional additive structural equation models. A key component of our approach is to decouple order search among the variables from feature or edge selection in a directed acyclic graph encoding the causal structure. We show that the former can be done with nonregularized (restricted) maximum likelihood estimation while the latter can be efficiently addressed using sparse regression techniques. Thus, we substantially simplify the problem of structure search and estimation for an important class of causal models. We establish consistency of the (restricted) maximum likelihood estimator for low- and high-dimensional scenarios, and we also allow for misspecification of the error distribution. Furthermore, we develop an efficient computational algorithm which can deal with many variables, and the new method's accuracy and performance is illustrated on simulated and real data.},
	number = {6},
	urldate = {2018-02-06},
	journal = {The Annals of Statistics},
	author = {Bühlmann, Peter and Peters, Jonas and Ernest, Jan},
	month = dec,
	year = {2014},
	note = {arXiv: 1310.1533},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Statistics - Methodology},
	pages = {2526--2556},
}

@article{tikka_identifying_2017,
	title = {Identifying causal effects with the {R} package causaleffect},
	volume = {76},
	journal = {Journal of Statistical Software},
	author = {Tikka, Santtu and Karvanen, Juha},
	year = {2017},
}

@article{peters_causal_2016,
	title = {Causal inference by using invariant prediction: identification and confidence intervals},
	volume = {78},
	issn = {1467-9868},
	shorttitle = {Causal inference by using invariant prediction},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/rssb.12167/abstract},
	doi = {10.1111/rssb.12167},
	abstract = {What is the difference between a prediction that is made with a causal model and that with a non-causal model? Suppose that we intervene on the predictor variables or change the whole environment. The predictions from a causal model will in general work as well under interventions as for observational data. In contrast, predictions from a non-causal model can potentially be very wrong if we actively intervene on variables. Here, we propose to exploit this invariance of a prediction under a causal model for causal inference: given different experimental settings (e.g. various interventions) we collect all models that do show invariance in their predictive accuracy across settings and interventions. The causal model will be a member of this set of models with high probability. This approach yields valid confidence intervals for the causal relationships in quite general scenarios. We examine the example of structural equation models in more detail and provide sufficient assumptions under which the set of causal predictors becomes identifiable. We further investigate robustness properties of our approach under model misspecification and discuss possible extensions. The empirical properties are studied for various data sets, including large-scale gene perturbation experiments.},
	language = {en},
	number = {5},
	urldate = {2018-02-01},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Peters, Jonas and Bühlmann, Peter and Meinshausen, Nicolai},
	month = nov,
	year = {2016},
	keywords = {Causal discovery, Causal inference, Confidence intervals, Invariant prediction},
	pages = {947--1012},
}

@article{rocke_model_2001,
	title = {A {Model} for {Measurement} {Error} for {Gene} {Expression} {Arrays}},
	volume = {8},
	number = {6},
	journal = {JOURNAL OF COMPUTATIONAL BIOLOGY},
	author = {ROCKE, DAVID M. and DURBIN, BLYTHE},
	year = {2001},
}

@article{van_den_bulcke_syntren:_2006,
	title = {{SynTReN}: a generator of synthetic gene expression data for design and analysis of structure learning algorithms},
	volume = {7},
	issn = {1471-2105},
	shorttitle = {{SynTReN}},
	url = {https://doi.org/10.1186/1471-2105-7-43},
	doi = {10.1186/1471-2105-7-43},
	abstract = {The development of algorithms to infer the structure of gene regulatory networks based on expression data is an important subject in bioinformatics research. Validation of these algorithms requires benchmark data sets for which the underlying network is known. Since experimental data sets of the appropriate size and design are usually not available, there is a clear need to generate well-characterized synthetic data sets that allow thorough testing of learning algorithms in a fast and reproducible manner.},
	urldate = {2018-02-01},
	journal = {BMC Bioinformatics},
	author = {Van den Bulcke, Tim and Van Leemput, Koenraad and Naudts, Bart and van Remortel, Piet and Ma, Hongwu and Verschoren, Alain and De Moor, Bart and Marchal, Kathleen},
	month = jan,
	year = {2006},
	pages = {43},
}

@article{mendes_artificial_2003,
	title = {Artificial gene networks for objective comparison of analysis algorithms},
	volume = {19},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/19/suppl_2/ii122/180406},
	doi = {10.1093/bioinformatics/btg1069},
	abstract = {Motivation: Large-scale gene expression profiling generates data
  sets that are rich in observed features but poor in numbers of
  observations. The analysis of such data sets is a challenge that has
  been object of vigorous research. The algorithms in use for this
  purpose have been poorly documented and rarely compared objectively,
  posing a problem of uncertainty about the outcomes of the analyses.
  One way to objectively test such analysis algorithms is to apply
  them on computational gene network models for which the mechanisms
  are completely know.Results: We present a system that generates random artificial
  gene networks according to well-defined topological and kinetic
  properties. These are used to run in silico experiments simulating
  real laboratory microarray experiments. Noise with controlled
  properties is added to the simulation results several times
  emulating measurement replicates, before expression ratios are
  calculated.Availability: The data sets and kinetic models described here are
  available from http://www.vbi.vt.edu/{\textasciitilde}mendes/AGN/as
  biochemical dynamic models in SBML and Gepasi formats.Contact: mendes@vt.edu*To whom correspondence should be
  addressed.},
	language = {en},
	number = {suppl\_2},
	urldate = {2018-02-01},
	journal = {Bioinformatics},
	author = {Mendes, Pedro and Sha, Wei and Ye, Keying},
	month = sep,
	year = {2003},
	pages = {ii122--ii129},
}

@article{stovitz_paediatric_2018,
	title = {Paediatric obesity appears to lower the risk of diabetes if selection bias is ignored},
	copyright = {© Article author(s) (or their employer(s) unless otherwise stated in the text of the article) 2018. All rights reserved. No commercial use is permitted unless otherwise expressly granted.},
	issn = {0143-005X, 1470-2738},
	url = {http://jech.bmj.com/content/early/2018/01/25/jech-2017-209985},
	doi = {10.1136/jech-2017-209985},
	abstract = {Background Frustrated with the onslaught of articles reporting fascination with results that appear paradoxical but are merely due to selection bias, we studied the apparent effect of obesity on diabetes risk in youth who had a test for diabetes. We hypothesised that obese subjects would have lower rates of diabetes than non-obese subjects due to selection bias, and consequently, obesity would appear to lower the risk of diabetes.
Methods Retrospective cohort study of children (4–9 years), pre-teens (10–12 years) and teenagers (13–19 years). Participation was restricted to those who had a test of haemoglobin A1C along with measured height and weight. Body mass index percentile via the Centers for Disease Control and Prevention age and sex standards was calculated and categorised. The main outcome was A1C\%, subsequently categorised at the level for diagnosis of diabetes mellitus (≥6.5\%).
Results The sample consisted of 134 (2\%) underweight, 1718 (30\%) healthy weight, 660 (12\%) overweight and 3190 (56\%) obese individuals. 16\% (n=936) had an A1C≥6.5\%. Overall, healthy weight children had 8.2 times the risk of A1C≥6.5\% (95\% CI 5.3 to 12.7) compared with those in the obese category. The relative risk was 13 in pre-teens (95\% CI 8.5 to 20.0) and 3.9 in teenagers (95\% CI 3.3 to 4.7).
Conclusions Healthy weight was associated with a 4–13 times higher relative risk of diabetes mellitus compared with being obese. While apparently shocking, the study’s fatal flaw (selection bias) explains the ‘paradoxical’ finding. Ignoring selection bias can delay advances in medical science.},
	language = {en},
	urldate = {2018-01-31},
	journal = {J Epidemiol Community Health},
	author = {Stovitz, Steven D. and Banack, Hailey R. and Kaufman, Jay S.},
	month = jan,
	year = {2018},
	pmid = {29374028},
	keywords = {epidemiological methods, obesity, research design in epidemiology, study design},
	pages = {jech--2017--209985},
}

@article{janson_eigenprism:_2015,
	title = {{EigenPrism}: {Inference} for {High}-{Dimensional} {Signal}-to-{Noise} {Ratios}},
	shorttitle = {{EigenPrism}},
	url = {http://arxiv.org/abs/1505.02097},
	abstract = {Consider the following three important problems in statistical inference, namely, constructing confidence intervals for (1) the error of a high-dimensional (\$p{\textgreater}n\$) regression estimator, (2) the linear regression noise level, and (3) the genetic signal-to-noise ratio of a continuous-valued trait (related to the heritability). All three problems turn out to be closely related to the little-studied problem of performing inference on the \${\textbackslash}ell\_2\$-norm of the signal in high-dimensional linear regression. We derive a novel procedure for this, which is asymptotically correct when the covariates are multivariate Gaussian and produces valid confidence intervals in finite samples as well. The procedure, called EigenPrism, is computationally fast and makes no assumptions on coefficient sparsity or knowledge of the noise level. We investigate the width of the EigenPrism confidence intervals, including a comparison with a Bayesian setting in which our interval is just 5\% wider than the Bayes credible interval. We are then able to unify the three aforementioned problems by showing that the EigenPrism procedure with only minor modifications is able to make important contributions to all three. We also investigate the robustness of coverage and find that the method applies in practice and in finite samples much more widely than just the case of multivariate Gaussian covariates. Finally, we apply EigenPrism to a genetic dataset to estimate the genetic signal-to-noise ratio for a number of continuous phenotypes.},
	urldate = {2018-01-30},
	journal = {arXiv:1505.02097 [stat]},
	author = {Janson, Lucas and Barber, Rina Foygel and Candès, Emmanuel},
	month = may,
	year = {2015},
	note = {arXiv: 1505.02097},
	keywords = {Statistics - Methodology},
}

@article{ertefaie_variable_2015,
	title = {Variable {Selection} in {Causal} {Inference} using a {Simultaneous} {Penalization} {Method}},
	url = {http://arxiv.org/abs/1511.08501},
	abstract = {In the causal adjustment setting, variable selection techniques based on one of either the outcome or treatment allocation model can result in the omission of confounders, which leads to bias, or the inclusion of spurious variables, which leads to variance inflation, in the propensity score. We propose a variable selection method based on a penalized objective function which considers the outcome and treatment assignment models simultaneously. The proposed method facilitates confounder selection in high-dimensional settings. We show that under regularity conditions our method attains the oracle property. The selected variables are used to form a doubly robust regression estimator of the treatment effect. We show that under some conditions our method attains the oracle property. Simulation results are presented and economic growth data are analyzed. Specifically, we study the effect of life expectancy as a measure of population health on the average growth rate of gross domestic product per capita.},
	urldate = {2018-01-30},
	journal = {arXiv:1511.08501 [stat]},
	author = {Ertefaie, Ashkan and Asgharian, Masoud and Stephens, David},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.08501},
	keywords = {Statistics - Methodology},
}

@article{schmitt_compendium_2016,
	title = {A {Compendium} of {Chromatin} {Contact} {Maps} {Reveals} {Spatially} {Active} {Regions} in the {Human} {Genome}},
	volume = {17},
	issn = {2211-1247},
	url = {http://www.sciencedirect.com/science/article/pii/S2211124716314814},
	doi = {10.1016/j.celrep.2016.10.061},
	abstract = {Summary
The three-dimensional configuration of DNA is integral to all nuclear processes in eukaryotes, yet our knowledge of the chromosome architecture is still limited. Genome-wide chromosome conformation capture studies have uncovered features of chromatin organization in cultured cells, but genome architecture in human tissues has yet to be explored. Here, we report the most comprehensive survey to date of chromatin organization in human tissues. Through integrative analysis of chromatin contact maps in 21 primary human tissues and cell types, we find topologically associating domains highly conserved in different tissues. We also discover genomic regions that exhibit unusually high levels of local chromatin interactions. These frequently interacting regions (FIREs) are enriched for super-enhancers and are near tissue-specifically expressed genes. They display strong tissue-specificity in local chromatin interactions. Additionally, FIRE formation is partially dependent on CTCF and the Cohesin complex. We further show that FIREs can help annotate the function of non-coding sequence variants.},
	number = {8},
	urldate = {2018-01-22},
	journal = {Cell Reports},
	author = {Schmitt, Anthony D. and Hu, Ming and Jung, Inkyung and Xu, Zheng and Qiu, Yunjiang and Tan, Catherine L. and Li, Yun and Lin, Shin and Lin, Yiing and Barr, Cathy L. and Ren, Bing},
	month = nov,
	year = {2016},
	pages = {2042--2059},
}

@article{hodos_cell-specific_2018,
	title = {Cell-specific prediction and application of drug-induced gene expression profiles},
	volume = {23},
	issn = {2335-6936},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5753597/},
	abstract = {Gene expression profiling of in vitro drug perturbations is useful for many biomedical discovery applications including drug repurposing and elucidation of drug mechanisms. However, limited data availability across cell types has hindered our capacity to leverage or explore the cell-specificity of these perturbations. While recent efforts have generated a large number of drug perturbation profiles across a variety of human cell types, many gaps remain in this combinatorial drug-cell space. Hence, we asked whether it is possible to fill these gaps by predicting cell-specific drug perturbation profiles using available expression data from related conditions--i.e. from other drugs and cell types. We developed a computational framework that first arranges existing profiles into a three-dimensional array (or tensor) indexed by drugs, genes, and cell types, and then uses either local (nearest-neighbors) or global (tensor completion) information to predict unmeasured profiles. We evaluate prediction accuracy using a variety of metrics, and find that the two methods have complementary performance, each superior in different regions in the drug-cell space. Predictions achieve correlations of 0.68 with true values, and maintain accurate differentially expressed genes (AUC 0.81). Finally, we demonstrate that the predicted profiles add value for making downstream associations with drug targets and therapeutic classes.},
	urldate = {2018-01-22},
	journal = {Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing},
	author = {Hodos, Rachel and Zhang, Ping and Lee, Hao-Chih and Duan, Qiaonan and Wang, Zichen and Clark, Neil R. and Ma'ayan, Avi and Wang, Fei and Kidd, Brian and Hu, Jianying and Sontag, David and Dudley, Joel},
	year = {2018},
	pmid = {29218867},
	pmcid = {PMC5753597},
	pages = {32--43},
}

@article{keenan_library_2017,
	title = {The {Library} of {Integrated} {Network}-{Based} {Cellular} {Signatures} {NIH} {Program}: {System}-{Level} {Cataloging} of {Human} {Cells} {Response} to {Perturbations}},
	volume = {0},
	issn = {2405-4712},
	shorttitle = {The {Library} of {Integrated} {Network}-{Based} {Cellular} {Signatures} {NIH} {Program}},
	url = {http://www.cell.com/cell-systems/abstract/S2405-4712(17)30490-8},
	doi = {10.1016/j.cels.2017.11.001},
	abstract = {The Library of Integrated Network-Based Cellular Signatures (LINCS) is an NIH Common Fund program that catalogs how human cells globally respond to chemical, genetic, and disease perturbations. Resources generated by LINCS include experimental and computational methods, visualization tools, molecular and imaging data, and signatures. By assembling an integrated picture of the range of responses of human cells exposed to many perturbations, the LINCS program aims to better understand human disease and to advance the development of new therapies. Perturbations under study include drugs, genetic perturbations, tissue micro-environments, antibodies, and disease-causing mutations. Responses to perturbations are measured by transcript profiling, mass spectrometry, cell imaging, and biochemical methods, among other assays. The LINCS program focuses on cellular physiology shared among tissues and cell types relevant to an array of diseases, including cancer, heart disease, and neurodegenerative disorders. This Perspective describes LINCS technologies, datasets, tools, and approaches to data accessibility and reusability.},
	language = {English},
	number = {0},
	urldate = {2018-01-22},
	journal = {Cell Systems},
	author = {Keenan, Alexandra B. and Jenkins, Sherry L. and Jagodnik, Kathleen M. and Koplev, Simon and He, Edward and Torre, Denis and Wang, Zichen and Dohlman, Anders B. and Silverstein, Moshe C. and Lachmann, Alexander and Kuleshov, Maxim V. and Ma'ayan, Avi and Stathias, Vasileios and Terryn, Raymond and Cooper, Daniel and Forlin, Michele and Koleti, Amar and Vidovic, Dusica and Chung, Caty and Schürer, Stephan C. and Vasiliauskas, Jouzas and Pilarczyk, Marcin and Shamsaei, Behrouz and Fazel, Mehdi and Ren, Yan and Niu, Wen and Clark, Nicholas A. and White, Shana and Mahi, Naim and Zhang, Lixia and Kouril, Michal and Reichard, John F. and Sivaganesan, Siva and Medvedovic, Mario and Meller, Jaroslaw and Koch, Rick J. and Birtwistle, Marc R. and Iyengar, Ravi and Sobie, Eric A. and Azeloglu, Evren U. and Kaye, Julia and Osterloh, Jeannette and Haston, Kelly and Kalra, Jaslin and Finkbiener, Steve and Li, Jonathan and Milani, Pamela and Adam, Miriam and Escalante-Chong, Renan and Sachs, Karen and Lenail, Alex and Ramamoorthy, Divya and Fraenkel, Ernest and Daigle, Gavin and Hussain, Uzma and Coye, Alyssa and Rothstein, Jeffrey and Sareen, Dhruv and Ornelas, Loren and Banuelos, Maria and Mandefro, Berhan and Ho, Ritchie and Svendsen, Clive N. and Lim, Ryan G. and Stocksdale, Jennifer and Casale, Malcolm S. and Thompson, Terri G. and Wu, Jie and Thompson, Leslie M. and Dardov, Victoria and Venkatraman, Vidya and Matlock, Andrea and Eyk, Jennifer E. Van and Jaffe, Jacob D. and Papanastasiou, Malvina and Subramanian, Aravind and Golub, Todd R. and Erickson, Sean D. and Fallahi-Sichani, Mohammad and Hafner, Marc and Gray, Nathanael S. and Lin, Jia-Ren and Mills, Caitlin E. and Muhlich, Jeremy L. and Niepel, Mario and Shamu, Caroline E. and Williams, Elizabeth H. and Wrobel, David and Sorger, Peter K. and Heiser, Laura M. and Gray, Joe W. and Korkola, James E. and Mills, Gordon B. and LaBarge, Mark and Feiler, Heidi S. and Dane, Mark A. and Bucher, Elmar and Nederlof, Michel and Sudar, Damir and Gross, Sean and Kilburn, David F. and Smith, Rebecca and Devlin, Kaylyn and Margolis, Ron and Derr, Leslie and Lee, Albert and Pillai, Ajay},
	month = nov,
	year = {2017},
	pmid = {29199020},
	keywords = {BD2K, L1000, MCF10A, MEMA, P100, data integration, lincsprogram, lincsproject, systems biology, systems pharmacology},
}

@article{subramanian_next_2017,
	title = {A {Next} {Generation} {Connectivity} {Map}: {L1000} {Platform} and the {First} 1,000,000 {Profiles}},
	volume = {171},
	issn = {0092-8674, 1097-4172},
	shorttitle = {A {Next} {Generation} {Connectivity} {Map}},
	url = {http://www.cell.com/cell/abstract/S0092-8674(17)31309-0},
	doi = {10.1016/j.cell.2017.10.049},
	abstract = {We previously piloted the concept of a Connectivity Map (CMap), whereby genes, drugs, and disease states are connected by virtue of common gene-expression signatures. Here, we report more than a 1,000-fold scale-up of the CMap as part of the NIH LINCS Consortium, made possible by a new, low-cost, high-throughput reduced representation expression profiling method that we term L1000. We show that L1000 is highly reproducible, comparable to RNA sequencing, and suitable for computational inference of the expression levels of 81\% of non-measured transcripts. We further show that the expanded CMap can be used to discover mechanism of action of small molecules, functionally annotate genetic variants of disease genes, and inform clinical trials. The 1.3 million L1000 profiles described here, as well as tools for their analysis, are available at https://clue.io.},
	language = {English},
	number = {6},
	urldate = {2018-01-22},
	journal = {Cell},
	author = {Subramanian, Aravind and Narayan, Rajiv and Corsello, Steven M. and Peck, David D. and Natoli, Ted E. and Lu, Xiaodong and Gould, Joshua and Davis, John F. and Tubelli, Andrew A. and Asiedu, Jacob K. and Lahr, David L. and Hirschman, Jodi E. and Liu, Zihan and Donahue, Melanie and Julian, Bina and Khan, Mariya and Wadden, David and Smith, Ian C. and Lam, Daniel and Liberzon, Arthur and Toder, Courtney and Bagul, Mukta and Orzechowski, Marek and Enache, Oana M. and Piccioni, Federica and Johnson, Sarah A. and Lyons, Nicholas J. and Berger, Alice H. and Shamji, Alykhan F. and Brooks, Angela N. and Vrcic, Anita and Flynn, Corey and Rosains, Jacqueline and Takeda, David Y. and Hu, Roger and Davison, Desiree and Lamb, Justin and Ardlie, Kristin and Hogstrom, Larson and Greenside, Peyton and Gray, Nathanael S. and Clemons, Paul A. and Silver, Serena and Wu, Xiaoyun and Zhao, Wen-Ning and Read-Button, Willis and Wu, Xiaohua and Haggarty, Stephen J. and Ronco, Lucienne V. and Boehm, Jesse S. and Schreiber, Stuart L. and Doench, John G. and Bittker, Joshua A. and Root, David E. and Wong, Bang and Golub, Todd R.},
	month = nov,
	year = {2017},
	pmid = {29195078},
	keywords = {Functional genomics, chemical biology, gene expression profiling},
	pages = {1437--1452.e17},
}

@article{schmitt_compendium_2016-1,
	title = {A {Compendium} of {Chromatin} {Contact} {Maps} {Reveals} {Spatially} {Active} {Regions} in the {Human} {Genome}},
	volume = {17},
	issn = {2211-1247},
	url = {http://www.sciencedirect.com/science/article/pii/S2211124716314814},
	doi = {10.1016/j.celrep.2016.10.061},
	abstract = {Summary
The three-dimensional configuration of DNA is integral to all nuclear processes in eukaryotes, yet our knowledge of the chromosome architecture is still limited. Genome-wide chromosome conformation capture studies have uncovered features of chromatin organization in cultured cells, but genome architecture in human tissues has yet to be explored. Here, we report the most comprehensive survey to date of chromatin organization in human tissues. Through integrative analysis of chromatin contact maps in 21 primary human tissues and cell types, we find topologically associating domains highly conserved in different tissues. We also discover genomic regions that exhibit unusually high levels of local chromatin interactions. These frequently interacting regions (FIREs) are enriched for super-enhancers and are near tissue-specifically expressed genes. They display strong tissue-specificity in local chromatin interactions. Additionally, FIRE formation is partially dependent on CTCF and the Cohesin complex. We further show that FIREs can help annotate the function of non-coding sequence variants.},
	number = {8},
	urldate = {2018-01-22},
	journal = {Cell Reports},
	author = {Schmitt, Anthony D. and Hu, Ming and Jung, Inkyung and Xu, Zheng and Qiu, Yunjiang and Tan, Catherine L. and Li, Yun and Lin, Shin and Lin, Yiing and Barr, Cathy L. and Ren, Bing},
	month = nov,
	year = {2016},
	pages = {2042--2059},
}

@article{chai_review_2014,
	title = {A review on the computational approaches for gene regulatory network construction},
	volume = {48},
	journal = {Computers in biology and medicine},
	author = {Chai, Lian En and Loh, Swee Kuan and Low, Swee Thing and Mohamad, Mohd Saberi and Deris, Safaai and Zakaria, Zalmiyah},
	year = {2014},
	pages = {55--65},
}

@article{melancon_random_2001,
	title = {Random generation of directed acyclic graphs},
	volume = {10},
	journal = {Electronic Notes in Discrete Mathematics},
	author = {Melançon, Guy and Dutour, Isabelle and Bousquet-Mélou, Mireille},
	year = {2001},
	pages = {202--207},
}

@article{chai_review_2014-1,
	title = {A review on the computational approaches for gene regulatory network construction},
	volume = {48},
	journal = {Computers in biology and medicine},
	author = {Chai, Lian En and Loh, Swee Kuan and Low, Swee Thing and Mohamad, Mohd Saberi and Deris, Safaai and Zakaria, Zalmiyah},
	year = {2014},
	pages = {55--65},
}

@article{de_smet_advantages_2010,
	title = {Advantages and limitations of current network inference methods},
	volume = {8},
	number = {10},
	journal = {Nature Reviews Microbiology},
	author = {De Smet, Riet and Marchal, Kathleen},
	year = {2010},
	pages = {717--729},
}

@article{thompson_comparative_2015,
	title = {Comparative analysis of gene regulatory networks: from network reconstruction to evolution},
	volume = {31},
	shorttitle = {Comparative analysis of gene regulatory networks},
	journal = {Annual review of cell and developmental biology},
	author = {Thompson, Dawn and Regev, Aviv and Roy, Sushmita},
	year = {2015},
	pages = {399--428},
}

@article{kalisch_package_2017,
	title = {Package ‘pcalg’},
	author = {Kalisch, Markus and Hauser, Alain and Maechler, Martin and Colombo, Diego and Entner, Doris and Hoyer, Patrik and Hyttinen, Antti and Peters, Jonas and Andri, Nicoletta and Perkovic, Emilija},
	year = {2017},
}

@article{kalisch_package_2017-1,
	title = {Package ‘pcalg’},
	author = {Kalisch, Markus and Hauser, Alain and Maechler, Martin and Colombo, Diego and Entner, Doris and Hoyer, Patrik and Hyttinen, Antti and Peters, Jonas and Andri, Nicoletta and Perkovic, Emilija},
	year = {2017},
}

@article{kalisch_package_2017-2,
	title = {Package ‘pcalg’},
	author = {Kalisch, Markus and Hauser, Alain and Maechler, Martin and Colombo, Diego and Entner, Doris and Hoyer, Patrik and Hyttinen, Antti and Peters, Jonas and Andri, Nicoletta and Perkovic, Emilija},
	year = {2017},
}

@book{scutari_package_2017,
	title = {Package ‘bnlearn’},
	author = {Scutari, Marco and Scutari, Maintainer Marco and MMPC, Hiton-PC},
	year = {2017},
}

@article{melancon_generating_2004,
	title = {Generating connected acyclic digraphs uniformly at random},
	url = {http://arxiv.org/abs/cs/0403040},
	abstract = {We describe a simple algorithm based on a Markov chain process to generate simply connected acyclic directed graphs over a fixed set of vertices. This algorithm is an extension of a previous one, designed to generate acyclic digraphs, non necessarily connected.},
	urldate = {2018-01-10},
	journal = {arXiv:cs/0403040},
	author = {Melancon, Guy and Philippe, Fabrice},
	month = mar,
	year = {2004},
	note = {arXiv: cs/0403040},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Discrete Mathematics, F.2.2, G.2.2, G.3},
}

@book{nagarajan_bayesian_2013,
	title = {Bayesian networks in {R}},
	volume = {122},
	publisher = {Springer},
	author = {Nagarajan, Radhakrishnan and Scutari, Marco and Lèbre, Sophie},
	year = {2013},
}

@article{kurtz_sparse_2015,
	title = {Sparse and {Compositionally} {Robust} {Inference} of {Microbial} {Ecological} {Networks}},
	volume = {11},
	issn = {1553-7358},
	url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004226},
	doi = {10.1371/journal.pcbi.1004226},
	abstract = {Author Summary Genomic survey of microbes by 16S rRNA gene sequencing and metagenomics has inspired appreciation for the role of complex communities in diverse ecosystems. However, due to the unique properties of community composition data, standard data analysis tools are likely to produce statistical artifacts. For a typical experiment studying microbial ecosystems these artifacts can lead to erroneous conclusions about patterns of associations between microbial taxa. We developed a new procedure that seeks to infer ecological associations between microbial populations, by 1) taking advantage of the proportionality invariance of relative abundance data and 2) making assumptions about the underlying network structure when the number of taxa in the dataset is larger than the number of sampled communities. Additionally, we employed a novel tool to generate biologically plausible synthetic data and objectively benchmark current association inference tools. Finally, we tested our procedures on a large-scale 16S rRNA gene sequencing dataset sampled from the human gut.},
	language = {en},
	number = {5},
	urldate = {2018-01-10},
	journal = {PLOS Computational Biology},
	author = {Kurtz, Zachary D. and Müller, Christian L. and Miraldi, Emily R. and Littman, Dan R. and Blaser, Martin J. and Bonneau, Richard A.},
	month = may,
	year = {2015},
	keywords = {Community ecology, Covariance, Microbial ecology, Microbiome, Network analysis, Scale-free networks, Statistical data, Theoretical ecology},
	pages = {e1004226},
}

@article{kurtz_sparse_2015-1,
	title = {Sparse and {Compositionally} {Robust} {Inference} of {Microbial} {Ecological} {Networks}},
	volume = {11},
	issn = {1553-7358},
	url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004226},
	doi = {10.1371/journal.pcbi.1004226},
	abstract = {Author Summary Genomic survey of microbes by 16S rRNA gene sequencing and metagenomics has inspired appreciation for the role of complex communities in diverse ecosystems. However, due to the unique properties of community composition data, standard data analysis tools are likely to produce statistical artifacts. For a typical experiment studying microbial ecosystems these artifacts can lead to erroneous conclusions about patterns of associations between microbial taxa. We developed a new procedure that seeks to infer ecological associations between microbial populations, by 1) taking advantage of the proportionality invariance of relative abundance data and 2) making assumptions about the underlying network structure when the number of taxa in the dataset is larger than the number of sampled communities. Additionally, we employed a novel tool to generate biologically plausible synthetic data and objectively benchmark current association inference tools. Finally, we tested our procedures on a large-scale 16S rRNA gene sequencing dataset sampled from the human gut.},
	language = {en},
	number = {5},
	urldate = {2018-01-10},
	journal = {PLOS Computational Biology},
	author = {Kurtz, Zachary D. and Müller, Christian L. and Miraldi, Emily R. and Littman, Dan R. and Blaser, Martin J. and Bonneau, Richard A.},
	month = may,
	year = {2015},
	keywords = {Community ecology, Covariance, Microbial ecology, Microbiome, Network analysis, Scale-free networks, Statistical data, Theoretical ecology},
	pages = {e1004226},
}

@article{marbach_wisdom_2012,
	title = {Wisdom of crowds for robust gene network inference},
	volume = {9},
	copyright = {2012 Nature Publishing Group},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.2016},
	doi = {10.1038/nmeth.2016},
	abstract = {{\textless}p{\textgreater}This analysis comprehensively compares methods for gene regulatory network inference submitted through the DREAM5 challenge. It demonstrates that integration of predictions from multiple methods shows the most robust performance across data sets.{\textless}/p{\textgreater}},
	language = {En},
	number = {8},
	urldate = {2018-01-10},
	journal = {Nature Methods},
	author = {Marbach, Daniel and Costello, James C. and Küffner, Robert and Vega, Nicole M. and Prill, Robert J. and Camacho, Diogo M. and Allison, Kyle R. and Consortium, The DREAM5 and Aderhold, Andrej and Allison, Kyle R. and Bonneau, Richard and Camacho, Diogo M. and Chen, Yukun and Collins, James J. and Cordero, Francesca and Costello, James C. and Crane, Martin and Dondelinger, Frank and Drton, Mathias and Esposito, Roberto and Foygel, Rina and Fuente, Alberto de la and Gertheiss, Jan and Geurts, Pierre and Greenfield, Alex and Grzegorczyk, Marco and Haury, Anne-Claire and Holmes, Benjamin and Hothorn, Torsten and Husmeier, Dirk and Huynh-Thu, Vân Anh and Irrthum, Alexandre and Kellis, Manolis and Karlebach, Guy and Küffner, Robert and Lèbre, Sophie and Leo, Vincenzo De and Madar, Aviv and Mani, Subramani and Marbach, Daniel and Mordelet, Fantine and Ostrer, Harry and Ouyang, Zhengyu and Pandya, Ravi and Petri, Tobias and Pinna, Andrea and Poultney, Christopher S. and Prill, Robert J. and Rezny, Serena and Ruskin, Heather J. and Saeys, Yvan and Shamir, Ron and Sîrbu, Alina and Song, Mingzhou and Soranzo, Nicola and Statnikov, Alexander and Stolovitzky, Gustavo and Vega, Nicci and Vera-Licona, Paola and Vert, Jean-Philippe and Visconti, Alessia and Wang, Haizhou and Wehenkel, Louis and Windhager, Lukas and Zhang, Yang and Zimmer, Ralf and Kellis, Manolis and Collins, James J. and Stolovitzky, Gustavo},
	month = aug,
	year = {2012},
	pages = {796},
}

@article{hill_inferring_2016,
	title = {Inferring causal molecular networks: empirical assessment through a community-based effort},
	volume = {13},
	copyright = {2016 Nature Publishing Group},
	issn = {1548-7105},
	shorttitle = {Inferring causal molecular networks},
	url = {https://www.nature.com/articles/nmeth.3773},
	doi = {10.1038/nmeth.3773},
	abstract = {{\textless}p{\textgreater}The HPN-DREAM community challenge assessed the ability of computational methods to infer causal molecular networks, focusing specifically on the task of inferring causal protein signaling networks in cancer cell lines.{\textless}/p{\textgreater}},
	language = {En},
	number = {4},
	urldate = {2018-01-10},
	journal = {Nature Methods},
	author = {Hill, Steven M. and Heiser, Laura M. and Cokelaer, Thomas and Unger, Michael and Nesser, Nicole K. and Carlin, Daniel E. and Zhang, Yang and Sokolov, Artem and Paull, Evan O. and Wong, Chris K. and Graim, Kiley and Bivol, Adrian and Wang, Haizhou and Zhu, Fan and Afsari, Bahman and Danilova, Ludmila V. and Favorov, Alexander V. and Lee, Wai Shing and Taylor, Dane and Hu, Chenyue W. and Long, Byron L. and Noren, David P. and Bisberg, Alexander J. and Consortium, The HPN-DREAM and Afsari, Bahman and Al-Ouran, Rami and Anton, Bernat and Arodz, Tomasz and Sichani, Omid Askari and Bagheri, Neda and Berlow, Noah and Bisberg, Alexander J. and Bivol, Adrian and Bohler, Anwesha and Bonet, Jaume and Bonneau, Richard and Budak, Gungor and Bunescu, Razvan and Caglar, Mehmet and Cai, Binghuang and Cai, Chunhui and Carlin, Daniel E. and Carlon, Azzurra and Chen, Lujia and Ciaccio, Mark F. and Cokelaer, Thomas and Cooper, Gregory and Creighton, Chad J. and Daneshmand, Seyed-Mohammad-Hadi and Fuente, Alberto de la and Camillo, Barbara Di and Danilova, Ludmila V. and Dutta-Moscato, Joyeeta and Emmett, Kevin and Evelo, Chris and Fassia, Mohammad-Kasim H. and Favorov, Alexander V. and Fertig, Elana J. and Finkle, Justin D. and Finotello, Francesca and Friend, Stephen and Gao, Xi and Gao, Jean and Garcia-Garcia, Javier and Ghosh, Samik and Giaretta, Alberto and Graim, Kiley and Gray, Joe W. and Großeholz, Ruth and Guan, Yuanfang and Guinney, Justin and Hafemeister, Christoph and Hahn, Oliver and Haider, Saad and Hase, Takeshi and Heiser, Laura M. and Hill, Steven M. and Hodgson, Jay and Hoff, Bruce and Hsu, Chih Hao and Hu, Chenyue W. and Hu, Ying and Huang, Xun and Jalili, Mahdi and Jiang, Xia and Kacprowski, Tim and Kaderali, Lars and Kang, Mingon and Kannan, Venkateshan and Kellen, Michael and Kikuchi, Kaito and Kim, Dong-Chul and Kitano, Hiroaki and Knapp, Bettina and Komatsoulis, George and Koeppl, Heinz and Krämer, Andreas and Kursa, Miron Bartosz and Kutmon, Martina and Lee, Wai Shing and Li, Yichao and Liang, Xiaoyu and Liu, Zhaoqi and Liu, Yu and Long, Byron L. and Lu, Songjian and Lu, Xinghua and Manfrini, Marco and Matos, Marta R. A. and Meerzaman, Daoud and Mills, Gordon B. and Min, Wenwen and Mukherjee, Sach and Müller, Christian Lorenz and Neapolitan, Richard E. and Nesser, Nicole K. and Noren, David P. and Norman, Thea and Oliva, Baldo and Opiyo, Stephen Obol and Pal, Ranadip and Palinkas, Aljoscha and Paull, Evan O. and Planas-Iglesias, Joan and Poglayen, Daniel and Qutub, Amina A. and Saez-Rodriguez, Julio and Sambo, Francesco and Sanavia, Tiziana and Sharifi-Zarchi, Ali and Slawek, Janusz and Sokolov, Artem and Song, Mingzhou and Spellman, Paul T. and Streck, Adam and Stolovitzky, Gustavo and Strunz, Sonja and Stuart, Joshua M. and Taylor, Dane and Tegnér, Jesper and Thobe, Kirste and Toffolo, Gianna Maria and Trifoglio, Emanuele and Unger, Michael and Wan, Qian and Wang, Haizhou and Welch, Lonnie and Wong, Chris K. and Wu, Jia J. and Xue, Albert Y. and Yamanaka, Ryota and Yan, Chunhua and Zairis, Sakellarios and Zengerling, Michael and Zenil, Hector and Zhang, Shihua and Zhang, Yang and Zhu, Fan and Zi, Zhike and Mills, Gordon B. and Gray, Joe W. and Kellen, Michael and Norman, Thea and Friend, Stephen and Qutub, Amina A. and Fertig, Elana J. and Guan, Yuanfang and Song, Mingzhou and Stuart, Joshua M. and Spellman, Paul T. and Koeppl, Heinz and Stolovitzky, Gustavo and Saez-Rodriguez, Julio and Mukherjee, Sach},
	month = apr,
	year = {2016},
	pages = {310},
}

@article{statnikov_ultra-scalable_2015,
	title = {Ultra-scalable and efficient methods for hybrid observational and experimental local causal pathway discovery.},
	volume = {16},
	journal = {Journal of Machine Learning Research},
	author = {Statnikov, Alexander R. and Ma, Sisi and Henaff, Mikael and Lytkin, Nikita I. and Efstathiadis, Efstratios and Peskin, Eric R. and Aliferis, Constantin F.},
	year = {2015},
	pages = {3219--3267},
}

@article{brent_past_2016,
	title = {Past roadblocks and new opportunities in transcription factor network mapping},
	volume = {32},
	issn = {0168-9525},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5117949/},
	doi = {10.1016/j.tig.2016.08.009},
	abstract = {One of the principal mechanisms by which cells differentiate and respond to changes in external signals or conditions is by changing the activity levels of transcription factors (TFs). This changes the transcription rates of target genes via the cell’s TF network, which ultimately contributes to reconfiguring cellular state. Since microarrays provided our first window into global cellular state, computational biologists have eagerly attacked the problem of mapping TF networks, a key part of the cell’s control circuitry. In retrospect, however, steady-state mRNA abundance levels were a poor substitute for TF activity levels and gene transcription rates. Likewise, mapping TF binding through chromatin immunoprecipitation proved less predictive of functional regulation and less amenable to systematic elucidation of complete networks than originally hoped. This review explains these roadblocks and the current, unprecedented blossoming of new experimental techniques built on second generation sequencing, which hold out the promise of rapid progress in TF network mapping.},
	number = {11},
	urldate = {2018-01-10},
	journal = {Trends in genetics : TIG},
	author = {Brent, Michael R.},
	month = nov,
	year = {2016},
	pmid = {27720190},
	pmcid = {PMC5117949},
	pages = {736--750},
}

@article{brent_past_2016-1,
	title = {Past {Roadblocks} and {New} {Opportunities} in {Transcription} {Factor} {Network} {Mapping}},
	volume = {32},
	issn = {0168-9525},
	url = {http://www.sciencedirect.com/science/article/pii/S0168952516301019},
	doi = {10.1016/j.tig.2016.08.009},
	abstract = {One of the principal mechanisms by which cells differentiate and respond to changes in external signals or conditions is by changing the activity levels of transcription factors (TFs). This changes the transcription rates of target genes via the cell's TF network, which ultimately contributes to reconfiguring cellular state. Since microarrays provided our first window into global cellular state, computational biologists have eagerly attacked the problem of mapping TF networks, a key part of the cell's control circuitry. In retrospect, however, steady-state mRNA abundance levels were a poor substitute for TF activity levels and gene transcription rates. Likewise, mapping TF binding through chromatin immunoprecipitation proved less predictive of functional regulation and less amenable to systematic elucidation of complete networks than originally hoped. This review explains these roadblocks and the current, unprecedented blossoming of new experimental techniques built on second-generation sequencing, which hold out the promise of rapid progress in TF network mapping.},
	number = {11},
	urldate = {2018-01-10},
	journal = {Trends in Genetics},
	author = {Brent, Michael R.},
	month = nov,
	year = {2016},
	keywords = {computational methods, gene expression profiling, nascent RNA sequencing, regulatory systems biology, transcription factor activity, transcriptional regulatory networks.},
	pages = {736--750},
}

@article{villaverde_biopredyn-bench:_2015,
	title = {{BioPreDyn}-bench: a suite of benchmark problems for dynamic modelling in systems biology},
	volume = {9},
	issn = {1752-0509},
	shorttitle = {{BioPreDyn}-bench},
	url = {https://doi.org/10.1186/s12918-015-0144-4},
	doi = {10.1186/s12918-015-0144-4},
	abstract = {Dynamic modelling is one of the cornerstones of systems biology. Many research efforts are currently being invested in the development and exploitation of large-scale kinetic models. The associated problems of parameter estimation (model calibration) and optimal experimental design are particularly challenging. The community has already developed many methods and software packages which aim to facilitate these tasks. However, there is a lack of suitable benchmark problems which allow a fair and systematic evaluation and comparison of these contributions.},
	urldate = {2018-01-10},
	journal = {BMC Systems Biology},
	author = {Villaverde, Alejandro F. and Henriques, David and Smallbone, Kieran and Bongard, Sophia and Schmid, Joachim and Cicin-Sain, Damjan and Crombach, Anton and Saez-Rodriguez, Julio and Mauch, Klaus and Balsa-Canto, Eva and Mendes, Pedro and Jaeger, Johannes and Banga, Julio R.},
	month = feb,
	year = {2015},
	keywords = {Benchmarks, Dynamic modelling, Large-scale, Metabolism, Model calibration, Optimization, Parameter estimation, Signal transduction, Transcription, development},
	pages = {8},
}

@article{pinna_simulating_2011,
	title = {Simulating systems genetics data with {SysGenSIM}},
	volume = {27},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/27/17/2459/224428},
	doi = {10.1093/bioinformatics/btr407},
	abstract = {Summary: SysGenSIM is a software package to simulate Systems Genetics (SG) experiments in model organisms, for the purpose of evaluating and comparing statistical and computational methods and their implementations for analyses of SG data [e.g. methods for expression quantitative trait loci (eQTL) mapping and network inference]. SysGenSIM allows the user to select a variety of network topologies, genetic and kinetic parameters to simulate SG data ( genotyping, gene expression and phenotyping) with large gene networks with thousands of nodes. The software is encoded in MATLAB, and a user-friendly graphical user interface is provided.Availability: The open-source software code and user manual can be downloaded at: http://sysgensim.sourceforge.net/Contact:alf@crs4.it},
	language = {en},
	number = {17},
	urldate = {2018-01-10},
	journal = {Bioinformatics},
	author = {Pinna, Andrea and Soranzo, Nicola and Hoeschele, Ina and de la Fuente, Alberto},
	month = sep,
	year = {2011},
	pages = {2459--2462},
}

@article{tripathi_sgnesr:_2017,
	title = {{sgnesR}: {An} {R} package for simulating gene expression data from an underlying real gene network structure considering delay parameters},
	volume = {18},
	issn = {1471-2105},
	shorttitle = {{sgnesR}},
	url = {https://doi.org/10.1186/s12859-017-1731-8},
	doi = {10.1186/s12859-017-1731-8},
	abstract = {sgnesR (Stochastic Gene Network Expression Simulator in R) is an R package that provides an interface to simulate gene expression data from a given gene network using the stochastic simulation algorithm (SSA). The package allows various options for delay parameters and can easily included in reactions for promoter delay, RNA delay and Protein delay. A user can tune these parameters to model various types of reactions within a cell. As examples, we present two network models to generate expression profiles. We also demonstrated the inference of networks and the evaluation of association measure of edge and non-edge components from the generated expression profiles.},
	urldate = {2018-01-10},
	journal = {BMC Bioinformatics},
	author = {Tripathi, Shailesh and Lloyd-Price, Jason and Ribeiro, Andre and Yli-Harja, Olli and Dehmer, Matthias and Emmert-Streib, Frank},
	month = jul,
	year = {2017},
	keywords = {Gene expression data, Gene network, Simulation},
	pages = {325},
}

@article{coker_signet:_2017,
	title = {{SiGNet}: {A} signaling network data simulator to enable signaling network inference},
	volume = {12},
	issn = {1932-6203},
	shorttitle = {{SiGNet}},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0177701},
	doi = {10.1371/journal.pone.0177701},
	abstract = {Network models are widely used to describe complex signaling systems. Cellular wiring varies in different cellular contexts and numerous inference techniques have been developed to infer the structure of a network from experimental data of the network’s behavior. To objectively identify which inference strategy is best suited to a specific network, a gold standard network and dataset are required. However, suitable datasets for benchmarking are difficult to find. Numerous tools exist that can simulate data for transcriptional networks, but these are of limited use for the study of signaling networks. Here, we describe SiGNet (Signal Generator for Networks): a Cytoscape app that simulates experimental data for a signaling network of known structure. SiGNet has been developed and tested against published experimental data, incorporating information on network architecture, and the directionality and strength of interactions to create biological data in silico. SiGNet is the first tool to simulate biological signaling data, enabling an accurate and systematic assessment of inference strategies. SiGNet can also be used to produce preliminary models of key biological pathways following perturbation.},
	language = {en},
	number = {5},
	urldate = {2018-01-10},
	journal = {PLOS ONE},
	author = {Coker, Elizabeth A. and Mitsopoulos, Costas and Workman, Paul and Al-Lazikani, Bissan},
	month = may,
	year = {2017},
	keywords = {Biochemical simulations, Cell signaling structures, DNA transcription, Network motifs, Protein interaction networks, Protein structure networks, Signaling networks, Simulation and modeling},
	pages = {e0177701},
}

@article{sachs_causal_2005,
	title = {Causal protein-signaling networks derived from multiparameter single-cell data},
	volume = {308},
	number = {5721},
	journal = {Science},
	author = {Sachs, Karen and Perez, Omar and Pe'er, Dana and Lauffenburger, Douglas A. and Nolan, Garry P.},
	year = {2005},
	pages = {523--529},
}

@article{le_novere_quantitative_2015,
	title = {Quantitative and logic modelling of molecular and gene networks},
	author = {Le Novère, Nicolas},
	year = {2015},
}

@article{hill_bayesian_2011,
	title = {Bayesian nonparametric modeling for causal inference},
	volume = {20},
	number = {1},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Hill, Jennifer L.},
	year = {2011},
	pages = {217--240},
}

@article{shalit_estimating_2016,
	title = {Estimating individual treatment effect: generalization bounds and algorithms},
	shorttitle = {Estimating individual treatment effect},
	url = {http://arxiv.org/abs/1606.03976},
	abstract = {There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a "balanced" representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization-error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.},
	urldate = {2018-01-08},
	journal = {arXiv:1606.03976 [cs, stat]},
	author = {Shalit, Uri and Johansson, Fredrik D. and Sontag, David},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.03976},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning},
}

@article{louizos_causal_2017,
	title = {Causal {Effect} {Inference} with {Deep} {Latent}-{Variable} {Models}},
	url = {http://arxiv.org/abs/1705.08821},
	abstract = {Learning individual-level causal effects from observational data, such as inferring the most effective medication for a specific patient, is a problem of growing importance for policy makers. The most important aspect of inferring causal effects from observational data is the handling of confounders, factors that affect both an intervention and its outcome. A carefully designed observational study attempts to measure all important confounders. However, even if one does not have direct access to all confounders, there may exist noisy and uncertain measurement of proxies for confounders. We build on recent advances in latent variable modeling to simultaneously estimate the unknown latent space summarizing the confounders and the causal effect. Our method is based on Variational Autoencoders (VAE) which follow the causal structure of inference with proxies. We show our method is significantly more robust than existing methods, and matches the state-of-the-art on previous benchmarks focused on individual treatment effects.},
	urldate = {2018-01-08},
	journal = {arXiv:1705.08821 [cs, stat]},
	author = {Louizos, Christos and Shalit, Uri and Mooij, Joris and Sontag, David and Zemel, Richard and Welling, Max},
	month = may,
	year = {2017},
	note = {arXiv: 1705.08821},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{muandet_kernel_2017,
	title = {Kernel mean embedding of distributions: {A} review and beyond},
	volume = {10},
	shorttitle = {Kernel mean embedding of distributions},
	number = {1-2},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Muandet, Krikamol and Fukumizu, Kenji and Sriperumbudur, Bharath and Schölkopf, Bernhard},
	year = {2017},
	pages = {1--141},
}

@article{muandet_kernel_2017-1,
	title = {Kernel mean embedding of distributions: {A} review and beyond},
	volume = {10},
	shorttitle = {Kernel mean embedding of distributions},
	number = {1-2},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Muandet, Krikamol and Fukumizu, Kenji and Sriperumbudur, Bharath and Schölkopf, Bernhard},
	year = {2017},
	pages = {1--141},
}

@article{szabo_learning_2016,
	title = {Learning theory for distribution regression},
	volume = {17},
	number = {152},
	journal = {Journal of Machine Learning Research},
	author = {Szabó, Zoltán and Sriperumbudur, Bharath K. and Póczos, Barnabás and Gretton, Arthur},
	year = {2016},
	pages = {1--40},
}

@incollection{janzing_justifying_2015,
	title = {Justifying information-geometric causal inference},
	booktitle = {Measures of {Complexity}},
	publisher = {Springer},
	author = {Janzing, Dominik and Steudel, Bastian and Shajarisales, Naji and Schölkopf, Bernhard},
	year = {2015},
	pages = {253--265},
}

@article{louizos_causal_2017-1,
	title = {Causal {Effect} {Inference} with {Deep} {Latent}-{Variable} {Models}},
	url = {http://arxiv.org/abs/1705.08821},
	abstract = {Learning individual-level causal effects from observational data, such as inferring the most effective medication for a specific patient, is a problem of growing importance for policy makers. The most important aspect of inferring causal effects from observational data is the handling of confounders, factors that affect both an intervention and its outcome. A carefully designed observational study attempts to measure all important confounders. However, even if one does not have direct access to all confounders, there may exist noisy and uncertain measurement of proxies for confounders. We build on recent advances in latent variable modeling to simultaneously estimate the unknown latent space summarizing the confounders and the causal effect. Our method is based on Variational Autoencoders (VAE) which follow the causal structure of inference with proxies. We show our method is significantly more robust than existing methods, and matches the state-of-the-art on previous benchmarks focused on individual treatment effects.},
	urldate = {2018-01-05},
	journal = {arXiv:1705.08821 [cs, stat]},
	author = {Louizos, Christos and Shalit, Uri and Mooij, Joris and Sontag, David and Zemel, Richard and Welling, Max},
	month = may,
	year = {2017},
	note = {arXiv: 1705.08821},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{sella_miic_2017,
	title = {{MIIC} online: a web server to reconstruct causal or non-causal networks from non-perturbative data},
	shorttitle = {{MIIC} online},
	journal = {Bioinformatics},
	author = {Sella, Nadir and Verny, Louis and Uguzzoni, Guido and Affeldt, Séverine and Isambert, Hervé},
	year = {2017},
	keywords = {team},
}

@article{peters_identifiability_2012,
	title = {Identifiability of causal graphs using functional models},
	journal = {arXiv preprint arXiv:1202.3757},
	author = {Peters, Jonas and Mooij, Joris and Janzing, Dominik and Schölkopf, Bernhard},
	year = {2012},
}

@article{peters_identifiability_2014,
	title = {Identifiability of {Gaussian} structural equation models with equal error variances},
	volume = {101},
	issn = {1464-3510, 0006-3444},
	url = {http://arxiv.org/abs/1205.2536},
	doi = {10.1093/biomet/ast043},
	abstract = {We consider structural equation models in which variables can be written as a function of their parents and noise terms, which are assumed to be jointly independent. Corresponding to each structural equation model, there is a directed acyclic graph describing the relationships between the variables. In Gaussian structural equation models with linear functions, the graph can be identified from the joint distribution only up to Markov equivalence classes, assuming faithfulness. In this work, we prove full identifiability if all noise variables have the same variances: the directed acyclic graph can be recovered from the joint Gaussian distribution. Our result has direct implications for causal inference: if the data follow a Gaussian structural equation model with equal error variances and assuming that all variables are observed, the causal structure can be inferred from observational data only. We propose a statistical method and an algorithm that exploit our theoretical findings.},
	number = {1},
	urldate = {2018-01-04},
	journal = {Biometrika},
	author = {Peters, Jonas and Bühlmann, Peter},
	month = mar,
	year = {2014},
	note = {arXiv: 1205.2536},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning},
	pages = {219--228},
}

@article{peters_identifiability_2013,
	title = {Identifiability of {Gaussian} structural equation models with equal error variances},
	volume = {101},
	number = {1},
	journal = {Biometrika},
	author = {Peters, Jonas and Bühlmann, Peter},
	year = {2013},
	pages = {219--228},
}

@article{dorie_automated_2017,
	title = {Automated versus do-it-yourself methods for causal inference: {Lessons} learned from a data analysis competition},
	shorttitle = {Automated versus do-it-yourself methods for causal inference},
	journal = {arXiv preprint arXiv:1707.02641},
	author = {Dorie, Vincent and Hill, Jennifer and Shalit, Uri and Scott, Marc and Cervone, Dan},
	year = {2017},
}

@article{shimizu_lingam:_2014,
	title = {{LiNGAM}: {Non}-{Gaussian} methods for estimating causal structures},
	volume = {41},
	shorttitle = {{LiNGAM}},
	number = {1},
	journal = {Behaviormetrika},
	author = {Shimizu, Shohei},
	year = {2014},
	pages = {65--98},
}

@article{cole_performance_2017,
	title = {Performance {Assessment} and {Selection} of {Normalization} {Procedures} for {Single}-{Cell} {RNA}-{Seq}},
	copyright = {© 2017, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/early/2017/12/16/235382},
	doi = {10.1101/235382},
	abstract = {Due to the presence of systematic measurement biases, data normalization is an essential preprocessing step in the analysis of single-cell RNA sequencing (scRNA-seq) data. While a variety of normalization procedures are available for bulk RNA-seq, their suitability with respect to single-cell data is still largely unexplored. Furthermore, there may be multiple, competing considerations behind the assessment of normalization performance, some of them study-specific. The choice of normalization method can have a large impact on the results of downstream analyses (e.g., clustering, inference of cell lineages, differential expression analysis), and thus it is critically important to assess the performance of competing methods in order to select a suitable procedure for the study at hand. We have developed scone - a framework that implements a wide range of normalization procedures in the context of scRNA-seq, and enables the assessment of their performance based on a comprehensive set of data-driven performance metrics. The accompanying open-source Bioconductor R software package scone (available at https://bioconductor.org/packages/scone) also provides numerical and graphical summaries of expression measures, data quality assessment, and data-adaptive gene and sample filtering criteria. We demonstrate the effectiveness of scone on a selection of scRNA-seq datasets across a variety of protocols, ranging from plate- to droplet-based methods. We show that scone is able to correctly rank normalization methods according to their performance in a given dataset and that selecting the best performing normalization leads to higher agreement with independent validation data than lowly-ranked methods.},
	language = {en},
	urldate = {2018-01-03},
	journal = {bioRxiv},
	author = {Cole, Michael B. and Risso, Davide and Wagner, Allon and DeTomaso, David and Ngai, John and Purdom, Elizabeth and Dudoit, Sandrine and Yosef, Nir},
	month = dec,
	year = {2017},
	pages = {235382},
}

@article{peters_causal_2014,
	title = {Causal {Discovery} with {Continuous} {Additive} {Noise} {Models}},
	volume = {15},
	journal = {Journal of Machine Learning Research},
	author = {Peters, Jonas and Mooij, Joris M. and Janzing, Dominik and Schölkopf, Bernhard},
	year = {2014},
	pages = {2009--2053},
}

@article{bareinboim_causal_2016,
	title = {Causal inference and the data-fusion problem},
	volume = {113},
	number = {27},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bareinboim, Elias and Pearl, Judea},
	year = {2016},
	pages = {7345--7352},
}

@techreport{athey_efficient_2016,
	title = {Efficient inference of average treatment effects in high dimensions via approximate residual balancing},
	author = {Athey, Susan and Imbens, Guido W. and Wager, Stefan},
	year = {2016},
}

@article{statnikov_new_2012,
	title = {New methods for separating causes from effects in genomics data},
	volume = {13},
	number = {8},
	journal = {BMC genomics},
	author = {Statnikov, Alexander and Henaff, Mikael and Lytkin, Nikita I. and Aliferis, Constantin F.},
	year = {2012},
	pages = {S22},
}

@article{hill_challenges_2011,
	title = {Challenges with propensity score strategies in a high-dimensional setting and a potential alternative},
	volume = {46},
	number = {3},
	journal = {Multivariate Behavioral Research},
	author = {Hill, Jennifer and Weiss, Christopher and Zhai, Fuhua},
	year = {2011},
	pages = {477--513},
}

@article{zigler_estimating_2012,
	title = {Estimating causal effects of air quality regulations using principal stratification for spatially correlated multivariate intermediate outcomes},
	volume = {13},
	number = {2},
	journal = {Biostatistics},
	author = {Zigler, Corwin M. and Dominici, Francesca and Wang, Yun},
	year = {2012},
	pages = {289--302},
}

@article{grimmer_estimating_2017,
	title = {Estimating heterogeneous treatment effects and the effects of heterogeneous treatments with ensemble methods},
	volume = {25},
	number = {4},
	journal = {Political Analysis},
	author = {Grimmer, Justin and Messing, Solomon and Westwood, Sean J.},
	year = {2017},
	pages = {413--434},
}

@article{athey_recursive_2016,
	title = {Recursive partitioning for heterogeneous causal effects},
	volume = {113},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/113/27/7353},
	doi = {10.1073/pnas.1510489113},
	abstract = {In this paper we propose methods for estimating heterogeneity in causal effects in experimental and observational studies and for conducting hypothesis tests about the magnitude of differences in treatment effects across subsets of the population. We provide a data-driven approach to partition the data into subpopulations that differ in the magnitude of their treatment effects. The approach enables the construction of valid confidence intervals for treatment effects, even with many covariates relative to the sample size, and without “sparsity” assumptions. We propose an “honest” approach to estimation, whereby one sample is used to construct the partition and another to estimate treatment effects for each subpopulation. Our approach builds on regression tree methods, modified to optimize for goodness of fit in treatment effects and to account for honest estimation. Our model selection criterion anticipates that bias will be eliminated by honest estimation and also accounts for the effect of making additional splits on the variance of treatment effect estimates within each subpopulation. We address the challenge that the “ground truth” for a causal effect is not observed for any individual unit, so that standard approaches to cross-validation must be modified. Through a simulation study, we show that for our preferred method honest estimation results in nominal coverage for 90\% confidence intervals, whereas coverage ranges between 74\% and 84\% for nonhonest approaches. Honest estimation requires estimating the model with a smaller sample size; the cost in terms of mean squared error of treatment effects for our preferred method ranges between 7–22\%.},
	language = {en},
	number = {27},
	urldate = {2018-01-03},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Athey, Susan and Imbens, Guido},
	month = jul,
	year = {2016},
	pmid = {27382149},
	keywords = {causal inference, cross-validation, heterogeneous treatment effects, potential outcomes, supervised machine learning},
	pages = {7353--7360},
}

@article{athey_machine_2015,
	title = {Machine learning methods for estimating heterogeneous causal effects},
	volume = {1050},
	number = {5},
	journal = {stat},
	author = {Athey, Susan and Imbens, Guido W.},
	year = {2015},
}

@article{wager_estimation_2015,
	title = {Estimation and {Inference} of {Heterogeneous} {Treatment} {Effects} using {Random} {Forests}},
	url = {http://arxiv.org/abs/1510.04342},
	abstract = {Many scientific and engineering challenges -- ranging from personalized medicine to customized marketing recommendations -- require an understanding of treatment effect heterogeneity. In this paper, we develop a non-parametric causal forest for estimating heterogeneous treatment effects that extends Breiman's widely used random forest algorithm. In the potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect, and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms. To our knowledge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially in the presence of irrelevant covariates.},
	urldate = {2018-01-03},
	journal = {arXiv:1510.04342 [math, stat]},
	author = {Wager, Stefan and Athey, Susan},
	month = oct,
	year = {2015},
	note = {arXiv: 1510.04342},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Methodology},
}

@article{rotmensch_learning_2017,
	title = {Learning a {Health} {Knowledge} {Graph} from {Electronic} {Medical} {Records}},
	volume = {7},
	copyright = {2017 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-017-05778-z},
	doi = {10.1038/s41598-017-05778-z},
	abstract = {Demand for clinical decision support systems in medicine and self-diagnostic symptom checkers has substantially increased in recent years. Existing platforms rely on knowledge bases manually compiled through a labor-intensive process or automatically derived using simple pairwise statistics. This study explored an automated process to learn high quality knowledge bases linking diseases and symptoms directly from electronic medical records. Medical concepts were extracted from 273,174 de-identified patient records and maximum likelihood estimation of three probabilistic models was used to automatically construct knowledge graphs: logistic regression, naive Bayes classifier and a Bayesian network using noisy OR gates. A graph of disease-symptom relationships was elicited from the learned parameters and the constructed knowledge graphs were evaluated and validated, with permission, against Google’s manually-constructed knowledge graph and against expert physician opinions. Our study shows that direct and automated construction of high quality health knowledge graphs from medical records using rudimentary concept extraction is feasible. The noisy OR model produces a high quality knowledge graph reaching precision of 0.85 for a recall of 0.6 in the clinical evaluation. Noisy OR significantly outperforms all tested models across evaluation frameworks (p {\textless} 0.01).},
	language = {En},
	number = {1},
	urldate = {2018-01-03},
	journal = {Scientific Reports},
	author = {Rotmensch, Maya and Halpern, Yoni and Tlimat, Abdulhakim and Horng, Steven and Sontag, David},
	month = jul,
	year = {2017},
	pages = {5994},
}

@article{saxe_complex_2016,
	title = {A complex systems approach to causal discovery in psychiatry},
	volume = {11},
	number = {3},
	journal = {PloS one},
	author = {Saxe, Glenn N. and Statnikov, Alexander and Fenyo, David and Ren, Jiwen and Li, Zhiguo and Prasad, Meera and Wall, Dennis and Bergman, Nora and Briggs, Ernestine C. and Aliferis, Constantin},
	year = {2016},
	pages = {e0151174},
}

@article{janzinga_information-geometric_2012,
	title = {Information-geometric approach to inferring causal directions},
	volume = {182},
	number = {183},
	journal = {Artificial Intelligence},
	author = {Janzinga, Dominik and Mooij, Joris and Zhang, Kun and Lemeirec, Jan and Zscheischler, Jakob and Daniušis, Povilas and Steudelf, Bastian and Schölkopf, Bernhard},
	year = {2012},
	pages = {1--31},
}

@article{janzing_information-geometric_2012,
	title = {Information-geometric approach to inferring causal directions},
	volume = {182},
	journal = {Artificial Intelligence},
	author = {Janzing, Dominik and Mooij, Joris and Zhang, Kun and Lemeire, Jan and Zscheischler, Jakob and Daniušis, Povilas and Steudel, Bastian and Schölkopf, Bernhard},
	year = {2012},
	pages = {1--31},
}

@article{matteson_independent_2013,
	title = {Independent component analysis via distance covariance},
	journal = {Journal of the American Statistical Association},
	author = {Matteson, David S. and Tsay, Ruey S.},
	year = {2013},
	pages = {1--16},
}

@article{rubin_for_2008,
	title = {For objective causal inference, design trumps analysis},
	journal = {The Annals of Applied Statistics},
	author = {Rubin, Donald B.},
	year = {2008},
	pages = {808--840},
}

@article{rubin_for_2008-1,
	title = {For objective causal inference, design trumps analysis},
	journal = {The Annals of Applied Statistics},
	author = {Rubin, Donald B.},
	year = {2008},
	pages = {808--840},
}

@inproceedings{mooij_regression_2009,
	title = {Regression by dependence minimization and its application to causal inference in additive noise models},
	booktitle = {Proceedings of the 26th annual international conference on machine learning},
	publisher = {ACM},
	author = {Mooij, Joris and Janzing, Dominik and Peters, Jonas and Schölkopf, Bernhard},
	year = {2009},
	keywords = {Additive noise model, HSIC},
	pages = {745--752},
}

@article{pfister_kernel-based_2016,
	title = {Kernel-based tests for joint independence},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Pfister, Niklas and Bühlmann, Peter and Schölkopf, Bernhard and Peters, Jonas},
	year = {2016},
}

@article{szekely_measuring_2007,
	title = {Measuring and testing dependence by correlation of distances},
	volume = {35},
	number = {6},
	journal = {The Annals of Statistics},
	author = {SZÉKELY, GÁBOR J. and RIZZO, MARIA L. and BAKIROV, NAIL K.},
	year = {2007},
	pages = {2769--2794},
}

@article{szekely_measuring_2007-1,
	title = {Measuring and testing dependence by correlation of distances},
	volume = {35},
	number = {6},
	journal = {The annals of statistics},
	author = {Székely, Gábor J. and Rizzo, Maria L. and Bakirov, Nail K.},
	year = {2007},
	pages = {2769--2794},
}

@article{gretton_measuring_2005,
	title = {Measuring {Statistical} {Dependence} with {Hilbert}-{Schmidt} {Norms}},
	author = {Gretton, Arthur and Bousquet, Olivier and Smola, Alexander and Scholkopf, Bernhard},
	year = {2005},
	keywords = {HSIC},
}

@inproceedings{fukumizu_kernel_2008,
	title = {Kernel measures of conditional dependence},
	booktitle = {Advances in neural information processing systems},
	author = {Fukumizu, Kenji and Gretton, Arthur and Sun, Xiaohai and Schölkopf, Bernhard},
	year = {2008},
	keywords = {HSIC},
	pages = {489--496},
}

@inproceedings{sun_kernel-based_2007,
	title = {A kernel-based causal learning algorithm},
	booktitle = {Proceedings of the 24th international conference on {Machine} learning},
	publisher = {ACM},
	author = {Sun, Xiaohai and Janzing, Dominik and Schölkopf, Bernhard and Fukumizu, Kenji},
	year = {2007},
	pages = {855--862},
}

@book{rasmussen_gaussian_2006,
	title = {Gaussian processes for machine learning},
	volume = {1},
	publisher = {MIT press Cambridge},
	author = {Rasmussen, Carl Edward and Williams, Christopher KI},
	year = {2006},
}

@article{gretton_kernel_2005,
	title = {Kernel methods for measuring independence},
	volume = {6},
	number = {Dec},
	journal = {Journal of Machine Learning Research},
	author = {Gretton, Arthur and Herbrich, Ralf and Smola, Alexander and Bousquet, Olivier and Schölkopf, Bernhard},
	year = {2005},
	pages = {2075--2129},
}

@article{rada-iglesias_is_2018,
	title = {Is {H3K4me1} at enhancers correlative or causative?},
	volume = {50},
	number = {1},
	journal = {Nature Genetics},
	author = {Rada-Iglesias, Alvaro},
	year = {2018},
	pages = {4},
}

@article{purnell_cell_2017,
	title = {Cell mechanics indicate cell fate},
	volume = {358},
	copyright = {Copyright © 2017 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuse},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/358/6370/1552.3},
	doi = {10.1126/science.358.6370.1552-c},
	abstract = {Cell Reprogramming
Gene expression changes are accompanied by biophysical phenotypes during differentiation or reprogramming, as has now been shown by measurements of cell stiffness or relative compliancy. Using real-time deformability cytometry (a microfluidic-based method that deforms cells by},
	language = {en},
	number = {6370},
	urldate = {2017-12-26},
	journal = {Science},
	author = {Purnell, Beverly A.},
	month = dec,
	year = {2017},
	pages = {1552--1553},
}

@inproceedings{sun_distinguishing_2007,
	title = {Distinguishing between cause and effect via kernel-based complexity measures for conditional distributions.},
	booktitle = {{ESANN}},
	author = {Sun, Xiaohai and Janzing, Dominik and Schölkopf, Bernhard},
	year = {2007},
	pages = {441--446},
}

@article{silva_learning_2006,
	title = {Learning the structure of linear latent variable models},
	volume = {7},
	number = {Feb},
	journal = {Journal of Machine Learning Research},
	author = {Silva, Ricardo and Scheine, Richard and Glymour, Clark and Spirtes, Peter},
	year = {2006},
	pages = {191--246},
}

@inproceedings{gretton_nonlinear_2009,
	title = {Nonlinear directed acyclic structure learning with weakly additive noise models},
	booktitle = {Advances in neural information processing systems},
	author = {Gretton, Arthur and Spirtes, Peter and Tillman, Robert E.},
	year = {2009},
	keywords = {Additive noise model, HSIC, kPC},
	pages = {1847--1855},
}

@article{schurch_how_2016,
	title = {How many biological replicates are needed in an {RNA}-seq experiment and which differential expression tool should you use?},
	volume = {22},
	issn = {1355-8382, 1469-9001},
	url = {http://rnajournal.cshlp.org/content/22/6/839},
	doi = {10.1261/rna.053959.115},
	abstract = {RNA-seq is now the technology of choice for genome-wide differential gene expression experiments, but it is not clear how many biological replicates are needed to ensure valid biological interpretation of the results or which statistical tools are best for analyzing the data. An RNA-seq experiment with 48 biological replicates in each of two conditions was performed to answer these questions and provide guidelines for experimental design. With three biological replicates, nine of the 11 tools evaluated found only 20\%–40\% of the significantly differentially expressed (SDE) genes identified with the full set of 42 clean replicates. This rises to {\textgreater}85\% for the subset of SDE genes changing in expression by more than fourfold. To achieve {\textgreater}85\% for all SDE genes regardless of fold change requires more than 20 biological replicates. The same nine tools successfully control their false discovery rate at ≲5\% for all numbers of replicates, while the remaining two tools fail to control their FDR adequately, particularly for low numbers of replicates. For future RNA-seq experiments, these results suggest that at least six biological replicates should be used, rising to at least 12 when it is important to identify SDE genes for all fold changes. If fewer than 12 replicates are used, a superior combination of true positive and false positive performances makes edgeR and DESeq2 the leading tools. For higher replicate numbers, minimizing false positives is more important and DESeq marginally outperforms the other tools.},
	language = {en},
	number = {6},
	urldate = {2017-12-22},
	journal = {RNA},
	author = {Schurch, Nicholas J. and Schofield, Pietá and Gierliński, Marek and Cole, Christian and Sherstnev, Alexander and Singh, Vijender and Wrobel, Nicola and Gharbi, Karim and Simpson, Gordon G. and Owen-Hughes, Tom and Blaxter, Mark and Barton, Geoffrey J.},
	month = jun,
	year = {2016},
	pmid = {27022035},
	keywords = {RNA-seq, benchmarking, differential expression, experimental design, replication, statistical power, yeast},
	pages = {839--851},
}

@article{guo_discrepancy_2017,
	title = {The discrepancy among single nucleotide variants detected by {DNA} and {RNA} high throughput sequencing data},
	volume = {18},
	issn = {1471-2164},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5629567/},
	doi = {10.1186/s12864-017-4022-x},
	abstract = {Background
High throughput sequencing technology enables the both the human genome and transcriptome to be screened at the single nucleotide resolution. Tools have been developed to infer single nucleotide variants (SNVs) from both DNA and RNA sequencing data. To evaluate how much difference can be expected between DNA and RNA sequencing data, and among tissue sources, we designed a study to examine the single nucleotide difference among five sources of high throughput sequencing data generated from the same individual, including exome sequencing from blood, tumor and adjacent normal tissue, and RNAseq from tumor and adjacent normal tissue.

Results
Through careful quality control and analysis of the SNVs, we found little difference between DNA-DNA pairs (1\%–2\%). However, between DNA-RNA pairs, SNV differences ranged anywhere from 10\% to 20\%.

Conclusions
Only a small portion of these differences can be explained by RNA editing. Instead, the majority of the DNA-RNA differences should be attributed to technical errors from sequencing and post-processing of RNAseq data. Our analysis results suggest that SNV detection using RNAseq is subject to high false positive rates.

Electronic supplementary material
The online version of this article (doi:10.1186/s12864-017-4022-x) contains supplementary material, which is available to authorized users.},
	number = {Suppl 6},
	urldate = {2017-12-22},
	journal = {BMC Genomics},
	author = {Guo, Yan and Zhao, Shilin and Sheng, Quanhu and Samuels, David C and Shyr, Yu},
	month = oct,
	year = {2017},
	pmid = {28984205},
	pmcid = {PMC5629567},
}

@article{consortium_genetic_2017,
	title = {Genetic effects on gene expression across human tissues},
	volume = {550},
	copyright = {2017 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature24277},
	doi = {10.1038/nature24277},
	abstract = {{\textless}p{\textgreater}Samples of different body regions from hundreds of human donors are used to study how genetic variation influences gene expression levels in 44 disease-relevant tissues.{\textless}/p{\textgreater}},
	language = {En},
	number = {7675},
	urldate = {2017-12-22},
	journal = {Nature},
	author = {Consortium, GTEx},
	month = oct,
	year = {2017},
	pages = {204},
}

@inproceedings{hoyer_nonlinear_2009,
	title = {Nonlinear causal discovery with additive noise models},
	booktitle = {Advances in neural information processing systems},
	author = {Hoyer, Patrik O. and Janzing, Dominik and Mooij, Joris M. and Peters, Jonas and Schölkopf, Bernhard},
	year = {2009},
	keywords = {Additive noise model},
	pages = {689--696},
}

@article{zhang_kernel-based_2012,
	title = {Kernel-based {Conditional} {Independence} {Test} and {Application} in {Causal} {Discovery}},
	url = {http://arxiv.org/abs/1202.3775},
	abstract = {Conditional independence testing is an important problem, especially in Bayesian network learning and causal discovery. Due to the curse of dimensionality, testing for conditional independence of continuous variables is particularly challenging. We propose a Kernel-based Conditional Independence test (KCI-test), by constructing an appropriate test statistic and deriving its asymptotic distribution under the null hypothesis of conditional independence. The proposed method is computationally efficient and easy to implement. Experimental results show that it outperforms other methods, especially when the conditioning set is large or the sample size is not very large, in which case other methods encounter difficulties.},
	urldate = {2017-12-21},
	journal = {arXiv:1202.3775 [cs, stat]},
	author = {Zhang, Kun and Peters, Jonas and Janzing, Dominik and Schoelkopf, Bernhard},
	month = feb,
	year = {2012},
	note = {arXiv: 1202.3775},
	keywords = {Computer Science - Learning, HSIC, Statistics - Machine Learning, kCI, kPC},
}

@book{gelman_bayesian_2014,
	title = {Bayesian data analysis},
	volume = {2},
	publisher = {CRC press Boca Raton, FL},
	author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
	year = {2014},
}

@article{frangakis_principal_2002,
	title = {Principal stratification in causal inference},
	volume = {58},
	number = {1},
	journal = {Biometrics},
	author = {Frangakis, Constantine E. and Rubin, Donald B.},
	year = {2002},
	pages = {21--29},
}

@incollection{heckerman_bayesian_2006,
	title = {A {Bayesian} {Approach} to {Causal} {Discovery}},
	isbn = {978-3-540-30609-2 978-3-540-33486-6},
	url = {https://link.springer.com/chapter/10.1007/3-540-33486-6_1},
	abstract = {We examine the Bayesian approach to the discovery of causal DAG models and compare it to the constraint-based approach. Both approaches rely on the Causal Markov condition, but the two differ significantly in theory and practice. An important difference between the approaches is that the constraint-based approach uses categorical information about conditional-independence constraints in the domain, whereas the Bayesian approach weighs the degree to which such constraints hold. As a result, the Bayesian approach has three distinct advantages over its constraint-based counterpart. One, conclusions derived from the Bayesian approach are not susceptible to incorrect categorical decisions about independence facts that can occur with data sets of finite size. Two, using the Bayesian approach, finer distinctions among model structures—both quantitative and qualitative—can be made. Three, information from several models can be combined to make better inferences and to better account for modeling uncertainty. In addition to describing the general Bayesian approach to causal discovery, we review approximation methods for missing data and hidden variables, and illustrate differences between the Bayesian and constraint-based methods using artificial and real examples.},
	language = {en},
	urldate = {2017-12-19},
	booktitle = {Innovations in {Machine} {Learning}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Heckerman, David and Meek, Christopher and Cooper, Gregory},
	year = {2006},
	doi = {10.1007/3-540-33486-6_1},
	pages = {1--28},
}

@book{peters_elements_2017,
	title = {Elements of causal inference: foundations and learning algorithms},
	volume = {12},
	shorttitle = {Elements of causal inference},
	author = {Peters, Jonas and Janzing, Dominik and Schölkopf, Bernhard},
	year = {2017},
}

@article{friedman_using_2000,
	title = {Using {Bayesian} networks to analyze expression data},
	volume = {7},
	number = {3-4},
	journal = {Journal of computational biology},
	author = {Friedman, Nir and Linial, Michal and Nachman, Iftach and Pe'er, Dana},
	year = {2000},
	pages = {601--620},
}

@book{pearl_causality_2009,
	title = {Causality},
	publisher = {Cambridge university press},
	author = {Pearl, Judea},
	year = {2009},
}

@article{colombo_learning_2012,
	title = {Learning high-dimensional directed acyclic graphs with latent and selection variables},
	journal = {The Annals of Statistics},
	author = {Colombo, Diego and Maathuis, Marloes H. and Kalisch, Markus and Richardson, Thomas S.},
	year = {2012},
	keywords = {RFCI},
	pages = {294--321},
}

@article{blobaum_novel_2017,
	title = {A {Novel} {Principle} for {Causal} {Inference} in {Data} with {Small} {Error} {Variance}},
	journal = {ESANN  2017  proceedings},
	author = {Blöbaum, Patrick and Shimizu, Shohei and Washio, Takashi},
	year = {2017},
}

@article{tashiro_estimation_2012,
	title = {Estimation of causal orders in a linear non-{Gaussian} acyclic model: a method robust against latent confounders},
	shorttitle = {Estimation of causal orders in a linear non-{Gaussian} acyclic model},
	journal = {Artificial Neural Networks and Machine Learning–ICANN 2012},
	author = {Tashiro, Tatsuya and Shimizu, Shohei and Hyvärinen, Aapo and Washio, Takashi},
	year = {2012},
	keywords = {LINGAM},
	pages = {491--498},
}

@article{shimizu_linear_2006,
	title = {A linear non-{Gaussian} acyclic model for causal discovery},
	volume = {7},
	number = {Oct},
	journal = {Journal of Machine Learning Research},
	author = {Shimizu, Shohei and Hoyer, Patrik O. and Hyvärinen, Aapo and Kerminen, Antti},
	year = {2006},
	keywords = {LINGAM},
	pages = {2003--2030},
}

@article{meinshausen_methods_2016,
	title = {Methods for causal inference from gene perturbation experiments and validation},
	volume = {113},
	number = {27},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Meinshausen, Nicolai and Hauser, Alain and Mooij, Joris M. and Peters, Jonas and Versteeg, Philip and Bühlmann, Peter},
	year = {2016},
	pages = {7361--7368},
}

@article{taylor_statistical_2015,
	title = {Statistical learning and selective inference},
	volume = {112},
	number = {25},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Taylor, Jonathan and Tibshirani, Robert J.},
	year = {2015},
	keywords = {selection},
	pages = {7629--7634},
}

@article{peters_causal_2016,
	title = {Causal inference by using invariant prediction: identification and confidence intervals},
	volume = {78},
	shorttitle = {Causal inference by using invariant prediction},
	number = {5},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Peters, Jonas and Bühlmann, Peter and Meinshausen, Nicolai},
	year = {2016},
	pages = {947--1012},
}

@inproceedings{meek_causal_1995,
	title = {Causal inference and causal explanation with background knowledge},
	booktitle = {Proceedings of the {Eleventh} conference on {Uncertainty} in artificial intelligence},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Meek, Christopher},
	year = {1995},
	pages = {403--410},
}

@inproceedings{lopez-paz_towards_2015,
	title = {Towards a learning theory of cause-effect inference},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Lopez-Paz, David and Muandet, Krikamol and Schölkopf, Bernhard and Tolstikhin, Iliya},
	year = {2015},
	pages = {1452--1461},
}

@article{triantafillou_constraint-based_2015,
	title = {Constraint-based causal discovery from multiple interventions over overlapping variable sets.},
	volume = {16},
	journal = {Journal of Machine Learning Research},
	author = {Triantafillou, Sofia and Tsamardinos, Ioannis},
	year = {2015},
	pages = {2147--2205},
}

@article{shimizu_directlingam:_2011,
	title = {{DirectLiNGAM}: {A} direct method for learning a linear non-{Gaussian} structural equation model},
	volume = {12},
	shorttitle = {{DirectLiNGAM}},
	number = {Apr},
	journal = {Journal of Machine Learning Research},
	author = {Shimizu, Shohei and Inazumi, Takanori and Sogawa, Yasuhiro and Hyvärinen, Aapo and Kawahara, Yoshinobu and Washio, Takashi and Hoyer, Patrik O. and Bollen, Kenneth},
	year = {2011},
	keywords = {LINGAM},
	pages = {1225--1248},
}

@article{harris_pc_2013,
	title = {{PC} {Algorithm} for {Nonparanormal} {Graphical} {Models}},
	volume = {14},
	url = {http://www.jmlr.org/papers/v14/harris13a.html},
	urldate = {2017-12-18},
	journal = {Journal of Machine Learning Research},
	author = {Harris, Naftali and Drton, Mathias},
	year = {2013},
	pages = {3365--3383},
}

@inproceedings{scheines_introduction_1997,
	title = {An {Introduction} to {Causal} {Inference}},
	abstract = {developed a theory of statistical causal inference. In his presentation at the Notre Dame},
	booktitle = {Causality in {Crisis}? {University} of {Notre} {Dame}},
	publisher = {Press},
	author = {Scheines, Richard},
	year = {1997},
	pages = {185--200},
}

@inproceedings{inazumi_use_2010,
	title = {Use of {Prior} {Knowledge} in a {Non}-{Gaussian} {Method} for {Learning} {Linear} {Structural} {Equation} {Models}},
	isbn = {978-3-642-15994-7 978-3-642-15995-4},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-15995-4_28},
	doi = {10.1007/978-3-642-15995-4_28},
	abstract = {We discuss causal structure learning based on linear structural equation models. Conventional learning methods most often assume Gaussianity and create many indistinguishable models. Therefore, in many cases it is difficult to obtain much information on the structure. Recently, a non-Gaussian learning method called LiNGAM has been proposed to identify the model structure without using prior knowledge on the structure. However, more efficient learning can be achieved if some prior knowledge on a part of the structure is available. In this paper, we propose to use prior knowledge to improve the performance of a state-of-art non-Gaussian method. Experiments on artificial data show that the accuracy and computational time are significantly improved even if the amount of prior knowledge is not so large.},
	language = {en},
	urldate = {2017-12-18},
	booktitle = {Latent {Variable} {Analysis} and {Signal} {Separation}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Inazumi, Takanori and Shimizu, Shohei and Washio, Takashi},
	month = sep,
	year = {2010},
	keywords = {LINGAM},
	pages = {221--228},
}

@article{heckerman_learning_1995,
	title = {Learning {Bayesian} networks: {The} combination of knowledge and statistical data},
	volume = {20},
	issn = {0885-6125, 1573-0565},
	shorttitle = {Learning {Bayesian} networks},
	url = {https://link.springer.com/article/10.1007/BF00994016},
	doi = {10.1007/BF00994016},
	abstract = {We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption oflikelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user's priors for network parameters can be encoded in a single Bayesian network for the next case to be seen—aprior network—and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at mostk=1 parent. For the general case (k{\textgreater}1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches.},
	language = {en},
	number = {3},
	urldate = {2017-12-18},
	journal = {Machine Learning},
	author = {Heckerman, David and Geiger, Dan and Chickering, David M.},
	month = sep,
	year = {1995},
	pages = {197--243},
}

@article{heinze-deml_causal_2018,
	title = {Causal {Structure} {Learning}},
	volume = {5},
	url = {https://doi.org/10.1146/annurev-statistics-031017-100630},
	doi = {10.1146/annurev-statistics-031017-100630},
	abstract = {Graphical models can represent a multivariate distribution in a convenient and accessible form as a graph. Causal models can be viewed as a special class of graphical models that represent not only the distribution of the observed system but also the distributions under external interventions. They hence enable predictions under hypothetical interventions, which is important for decision making. The challenging task of learning causal models from data always relies on some underlying assumptions. We discuss several recently proposed structure learning algorithms and their assumptions, and we compare their empirical performance under various scenario Expected final online publication date for the Annual Review of Statistics and Its Application Volume 5 is March 7, 2018. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.},
	number = {1},
	urldate = {2017-12-18},
	journal = {Annual Review of Statistics and Its Application},
	author = {Heinze-Deml, Christina and Maathuis, Marloes H. and Meinshausen, Nicolai},
	year = {2018},
	pages = {null},
}

@article{hyttinen_discovering_2013,
	title = {Discovering {Cyclic} {Causal} {Models} with {Latent} {Variables}: {A} {General} {SAT}-{Based} {Procedure}},
	shorttitle = {Discovering {Cyclic} {Causal} {Models} with {Latent} {Variables}},
	url = {http://arxiv.org/abs/1309.6836},
	abstract = {We present a very general approach to learning the structure of causal models based on d-separation constraints, obtained from any given set of overlapping passive observational or experimental data sets. The procedure allows for both directed cycles (feedback loops) and the presence of latent variables. Our approach is based on a logical representation of causal pathways, which permits the integration of quite general background knowledge, and inference is performed using a Boolean satisfiability (SAT) solver. The procedure is complete in that it exhausts the available information on whether any given edge can be determined to be present or absent, and returns "unknown" otherwise. Many existing constraint-based causal discovery algorithms can be seen as special cases, tailored to circumstances in which one or more restricting assumptions apply. Simulations illustrate the effect of these assumptions on discovery and how the present algorithm scales.},
	urldate = {2017-12-18},
	journal = {arXiv:1309.6836 [cs]},
	author = {Hyttinen, Antti and Hoyer, Patrik O. and Eberhardt, Frederick and Jarvisalo, Matti},
	month = sep,
	year = {2013},
	note = {arXiv: 1309.6836},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{frot_learning_2017,
	title = {Learning {Directed} {Acyclic} {Graphs} with {Hidden} {Variables} via {Latent} {Gaussian} {Graphical} {Model} {Selection}},
	url = {http://arxiv.org/abs/1708.01151},
	abstract = {We introduce a new method to estimate the Markov equivalence class of a directed acyclic graph (DAG) in the presence of hidden variables, in settings where the underlying DAG among the observed variables is sparse, and there are a few hidden variables that have a direct effect on many of the observed ones. Building on the so-called low rank plus sparse framework, we suggest a two-stage approach which first removes unwanted variation using latent Gaussian graphical model selection, and then estimates the Markov equivalence class of the underlying DAG by applying GES. This approach is consistent in certain high-dimensional regimes and performs favourably when compared to the state of the art, both in terms of graphical structure recovery and total causal effect estimation.},
	urldate = {2017-12-18},
	journal = {arXiv:1708.01151 [stat]},
	author = {Frot, Benjamin and Nandy, Preetam and Maathuis, Marloes H.},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.01151},
	keywords = {Statistics - Methodology},
}

@article{drton_structure_2017,
	title = {Structure {Learning} in {Graphical} {Modeling}},
	volume = {4},
	url = {https://doi.org/10.1146/annurev-statistics-060116-053803},
	doi = {10.1146/annurev-statistics-060116-053803},
	abstract = {A graphical model is a statistical model that is associated with a graph whose nodes correspond to variables of interest. The edges of the graph reflect allowed conditional dependencies among the variables. Graphical models have computationally convenient factorization properties and have long been a valuable tool for tractable modeling of multivariate distributions. More recently, applications such as reconstructing gene regulatory networks from gene expression data have driven major advances in structure learning, that is, estimating the graph underlying a model. We review some of these advances and discuss methods such as the graphical lasso and neighborhood selection for undirected graphical models (or Markov random fields) and the PC algorithm and score-based search methods for directed graphical models (or Bayesian networks). We further review extensions that account for effects of latent variables and heterogeneous data sources.},
	number = {1},
	urldate = {2017-12-18},
	journal = {Annual Review of Statistics and Its Application},
	author = {Drton, Mathias and Maathuis, Marloes H.},
	year = {2017},
	pages = {365--393},
}

@article{malec_literature-based_2017,
	title = {Literature-{Based} {Discovery} of {Confounding} in {Observational} {Clinical} {Data}},
	volume = {2016},
	issn = {1942-597X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5333204/},
	abstract = {Observational data recorded in the Electronic Health Record (EHR) can help us better understand the effects of therapeutic agents in routine clinical practice. As such data were not collected for research purposes, their reuse for research must compensate for additional information that may bias analyses and lead to faulty conclusions. Confounding is present when factors aside from the given predictor(s) affect the response of interest. However, these additional factors may not be known at the outset. In this paper, we present a scalable literature-based confounding variable discovery method for biomedical research applications with pharmacovigilance as our use case. We hypothesized that statistical models, adjusted with literature-derived confounders, will more accurately identify causative drug-adverse drug event (ADE) relationships. We evaluated our method with a curated reference standard, and found a pattern of improved performance {\textasciitilde} 5\% in two out of three models for gastrointestinal bleeding (pre-adjusted Area Under Curve ≥ 0.6).},
	urldate = {2017-12-18},
	journal = {AMIA Annual Symposium Proceedings},
	author = {Malec, Scott A. and Wei, Peng and Xu, Hua and Bernstam, Elmer V. and Myneni, Sahiti and Cohen, Trevor},
	month = feb,
	year = {2017},
	pmid = {28269951},
	pmcid = {PMC5333204},
	pages = {1920--1929},
}

@article{nandy_estimating_2017,
	title = {Estimating the effect of joint interventions from observational data in sparse high-dimensional settings},
	volume = {45},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1494921953},
	doi = {10.1214/16-AOS1462},
	abstract = {We consider the estimation of joint causal effects from observational data. In particular, we propose new methods to estimate the effect of multiple simultaneous interventions (e.g., multiple gene knockouts), under the assumption that the observational data come from an unknown linear structural equation model with independent errors. We derive asymptotic variances of our estimators when the underlying causal structure is partly known, as well as high-dimensional consistency when the causal structure is fully unknown and the joint distribution is multivariate Gaussian. We also propose a generalization of our methodology to the class of nonparanormal distributions. We evaluate the estimators in simulation studies and also illustrate them on data from the DREAM4 challenge.},
	language = {EN},
	number = {2},
	urldate = {2017-12-18},
	journal = {The Annals of Statistics},
	author = {Nandy, Preetam and Maathuis, Marloes H. and Richardson, Thomas S.},
	month = apr,
	year = {2017},
	zmnumber = {06754746},
	keywords = {Causal inference, directed acyclic graph (DAG), high-dimensional data, joint causal effects, linear structural equation model (linear SEM), multiple simultaneous interventions, nonparanormal distribution},
	pages = {647--674},
}

@inproceedings{affeldt_robust_2015,
	title = {Robust reconstruction of causal graphical models based on conditional 2-point and 3-point information},
	booktitle = {Proceedings of the {UAI} 2015 {Conference} on {Advances} in {Causal} {Inference}-{Volume} 1504},
	publisher = {CEUR-WS. org},
	author = {Affeldt, Séverine and Isambert, Hervé},
	year = {2015},
	keywords = {team},
	pages = {1--29},
}

@article{verny_learning_2017,
	title = {Learning causal networks with latent variables from multivariate information in genomic data},
	volume = {13},
	number = {10},
	journal = {PLoS Computational Biology},
	author = {Verny, Louis and Sella, Nadir and Affeldt, Séverine and Singh, Param Priya and Isambert, Hervé},
	year = {2017},
	keywords = {team},
	pages = {e1005662},
}

@inproceedings{affeldt_3off2:_2016,
	title = {3off2: {A} network reconstruction algorithm based on 2-point and 3-point information statistics},
	volume = {17},
	booktitle = {{BMC} bioinformatics},
	publisher = {BioMed Central Ltd},
	author = {Affeldt, Séverine and Verny, Louis and Isambert, Hervé},
	year = {2016},
	keywords = {team},
	pages = {12},
}

@book{pearl_causal_2016,
	title = {Causal inference in statistics: a primer},
	publisher = {John Wiley \& Sons},
	author = {Pearl, Judea and Glymour, Madelyn and Jewell, Nicholas P},
	year = {2016},
}

@inproceedings{kpotufe_consistency_2014,
	title = {Consistency of causal inference under the additive noise model},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Kpotufe, Samory and Sgouritsa, Eleni and Janzing, Dominik and Schölkopf, Bernhard},
	year = {2014},
	pages = {478--486},
}

@article{buhlmann_cam:_2014,
	title = {{CAM}: {Causal} additive models, high-dimensional order search and penalized regression},
	volume = {42},
	issn = {0090-5364},
	shorttitle = {{CAM}},
	url = {http://arxiv.org/abs/1310.1533},
	doi = {10.1214/14-AOS1260},
	abstract = {We develop estimation for potentially high-dimensional additive structural equation models. A key component of our approach is to decouple order search among the variables from feature or edge selection in a directed acyclic graph encoding the causal structure. We show that the former can be done with nonregularized (restricted) maximum likelihood estimation while the latter can be efficiently addressed using sparse regression techniques. Thus, we substantially simplify the problem of structure search and estimation for an important class of causal models. We establish consistency of the (restricted) maximum likelihood estimator for low- and high-dimensional scenarios, and we also allow for misspecification of the error distribution. Furthermore, we develop an efficient computational algorithm which can deal with many variables, and the new method's accuracy and performance is illustrated on simulated and real data.},
	number = {6},
	urldate = {2018-02-06},
	journal = {The Annals of Statistics},
	author = {Bühlmann, Peter and Peters, Jonas and Ernest, Jan},
	month = dec,
	year = {2014},
	note = {arXiv: 1310.1533},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Statistics - Methodology},
	pages = {2526--2556},
}

@article{tikka_identifying_2017,
	title = {Identifying causal effects with the {R} package causaleffect},
	volume = {76},
	journal = {Journal of Statistical Software},
	author = {Tikka, Santtu and Karvanen, Juha},
	year = {2017},
}

@article{peters_causal_2016-1,
	title = {Causal inference by using invariant prediction: identification and confidence intervals},
	volume = {78},
	issn = {1467-9868},
	shorttitle = {Causal inference by using invariant prediction},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/rssb.12167/abstract},
	doi = {10.1111/rssb.12167},
	abstract = {What is the difference between a prediction that is made with a causal model and that with a non-causal model? Suppose that we intervene on the predictor variables or change the whole environment. The predictions from a causal model will in general work as well under interventions as for observational data. In contrast, predictions from a non-causal model can potentially be very wrong if we actively intervene on variables. Here, we propose to exploit this invariance of a prediction under a causal model for causal inference: given different experimental settings (e.g. various interventions) we collect all models that do show invariance in their predictive accuracy across settings and interventions. The causal model will be a member of this set of models with high probability. This approach yields valid confidence intervals for the causal relationships in quite general scenarios. We examine the example of structural equation models in more detail and provide sufficient assumptions under which the set of causal predictors becomes identifiable. We further investigate robustness properties of our approach under model misspecification and discuss possible extensions. The empirical properties are studied for various data sets, including large-scale gene perturbation experiments.},
	language = {en},
	number = {5},
	urldate = {2018-02-01},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Peters, Jonas and Bühlmann, Peter and Meinshausen, Nicolai},
	month = nov,
	year = {2016},
	keywords = {Causal discovery, Causal inference, Confidence intervals, Invariant prediction},
	pages = {947--1012},
}

@article{rocke_model_2001,
	title = {A {Model} for {Measurement} {Error} for {Gene} {Expression} {Arrays}},
	volume = {8},
	number = {6},
	journal = {JOURNAL OF COMPUTATIONAL BIOLOGY},
	author = {ROCKE, DAVID M. and DURBIN, BLYTHE},
	year = {2001},
}

@article{van_den_bulcke_syntren:_2006,
	title = {{SynTReN}: a generator of synthetic gene expression data for design and analysis of structure learning algorithms},
	volume = {7},
	issn = {1471-2105},
	shorttitle = {{SynTReN}},
	url = {https://doi.org/10.1186/1471-2105-7-43},
	doi = {10.1186/1471-2105-7-43},
	abstract = {The development of algorithms to infer the structure of gene regulatory networks based on expression data is an important subject in bioinformatics research. Validation of these algorithms requires benchmark data sets for which the underlying network is known. Since experimental data sets of the appropriate size and design are usually not available, there is a clear need to generate well-characterized synthetic data sets that allow thorough testing of learning algorithms in a fast and reproducible manner.},
	urldate = {2018-02-01},
	journal = {BMC Bioinformatics},
	author = {Van den Bulcke, Tim and Van Leemput, Koenraad and Naudts, Bart and van Remortel, Piet and Ma, Hongwu and Verschoren, Alain and De Moor, Bart and Marchal, Kathleen},
	month = jan,
	year = {2006},
	pages = {43},
}

@article{mendes_artificial_2003,
	title = {Artificial gene networks for objective comparison of analysis algorithms},
	volume = {19},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/19/suppl_2/ii122/180406},
	doi = {10.1093/bioinformatics/btg1069},
	abstract = {Motivation: Large-scale gene expression profiling generates data
  sets that are rich in observed features but poor in numbers of
  observations. The analysis of such data sets is a challenge that has
  been object of vigorous research. The algorithms in use for this
  purpose have been poorly documented and rarely compared objectively,
  posing a problem of uncertainty about the outcomes of the analyses.
  One way to objectively test such analysis algorithms is to apply
  them on computational gene network models for which the mechanisms
  are completely know.Results: We present a system that generates random artificial
  gene networks according to well-defined topological and kinetic
  properties. These are used to run in silico experiments simulating
  real laboratory microarray experiments. Noise with controlled
  properties is added to the simulation results several times
  emulating measurement replicates, before expression ratios are
  calculated.Availability: The data sets and kinetic models described here are
  available from http://www.vbi.vt.edu/{\textasciitilde}mendes/AGN/as
  biochemical dynamic models in SBML and Gepasi formats.Contact: mendes@vt.edu*To whom correspondence should be
  addressed.},
	language = {en},
	number = {suppl\_2},
	urldate = {2018-02-01},
	journal = {Bioinformatics},
	author = {Mendes, Pedro and Sha, Wei and Ye, Keying},
	month = sep,
	year = {2003},
	pages = {ii122--ii129},
}

@article{stovitz_paediatric_2018,
	title = {Paediatric obesity appears to lower the risk of diabetes if selection bias is ignored},
	copyright = {© Article author(s) (or their employer(s) unless otherwise stated in the text of the article) 2018. All rights reserved. No commercial use is permitted unless otherwise expressly granted.},
	issn = {0143-005X, 1470-2738},
	url = {http://jech.bmj.com/content/early/2018/01/25/jech-2017-209985},
	doi = {10.1136/jech-2017-209985},
	abstract = {Background Frustrated with the onslaught of articles reporting fascination with results that appear paradoxical but are merely due to selection bias, we studied the apparent effect of obesity on diabetes risk in youth who had a test for diabetes. We hypothesised that obese subjects would have lower rates of diabetes than non-obese subjects due to selection bias, and consequently, obesity would appear to lower the risk of diabetes.
Methods Retrospective cohort study of children (4–9 years), pre-teens (10–12 years) and teenagers (13–19 years). Participation was restricted to those who had a test of haemoglobin A1C along with measured height and weight. Body mass index percentile via the Centers for Disease Control and Prevention age and sex standards was calculated and categorised. The main outcome was A1C\%, subsequently categorised at the level for diagnosis of diabetes mellitus (≥6.5\%).
Results The sample consisted of 134 (2\%) underweight, 1718 (30\%) healthy weight, 660 (12\%) overweight and 3190 (56\%) obese individuals. 16\% (n=936) had an A1C≥6.5\%. Overall, healthy weight children had 8.2 times the risk of A1C≥6.5\% (95\% CI 5.3 to 12.7) compared with those in the obese category. The relative risk was 13 in pre-teens (95\% CI 8.5 to 20.0) and 3.9 in teenagers (95\% CI 3.3 to 4.7).
Conclusions Healthy weight was associated with a 4–13 times higher relative risk of diabetes mellitus compared with being obese. While apparently shocking, the study’s fatal flaw (selection bias) explains the ‘paradoxical’ finding. Ignoring selection bias can delay advances in medical science.},
	language = {en},
	urldate = {2018-01-31},
	journal = {J Epidemiol Community Health},
	author = {Stovitz, Steven D. and Banack, Hailey R. and Kaufman, Jay S.},
	month = jan,
	year = {2018},
	pmid = {29374028},
	keywords = {epidemiological methods, obesity, research design in epidemiology, study design},
	pages = {jech--2017--209985},
}

@article{janson_eigenprism:_2015,
	title = {{EigenPrism}: {Inference} for {High}-{Dimensional} {Signal}-to-{Noise} {Ratios}},
	shorttitle = {{EigenPrism}},
	url = {http://arxiv.org/abs/1505.02097},
	abstract = {Consider the following three important problems in statistical inference, namely, constructing confidence intervals for (1) the error of a high-dimensional (\$p{\textgreater}n\$) regression estimator, (2) the linear regression noise level, and (3) the genetic signal-to-noise ratio of a continuous-valued trait (related to the heritability). All three problems turn out to be closely related to the little-studied problem of performing inference on the \${\textbackslash}ell\_2\$-norm of the signal in high-dimensional linear regression. We derive a novel procedure for this, which is asymptotically correct when the covariates are multivariate Gaussian and produces valid confidence intervals in finite samples as well. The procedure, called EigenPrism, is computationally fast and makes no assumptions on coefficient sparsity or knowledge of the noise level. We investigate the width of the EigenPrism confidence intervals, including a comparison with a Bayesian setting in which our interval is just 5\% wider than the Bayes credible interval. We are then able to unify the three aforementioned problems by showing that the EigenPrism procedure with only minor modifications is able to make important contributions to all three. We also investigate the robustness of coverage and find that the method applies in practice and in finite samples much more widely than just the case of multivariate Gaussian covariates. Finally, we apply EigenPrism to a genetic dataset to estimate the genetic signal-to-noise ratio for a number of continuous phenotypes.},
	urldate = {2018-01-30},
	journal = {arXiv:1505.02097 [stat]},
	author = {Janson, Lucas and Barber, Rina Foygel and Candès, Emmanuel},
	month = may,
	year = {2015},
	note = {arXiv: 1505.02097},
	keywords = {Statistics - Methodology},
}

@article{ertefaie_variable_2015,
	title = {Variable {Selection} in {Causal} {Inference} using a {Simultaneous} {Penalization} {Method}},
	url = {http://arxiv.org/abs/1511.08501},
	abstract = {In the causal adjustment setting, variable selection techniques based on one of either the outcome or treatment allocation model can result in the omission of confounders, which leads to bias, or the inclusion of spurious variables, which leads to variance inflation, in the propensity score. We propose a variable selection method based on a penalized objective function which considers the outcome and treatment assignment models simultaneously. The proposed method facilitates confounder selection in high-dimensional settings. We show that under regularity conditions our method attains the oracle property. The selected variables are used to form a doubly robust regression estimator of the treatment effect. We show that under some conditions our method attains the oracle property. Simulation results are presented and economic growth data are analyzed. Specifically, we study the effect of life expectancy as a measure of population health on the average growth rate of gross domestic product per capita.},
	urldate = {2018-01-30},
	journal = {arXiv:1511.08501 [stat]},
	author = {Ertefaie, Ashkan and Asgharian, Masoud and Stephens, David},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.08501},
	keywords = {Statistics - Methodology},
}

@article{schmitt_compendium_2016,
	title = {A {Compendium} of {Chromatin} {Contact} {Maps} {Reveals} {Spatially} {Active} {Regions} in the {Human} {Genome}},
	volume = {17},
	issn = {2211-1247},
	url = {http://www.sciencedirect.com/science/article/pii/S2211124716314814},
	doi = {10.1016/j.celrep.2016.10.061},
	abstract = {Summary
The three-dimensional configuration of DNA is integral to all nuclear processes in eukaryotes, yet our knowledge of the chromosome architecture is still limited. Genome-wide chromosome conformation capture studies have uncovered features of chromatin organization in cultured cells, but genome architecture in human tissues has yet to be explored. Here, we report the most comprehensive survey to date of chromatin organization in human tissues. Through integrative analysis of chromatin contact maps in 21 primary human tissues and cell types, we find topologically associating domains highly conserved in different tissues. We also discover genomic regions that exhibit unusually high levels of local chromatin interactions. These frequently interacting regions (FIREs) are enriched for super-enhancers and are near tissue-specifically expressed genes. They display strong tissue-specificity in local chromatin interactions. Additionally, FIRE formation is partially dependent on CTCF and the Cohesin complex. We further show that FIREs can help annotate the function of non-coding sequence variants.},
	number = {8},
	urldate = {2018-01-22},
	journal = {Cell Reports},
	author = {Schmitt, Anthony D. and Hu, Ming and Jung, Inkyung and Xu, Zheng and Qiu, Yunjiang and Tan, Catherine L. and Li, Yun and Lin, Shin and Lin, Yiing and Barr, Cathy L. and Ren, Bing},
	month = nov,
	year = {2016},
	pages = {2042--2059},
}

@article{hodos_cell-specific_2018,
	title = {Cell-specific prediction and application of drug-induced gene expression profiles},
	volume = {23},
	issn = {2335-6936},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5753597/},
	abstract = {Gene expression profiling of in vitro drug perturbations is useful for many biomedical discovery applications including drug repurposing and elucidation of drug mechanisms. However, limited data availability across cell types has hindered our capacity to leverage or explore the cell-specificity of these perturbations. While recent efforts have generated a large number of drug perturbation profiles across a variety of human cell types, many gaps remain in this combinatorial drug-cell space. Hence, we asked whether it is possible to fill these gaps by predicting cell-specific drug perturbation profiles using available expression data from related conditions--i.e. from other drugs and cell types. We developed a computational framework that first arranges existing profiles into a three-dimensional array (or tensor) indexed by drugs, genes, and cell types, and then uses either local (nearest-neighbors) or global (tensor completion) information to predict unmeasured profiles. We evaluate prediction accuracy using a variety of metrics, and find that the two methods have complementary performance, each superior in different regions in the drug-cell space. Predictions achieve correlations of 0.68 with true values, and maintain accurate differentially expressed genes (AUC 0.81). Finally, we demonstrate that the predicted profiles add value for making downstream associations with drug targets and therapeutic classes.},
	urldate = {2018-01-22},
	journal = {Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing},
	author = {Hodos, Rachel and Zhang, Ping and Lee, Hao-Chih and Duan, Qiaonan and Wang, Zichen and Clark, Neil R. and Ma'ayan, Avi and Wang, Fei and Kidd, Brian and Hu, Jianying and Sontag, David and Dudley, Joel},
	year = {2018},
	pmid = {29218867},
	pmcid = {PMC5753597},
	pages = {32--43},
}

@article{keenan_library_2017,
	title = {The {Library} of {Integrated} {Network}-{Based} {Cellular} {Signatures} {NIH} {Program}: {System}-{Level} {Cataloging} of {Human} {Cells} {Response} to {Perturbations}},
	volume = {0},
	issn = {2405-4712},
	shorttitle = {The {Library} of {Integrated} {Network}-{Based} {Cellular} {Signatures} {NIH} {Program}},
	url = {http://www.cell.com/cell-systems/abstract/S2405-4712(17)30490-8},
	doi = {10.1016/j.cels.2017.11.001},
	abstract = {The Library of Integrated Network-Based Cellular Signatures (LINCS) is an NIH Common Fund program that catalogs how human cells globally respond to chemical, genetic, and disease perturbations. Resources generated by LINCS include experimental and computational methods, visualization tools, molecular and imaging data, and signatures. By assembling an integrated picture of the range of responses of human cells exposed to many perturbations, the LINCS program aims to better understand human disease and to advance the development of new therapies. Perturbations under study include drugs, genetic perturbations, tissue micro-environments, antibodies, and disease-causing mutations. Responses to perturbations are measured by transcript profiling, mass spectrometry, cell imaging, and biochemical methods, among other assays. The LINCS program focuses on cellular physiology shared among tissues and cell types relevant to an array of diseases, including cancer, heart disease, and neurodegenerative disorders. This Perspective describes LINCS technologies, datasets, tools, and approaches to data accessibility and reusability.},
	language = {English},
	number = {0},
	urldate = {2018-01-22},
	journal = {Cell Systems},
	author = {Keenan, Alexandra B. and Jenkins, Sherry L. and Jagodnik, Kathleen M. and Koplev, Simon and He, Edward and Torre, Denis and Wang, Zichen and Dohlman, Anders B. and Silverstein, Moshe C. and Lachmann, Alexander and Kuleshov, Maxim V. and Ma'ayan, Avi and Stathias, Vasileios and Terryn, Raymond and Cooper, Daniel and Forlin, Michele and Koleti, Amar and Vidovic, Dusica and Chung, Caty and Schürer, Stephan C. and Vasiliauskas, Jouzas and Pilarczyk, Marcin and Shamsaei, Behrouz and Fazel, Mehdi and Ren, Yan and Niu, Wen and Clark, Nicholas A. and White, Shana and Mahi, Naim and Zhang, Lixia and Kouril, Michal and Reichard, John F. and Sivaganesan, Siva and Medvedovic, Mario and Meller, Jaroslaw and Koch, Rick J. and Birtwistle, Marc R. and Iyengar, Ravi and Sobie, Eric A. and Azeloglu, Evren U. and Kaye, Julia and Osterloh, Jeannette and Haston, Kelly and Kalra, Jaslin and Finkbiener, Steve and Li, Jonathan and Milani, Pamela and Adam, Miriam and Escalante-Chong, Renan and Sachs, Karen and Lenail, Alex and Ramamoorthy, Divya and Fraenkel, Ernest and Daigle, Gavin and Hussain, Uzma and Coye, Alyssa and Rothstein, Jeffrey and Sareen, Dhruv and Ornelas, Loren and Banuelos, Maria and Mandefro, Berhan and Ho, Ritchie and Svendsen, Clive N. and Lim, Ryan G. and Stocksdale, Jennifer and Casale, Malcolm S. and Thompson, Terri G. and Wu, Jie and Thompson, Leslie M. and Dardov, Victoria and Venkatraman, Vidya and Matlock, Andrea and Eyk, Jennifer E. Van and Jaffe, Jacob D. and Papanastasiou, Malvina and Subramanian, Aravind and Golub, Todd R. and Erickson, Sean D. and Fallahi-Sichani, Mohammad and Hafner, Marc and Gray, Nathanael S. and Lin, Jia-Ren and Mills, Caitlin E. and Muhlich, Jeremy L. and Niepel, Mario and Shamu, Caroline E. and Williams, Elizabeth H. and Wrobel, David and Sorger, Peter K. and Heiser, Laura M. and Gray, Joe W. and Korkola, James E. and Mills, Gordon B. and LaBarge, Mark and Feiler, Heidi S. and Dane, Mark A. and Bucher, Elmar and Nederlof, Michel and Sudar, Damir and Gross, Sean and Kilburn, David F. and Smith, Rebecca and Devlin, Kaylyn and Margolis, Ron and Derr, Leslie and Lee, Albert and Pillai, Ajay},
	month = nov,
	year = {2017},
	pmid = {29199020},
	keywords = {BD2K, L1000, MCF10A, MEMA, P100, data integration, lincsprogram, lincsproject, systems biology, systems pharmacology},
}

@article{subramanian_next_2017,
	title = {A {Next} {Generation} {Connectivity} {Map}: {L1000} {Platform} and the {First} 1,000,000 {Profiles}},
	volume = {171},
	issn = {0092-8674, 1097-4172},
	shorttitle = {A {Next} {Generation} {Connectivity} {Map}},
	url = {http://www.cell.com/cell/abstract/S0092-8674(17)31309-0},
	doi = {10.1016/j.cell.2017.10.049},
	abstract = {We previously piloted the concept of a Connectivity Map (CMap), whereby genes, drugs, and disease states are connected by virtue of common gene-expression signatures. Here, we report more than a 1,000-fold scale-up of the CMap as part of the NIH LINCS Consortium, made possible by a new, low-cost, high-throughput reduced representation expression profiling method that we term L1000. We show that L1000 is highly reproducible, comparable to RNA sequencing, and suitable for computational inference of the expression levels of 81\% of non-measured transcripts. We further show that the expanded CMap can be used to discover mechanism of action of small molecules, functionally annotate genetic variants of disease genes, and inform clinical trials. The 1.3 million L1000 profiles described here, as well as tools for their analysis, are available at https://clue.io.},
	language = {English},
	number = {6},
	urldate = {2018-01-22},
	journal = {Cell},
	author = {Subramanian, Aravind and Narayan, Rajiv and Corsello, Steven M. and Peck, David D. and Natoli, Ted E. and Lu, Xiaodong and Gould, Joshua and Davis, John F. and Tubelli, Andrew A. and Asiedu, Jacob K. and Lahr, David L. and Hirschman, Jodi E. and Liu, Zihan and Donahue, Melanie and Julian, Bina and Khan, Mariya and Wadden, David and Smith, Ian C. and Lam, Daniel and Liberzon, Arthur and Toder, Courtney and Bagul, Mukta and Orzechowski, Marek and Enache, Oana M. and Piccioni, Federica and Johnson, Sarah A. and Lyons, Nicholas J. and Berger, Alice H. and Shamji, Alykhan F. and Brooks, Angela N. and Vrcic, Anita and Flynn, Corey and Rosains, Jacqueline and Takeda, David Y. and Hu, Roger and Davison, Desiree and Lamb, Justin and Ardlie, Kristin and Hogstrom, Larson and Greenside, Peyton and Gray, Nathanael S. and Clemons, Paul A. and Silver, Serena and Wu, Xiaoyun and Zhao, Wen-Ning and Read-Button, Willis and Wu, Xiaohua and Haggarty, Stephen J. and Ronco, Lucienne V. and Boehm, Jesse S. and Schreiber, Stuart L. and Doench, John G. and Bittker, Joshua A. and Root, David E. and Wong, Bang and Golub, Todd R.},
	month = nov,
	year = {2017},
	pmid = {29195078},
	keywords = {Functional genomics, chemical biology, gene expression profiling},
	pages = {1437--1452.e17},
}

@article{schmitt_compendium_2016-1,
	title = {A {Compendium} of {Chromatin} {Contact} {Maps} {Reveals} {Spatially} {Active} {Regions} in the {Human} {Genome}},
	volume = {17},
	issn = {2211-1247},
	url = {http://www.sciencedirect.com/science/article/pii/S2211124716314814},
	doi = {10.1016/j.celrep.2016.10.061},
	abstract = {Summary
The three-dimensional configuration of DNA is integral to all nuclear processes in eukaryotes, yet our knowledge of the chromosome architecture is still limited. Genome-wide chromosome conformation capture studies have uncovered features of chromatin organization in cultured cells, but genome architecture in human tissues has yet to be explored. Here, we report the most comprehensive survey to date of chromatin organization in human tissues. Through integrative analysis of chromatin contact maps in 21 primary human tissues and cell types, we find topologically associating domains highly conserved in different tissues. We also discover genomic regions that exhibit unusually high levels of local chromatin interactions. These frequently interacting regions (FIREs) are enriched for super-enhancers and are near tissue-specifically expressed genes. They display strong tissue-specificity in local chromatin interactions. Additionally, FIRE formation is partially dependent on CTCF and the Cohesin complex. We further show that FIREs can help annotate the function of non-coding sequence variants.},
	number = {8},
	urldate = {2018-01-22},
	journal = {Cell Reports},
	author = {Schmitt, Anthony D. and Hu, Ming and Jung, Inkyung and Xu, Zheng and Qiu, Yunjiang and Tan, Catherine L. and Li, Yun and Lin, Shin and Lin, Yiing and Barr, Cathy L. and Ren, Bing},
	month = nov,
	year = {2016},
	pages = {2042--2059},
}

@article{chai_review_2014,
	title = {A review on the computational approaches for gene regulatory network construction},
	volume = {48},
	journal = {Computers in biology and medicine},
	author = {Chai, Lian En and Loh, Swee Kuan and Low, Swee Thing and Mohamad, Mohd Saberi and Deris, Safaai and Zakaria, Zalmiyah},
	year = {2014},
	pages = {55--65},
}

@article{melancon_random_2001,
	title = {Random generation of directed acyclic graphs},
	volume = {10},
	journal = {Electronic Notes in Discrete Mathematics},
	author = {Melançon, Guy and Dutour, Isabelle and Bousquet-Mélou, Mireille},
	year = {2001},
	pages = {202--207},
}

@article{chai_review_2014-1,
	title = {A review on the computational approaches for gene regulatory network construction},
	volume = {48},
	journal = {Computers in biology and medicine},
	author = {Chai, Lian En and Loh, Swee Kuan and Low, Swee Thing and Mohamad, Mohd Saberi and Deris, Safaai and Zakaria, Zalmiyah},
	year = {2014},
	pages = {55--65},
}

@article{de_smet_advantages_2010,
	title = {Advantages and limitations of current network inference methods},
	volume = {8},
	number = {10},
	journal = {Nature Reviews Microbiology},
	author = {De Smet, Riet and Marchal, Kathleen},
	year = {2010},
	pages = {717--729},
}

@article{thompson_comparative_2015,
	title = {Comparative analysis of gene regulatory networks: from network reconstruction to evolution},
	volume = {31},
	shorttitle = {Comparative analysis of gene regulatory networks},
	journal = {Annual review of cell and developmental biology},
	author = {Thompson, Dawn and Regev, Aviv and Roy, Sushmita},
	year = {2015},
	pages = {399--428},
}

@article{kalisch_package_2017,
	title = {Package ‘pcalg’},
	author = {Kalisch, Markus and Hauser, Alain and Maechler, Martin and Colombo, Diego and Entner, Doris and Hoyer, Patrik and Hyttinen, Antti and Peters, Jonas and Andri, Nicoletta and Perkovic, Emilija},
	year = {2017},
}

@article{kalisch_package_2017-1,
	title = {Package ‘pcalg’},
	author = {Kalisch, Markus and Hauser, Alain and Maechler, Martin and Colombo, Diego and Entner, Doris and Hoyer, Patrik and Hyttinen, Antti and Peters, Jonas and Andri, Nicoletta and Perkovic, Emilija},
	year = {2017},
}

@article{kalisch_package_2017-2,
	title = {Package ‘pcalg’},
	author = {Kalisch, Markus and Hauser, Alain and Maechler, Martin and Colombo, Diego and Entner, Doris and Hoyer, Patrik and Hyttinen, Antti and Peters, Jonas and Andri, Nicoletta and Perkovic, Emilija},
	year = {2017},
}

@book{scutari_package_2017,
	title = {Package ‘bnlearn’},
	author = {Scutari, Marco and Scutari, Maintainer Marco and MMPC, Hiton-PC},
	year = {2017},
}

@article{melancon_generating_2004,
	title = {Generating connected acyclic digraphs uniformly at random},
	url = {http://arxiv.org/abs/cs/0403040},
	abstract = {We describe a simple algorithm based on a Markov chain process to generate simply connected acyclic directed graphs over a fixed set of vertices. This algorithm is an extension of a previous one, designed to generate acyclic digraphs, non necessarily connected.},
	urldate = {2018-01-10},
	journal = {arXiv:cs/0403040},
	author = {Melancon, Guy and Philippe, Fabrice},
	month = mar,
	year = {2004},
	note = {arXiv: cs/0403040},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Discrete Mathematics, F.2.2, G.2.2, G.3},
}

@book{nagarajan_bayesian_2013,
	title = {Bayesian networks in {R}},
	volume = {122},
	publisher = {Springer},
	author = {Nagarajan, Radhakrishnan and Scutari, Marco and Lèbre, Sophie},
	year = {2013},
}

@article{kurtz_sparse_2015,
	title = {Sparse and {Compositionally} {Robust} {Inference} of {Microbial} {Ecological} {Networks}},
	volume = {11},
	issn = {1553-7358},
	url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004226},
	doi = {10.1371/journal.pcbi.1004226},
	abstract = {Author Summary Genomic survey of microbes by 16S rRNA gene sequencing and metagenomics has inspired appreciation for the role of complex communities in diverse ecosystems. However, due to the unique properties of community composition data, standard data analysis tools are likely to produce statistical artifacts. For a typical experiment studying microbial ecosystems these artifacts can lead to erroneous conclusions about patterns of associations between microbial taxa. We developed a new procedure that seeks to infer ecological associations between microbial populations, by 1) taking advantage of the proportionality invariance of relative abundance data and 2) making assumptions about the underlying network structure when the number of taxa in the dataset is larger than the number of sampled communities. Additionally, we employed a novel tool to generate biologically plausible synthetic data and objectively benchmark current association inference tools. Finally, we tested our procedures on a large-scale 16S rRNA gene sequencing dataset sampled from the human gut.},
	language = {en},
	number = {5},
	urldate = {2018-01-10},
	journal = {PLOS Computational Biology},
	author = {Kurtz, Zachary D. and Müller, Christian L. and Miraldi, Emily R. and Littman, Dan R. and Blaser, Martin J. and Bonneau, Richard A.},
	month = may,
	year = {2015},
	keywords = {Community ecology, Covariance, Microbial ecology, Microbiome, Network analysis, Scale-free networks, Statistical data, Theoretical ecology},
	pages = {e1004226},
}

@article{kurtz_sparse_2015-1,
	title = {Sparse and {Compositionally} {Robust} {Inference} of {Microbial} {Ecological} {Networks}},
	volume = {11},
	issn = {1553-7358},
	url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004226},
	doi = {10.1371/journal.pcbi.1004226},
	abstract = {Author Summary Genomic survey of microbes by 16S rRNA gene sequencing and metagenomics has inspired appreciation for the role of complex communities in diverse ecosystems. However, due to the unique properties of community composition data, standard data analysis tools are likely to produce statistical artifacts. For a typical experiment studying microbial ecosystems these artifacts can lead to erroneous conclusions about patterns of associations between microbial taxa. We developed a new procedure that seeks to infer ecological associations between microbial populations, by 1) taking advantage of the proportionality invariance of relative abundance data and 2) making assumptions about the underlying network structure when the number of taxa in the dataset is larger than the number of sampled communities. Additionally, we employed a novel tool to generate biologically plausible synthetic data and objectively benchmark current association inference tools. Finally, we tested our procedures on a large-scale 16S rRNA gene sequencing dataset sampled from the human gut.},
	language = {en},
	number = {5},
	urldate = {2018-01-10},
	journal = {PLOS Computational Biology},
	author = {Kurtz, Zachary D. and Müller, Christian L. and Miraldi, Emily R. and Littman, Dan R. and Blaser, Martin J. and Bonneau, Richard A.},
	month = may,
	year = {2015},
	keywords = {Community ecology, Covariance, Microbial ecology, Microbiome, Network analysis, Scale-free networks, Statistical data, Theoretical ecology},
	pages = {e1004226},
}

@article{marbach_wisdom_2012,
	title = {Wisdom of crowds for robust gene network inference},
	volume = {9},
	copyright = {2012 Nature Publishing Group},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.2016},
	doi = {10.1038/nmeth.2016},
	abstract = {{\textless}p{\textgreater}This analysis comprehensively compares methods for gene regulatory network inference submitted through the DREAM5 challenge. It demonstrates that integration of predictions from multiple methods shows the most robust performance across data sets.{\textless}/p{\textgreater}},
	language = {En},
	number = {8},
	urldate = {2018-01-10},
	journal = {Nature Methods},
	author = {Marbach, Daniel and Costello, James C. and Küffner, Robert and Vega, Nicole M. and Prill, Robert J. and Camacho, Diogo M. and Allison, Kyle R. and Consortium, The DREAM5 and Aderhold, Andrej and Allison, Kyle R. and Bonneau, Richard and Camacho, Diogo M. and Chen, Yukun and Collins, James J. and Cordero, Francesca and Costello, James C. and Crane, Martin and Dondelinger, Frank and Drton, Mathias and Esposito, Roberto and Foygel, Rina and Fuente, Alberto de la and Gertheiss, Jan and Geurts, Pierre and Greenfield, Alex and Grzegorczyk, Marco and Haury, Anne-Claire and Holmes, Benjamin and Hothorn, Torsten and Husmeier, Dirk and Huynh-Thu, Vân Anh and Irrthum, Alexandre and Kellis, Manolis and Karlebach, Guy and Küffner, Robert and Lèbre, Sophie and Leo, Vincenzo De and Madar, Aviv and Mani, Subramani and Marbach, Daniel and Mordelet, Fantine and Ostrer, Harry and Ouyang, Zhengyu and Pandya, Ravi and Petri, Tobias and Pinna, Andrea and Poultney, Christopher S. and Prill, Robert J. and Rezny, Serena and Ruskin, Heather J. and Saeys, Yvan and Shamir, Ron and Sîrbu, Alina and Song, Mingzhou and Soranzo, Nicola and Statnikov, Alexander and Stolovitzky, Gustavo and Vega, Nicci and Vera-Licona, Paola and Vert, Jean-Philippe and Visconti, Alessia and Wang, Haizhou and Wehenkel, Louis and Windhager, Lukas and Zhang, Yang and Zimmer, Ralf and Kellis, Manolis and Collins, James J. and Stolovitzky, Gustavo},
	month = aug,
	year = {2012},
	pages = {796},
}

@article{hill_inferring_2016,
	title = {Inferring causal molecular networks: empirical assessment through a community-based effort},
	volume = {13},
	copyright = {2016 Nature Publishing Group},
	issn = {1548-7105},
	shorttitle = {Inferring causal molecular networks},
	url = {https://www.nature.com/articles/nmeth.3773},
	doi = {10.1038/nmeth.3773},
	abstract = {{\textless}p{\textgreater}The HPN-DREAM community challenge assessed the ability of computational methods to infer causal molecular networks, focusing specifically on the task of inferring causal protein signaling networks in cancer cell lines.{\textless}/p{\textgreater}},
	language = {En},
	number = {4},
	urldate = {2018-01-10},
	journal = {Nature Methods},
	author = {Hill, Steven M. and Heiser, Laura M. and Cokelaer, Thomas and Unger, Michael and Nesser, Nicole K. and Carlin, Daniel E. and Zhang, Yang and Sokolov, Artem and Paull, Evan O. and Wong, Chris K. and Graim, Kiley and Bivol, Adrian and Wang, Haizhou and Zhu, Fan and Afsari, Bahman and Danilova, Ludmila V. and Favorov, Alexander V. and Lee, Wai Shing and Taylor, Dane and Hu, Chenyue W. and Long, Byron L. and Noren, David P. and Bisberg, Alexander J. and Consortium, The HPN-DREAM and Afsari, Bahman and Al-Ouran, Rami and Anton, Bernat and Arodz, Tomasz and Sichani, Omid Askari and Bagheri, Neda and Berlow, Noah and Bisberg, Alexander J. and Bivol, Adrian and Bohler, Anwesha and Bonet, Jaume and Bonneau, Richard and Budak, Gungor and Bunescu, Razvan and Caglar, Mehmet and Cai, Binghuang and Cai, Chunhui and Carlin, Daniel E. and Carlon, Azzurra and Chen, Lujia and Ciaccio, Mark F. and Cokelaer, Thomas and Cooper, Gregory and Creighton, Chad J. and Daneshmand, Seyed-Mohammad-Hadi and Fuente, Alberto de la and Camillo, Barbara Di and Danilova, Ludmila V. and Dutta-Moscato, Joyeeta and Emmett, Kevin and Evelo, Chris and Fassia, Mohammad-Kasim H. and Favorov, Alexander V. and Fertig, Elana J. and Finkle, Justin D. and Finotello, Francesca and Friend, Stephen and Gao, Xi and Gao, Jean and Garcia-Garcia, Javier and Ghosh, Samik and Giaretta, Alberto and Graim, Kiley and Gray, Joe W. and Großeholz, Ruth and Guan, Yuanfang and Guinney, Justin and Hafemeister, Christoph and Hahn, Oliver and Haider, Saad and Hase, Takeshi and Heiser, Laura M. and Hill, Steven M. and Hodgson, Jay and Hoff, Bruce and Hsu, Chih Hao and Hu, Chenyue W. and Hu, Ying and Huang, Xun and Jalili, Mahdi and Jiang, Xia and Kacprowski, Tim and Kaderali, Lars and Kang, Mingon and Kannan, Venkateshan and Kellen, Michael and Kikuchi, Kaito and Kim, Dong-Chul and Kitano, Hiroaki and Knapp, Bettina and Komatsoulis, George and Koeppl, Heinz and Krämer, Andreas and Kursa, Miron Bartosz and Kutmon, Martina and Lee, Wai Shing and Li, Yichao and Liang, Xiaoyu and Liu, Zhaoqi and Liu, Yu and Long, Byron L. and Lu, Songjian and Lu, Xinghua and Manfrini, Marco and Matos, Marta R. A. and Meerzaman, Daoud and Mills, Gordon B. and Min, Wenwen and Mukherjee, Sach and Müller, Christian Lorenz and Neapolitan, Richard E. and Nesser, Nicole K. and Noren, David P. and Norman, Thea and Oliva, Baldo and Opiyo, Stephen Obol and Pal, Ranadip and Palinkas, Aljoscha and Paull, Evan O. and Planas-Iglesias, Joan and Poglayen, Daniel and Qutub, Amina A. and Saez-Rodriguez, Julio and Sambo, Francesco and Sanavia, Tiziana and Sharifi-Zarchi, Ali and Slawek, Janusz and Sokolov, Artem and Song, Mingzhou and Spellman, Paul T. and Streck, Adam and Stolovitzky, Gustavo and Strunz, Sonja and Stuart, Joshua M. and Taylor, Dane and Tegnér, Jesper and Thobe, Kirste and Toffolo, Gianna Maria and Trifoglio, Emanuele and Unger, Michael and Wan, Qian and Wang, Haizhou and Welch, Lonnie and Wong, Chris K. and Wu, Jia J. and Xue, Albert Y. and Yamanaka, Ryota and Yan, Chunhua and Zairis, Sakellarios and Zengerling, Michael and Zenil, Hector and Zhang, Shihua and Zhang, Yang and Zhu, Fan and Zi, Zhike and Mills, Gordon B. and Gray, Joe W. and Kellen, Michael and Norman, Thea and Friend, Stephen and Qutub, Amina A. and Fertig, Elana J. and Guan, Yuanfang and Song, Mingzhou and Stuart, Joshua M. and Spellman, Paul T. and Koeppl, Heinz and Stolovitzky, Gustavo and Saez-Rodriguez, Julio and Mukherjee, Sach},
	month = apr,
	year = {2016},
	pages = {310},
}

@article{statnikov_ultra-scalable_2015,
	title = {Ultra-scalable and efficient methods for hybrid observational and experimental local causal pathway discovery.},
	volume = {16},
	journal = {Journal of Machine Learning Research},
	author = {Statnikov, Alexander R. and Ma, Sisi and Henaff, Mikael and Lytkin, Nikita I. and Efstathiadis, Efstratios and Peskin, Eric R. and Aliferis, Constantin F.},
	year = {2015},
	pages = {3219--3267},
}

@article{brent_past_2016,
	title = {Past roadblocks and new opportunities in transcription factor network mapping},
	volume = {32},
	issn = {0168-9525},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5117949/},
	doi = {10.1016/j.tig.2016.08.009},
	abstract = {One of the principal mechanisms by which cells differentiate and respond to changes in external signals or conditions is by changing the activity levels of transcription factors (TFs). This changes the transcription rates of target genes via the cell’s TF network, which ultimately contributes to reconfiguring cellular state. Since microarrays provided our first window into global cellular state, computational biologists have eagerly attacked the problem of mapping TF networks, a key part of the cell’s control circuitry. In retrospect, however, steady-state mRNA abundance levels were a poor substitute for TF activity levels and gene transcription rates. Likewise, mapping TF binding through chromatin immunoprecipitation proved less predictive of functional regulation and less amenable to systematic elucidation of complete networks than originally hoped. This review explains these roadblocks and the current, unprecedented blossoming of new experimental techniques built on second generation sequencing, which hold out the promise of rapid progress in TF network mapping.},
	number = {11},
	urldate = {2018-01-10},
	journal = {Trends in genetics : TIG},
	author = {Brent, Michael R.},
	month = nov,
	year = {2016},
	pmid = {27720190},
	pmcid = {PMC5117949},
	pages = {736--750},
}

@article{brent_past_2016-1,
	title = {Past {Roadblocks} and {New} {Opportunities} in {Transcription} {Factor} {Network} {Mapping}},
	volume = {32},
	issn = {0168-9525},
	url = {http://www.sciencedirect.com/science/article/pii/S0168952516301019},
	doi = {10.1016/j.tig.2016.08.009},
	abstract = {One of the principal mechanisms by which cells differentiate and respond to changes in external signals or conditions is by changing the activity levels of transcription factors (TFs). This changes the transcription rates of target genes via the cell's TF network, which ultimately contributes to reconfiguring cellular state. Since microarrays provided our first window into global cellular state, computational biologists have eagerly attacked the problem of mapping TF networks, a key part of the cell's control circuitry. In retrospect, however, steady-state mRNA abundance levels were a poor substitute for TF activity levels and gene transcription rates. Likewise, mapping TF binding through chromatin immunoprecipitation proved less predictive of functional regulation and less amenable to systematic elucidation of complete networks than originally hoped. This review explains these roadblocks and the current, unprecedented blossoming of new experimental techniques built on second-generation sequencing, which hold out the promise of rapid progress in TF network mapping.},
	number = {11},
	urldate = {2018-01-10},
	journal = {Trends in Genetics},
	author = {Brent, Michael R.},
	month = nov,
	year = {2016},
	keywords = {computational methods, gene expression profiling, nascent RNA sequencing, regulatory systems biology, transcription factor activity, transcriptional regulatory networks.},
	pages = {736--750},
}

@article{villaverde_biopredyn-bench:_2015,
	title = {{BioPreDyn}-bench: a suite of benchmark problems for dynamic modelling in systems biology},
	volume = {9},
	issn = {1752-0509},
	shorttitle = {{BioPreDyn}-bench},
	url = {https://doi.org/10.1186/s12918-015-0144-4},
	doi = {10.1186/s12918-015-0144-4},
	abstract = {Dynamic modelling is one of the cornerstones of systems biology. Many research efforts are currently being invested in the development and exploitation of large-scale kinetic models. The associated problems of parameter estimation (model calibration) and optimal experimental design are particularly challenging. The community has already developed many methods and software packages which aim to facilitate these tasks. However, there is a lack of suitable benchmark problems which allow a fair and systematic evaluation and comparison of these contributions.},
	urldate = {2018-01-10},
	journal = {BMC Systems Biology},
	author = {Villaverde, Alejandro F. and Henriques, David and Smallbone, Kieran and Bongard, Sophia and Schmid, Joachim and Cicin-Sain, Damjan and Crombach, Anton and Saez-Rodriguez, Julio and Mauch, Klaus and Balsa-Canto, Eva and Mendes, Pedro and Jaeger, Johannes and Banga, Julio R.},
	month = feb,
	year = {2015},
	keywords = {Benchmarks, Dynamic modelling, Large-scale, Metabolism, Model calibration, Optimization, Parameter estimation, Signal transduction, Transcription, development},
	pages = {8},
}

@article{pinna_simulating_2011,
	title = {Simulating systems genetics data with {SysGenSIM}},
	volume = {27},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/27/17/2459/224428},
	doi = {10.1093/bioinformatics/btr407},
	abstract = {Summary: SysGenSIM is a software package to simulate Systems Genetics (SG) experiments in model organisms, for the purpose of evaluating and comparing statistical and computational methods and their implementations for analyses of SG data [e.g. methods for expression quantitative trait loci (eQTL) mapping and network inference]. SysGenSIM allows the user to select a variety of network topologies, genetic and kinetic parameters to simulate SG data ( genotyping, gene expression and phenotyping) with large gene networks with thousands of nodes. The software is encoded in MATLAB, and a user-friendly graphical user interface is provided.Availability: The open-source software code and user manual can be downloaded at: http://sysgensim.sourceforge.net/Contact:alf@crs4.it},
	language = {en},
	number = {17},
	urldate = {2018-01-10},
	journal = {Bioinformatics},
	author = {Pinna, Andrea and Soranzo, Nicola and Hoeschele, Ina and de la Fuente, Alberto},
	month = sep,
	year = {2011},
	pages = {2459--2462},
}

@article{tripathi_sgnesr:_2017,
	title = {{sgnesR}: {An} {R} package for simulating gene expression data from an underlying real gene network structure considering delay parameters},
	volume = {18},
	issn = {1471-2105},
	shorttitle = {{sgnesR}},
	url = {https://doi.org/10.1186/s12859-017-1731-8},
	doi = {10.1186/s12859-017-1731-8},
	abstract = {sgnesR (Stochastic Gene Network Expression Simulator in R) is an R package that provides an interface to simulate gene expression data from a given gene network using the stochastic simulation algorithm (SSA). The package allows various options for delay parameters and can easily included in reactions for promoter delay, RNA delay and Protein delay. A user can tune these parameters to model various types of reactions within a cell. As examples, we present two network models to generate expression profiles. We also demonstrated the inference of networks and the evaluation of association measure of edge and non-edge components from the generated expression profiles.},
	urldate = {2018-01-10},
	journal = {BMC Bioinformatics},
	author = {Tripathi, Shailesh and Lloyd-Price, Jason and Ribeiro, Andre and Yli-Harja, Olli and Dehmer, Matthias and Emmert-Streib, Frank},
	month = jul,
	year = {2017},
	keywords = {Gene expression data, Gene network, Simulation},
	pages = {325},
}

@article{coker_signet:_2017,
	title = {{SiGNet}: {A} signaling network data simulator to enable signaling network inference},
	volume = {12},
	issn = {1932-6203},
	shorttitle = {{SiGNet}},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0177701},
	doi = {10.1371/journal.pone.0177701},
	abstract = {Network models are widely used to describe complex signaling systems. Cellular wiring varies in different cellular contexts and numerous inference techniques have been developed to infer the structure of a network from experimental data of the network’s behavior. To objectively identify which inference strategy is best suited to a specific network, a gold standard network and dataset are required. However, suitable datasets for benchmarking are difficult to find. Numerous tools exist that can simulate data for transcriptional networks, but these are of limited use for the study of signaling networks. Here, we describe SiGNet (Signal Generator for Networks): a Cytoscape app that simulates experimental data for a signaling network of known structure. SiGNet has been developed and tested against published experimental data, incorporating information on network architecture, and the directionality and strength of interactions to create biological data in silico. SiGNet is the first tool to simulate biological signaling data, enabling an accurate and systematic assessment of inference strategies. SiGNet can also be used to produce preliminary models of key biological pathways following perturbation.},
	language = {en},
	number = {5},
	urldate = {2018-01-10},
	journal = {PLOS ONE},
	author = {Coker, Elizabeth A. and Mitsopoulos, Costas and Workman, Paul and Al-Lazikani, Bissan},
	month = may,
	year = {2017},
	keywords = {Biochemical simulations, Cell signaling structures, DNA transcription, Network motifs, Protein interaction networks, Protein structure networks, Signaling networks, Simulation and modeling},
	pages = {e0177701},
}

@article{sachs_causal_2005,
	title = {Causal protein-signaling networks derived from multiparameter single-cell data},
	volume = {308},
	number = {5721},
	journal = {Science},
	author = {Sachs, Karen and Perez, Omar and Pe'er, Dana and Lauffenburger, Douglas A. and Nolan, Garry P.},
	year = {2005},
	pages = {523--529},
}

@article{le_novere_quantitative_2015,
	title = {Quantitative and logic modelling of molecular and gene networks},
	author = {Le Novère, Nicolas},
	year = {2015},
}

@article{hill_bayesian_2011,
	title = {Bayesian nonparametric modeling for causal inference},
	volume = {20},
	number = {1},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Hill, Jennifer L.},
	year = {2011},
	pages = {217--240},
}

@article{shalit_estimating_2016,
	title = {Estimating individual treatment effect: generalization bounds and algorithms},
	shorttitle = {Estimating individual treatment effect},
	url = {http://arxiv.org/abs/1606.03976},
	abstract = {There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a "balanced" representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization-error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.},
	urldate = {2018-01-08},
	journal = {arXiv:1606.03976 [cs, stat]},
	author = {Shalit, Uri and Johansson, Fredrik D. and Sontag, David},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.03976},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning},
}

@article{louizos_causal_2017,
	title = {Causal {Effect} {Inference} with {Deep} {Latent}-{Variable} {Models}},
	url = {http://arxiv.org/abs/1705.08821},
	abstract = {Learning individual-level causal effects from observational data, such as inferring the most effective medication for a specific patient, is a problem of growing importance for policy makers. The most important aspect of inferring causal effects from observational data is the handling of confounders, factors that affect both an intervention and its outcome. A carefully designed observational study attempts to measure all important confounders. However, even if one does not have direct access to all confounders, there may exist noisy and uncertain measurement of proxies for confounders. We build on recent advances in latent variable modeling to simultaneously estimate the unknown latent space summarizing the confounders and the causal effect. Our method is based on Variational Autoencoders (VAE) which follow the causal structure of inference with proxies. We show our method is significantly more robust than existing methods, and matches the state-of-the-art on previous benchmarks focused on individual treatment effects.},
	urldate = {2018-01-08},
	journal = {arXiv:1705.08821 [cs, stat]},
	author = {Louizos, Christos and Shalit, Uri and Mooij, Joris and Sontag, David and Zemel, Richard and Welling, Max},
	month = may,
	year = {2017},
	note = {arXiv: 1705.08821},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{muandet_kernel_2017,
	title = {Kernel mean embedding of distributions: {A} review and beyond},
	volume = {10},
	shorttitle = {Kernel mean embedding of distributions},
	number = {1-2},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Muandet, Krikamol and Fukumizu, Kenji and Sriperumbudur, Bharath and Schölkopf, Bernhard},
	year = {2017},
	pages = {1--141},
}

@article{muandet_kernel_2017-1,
	title = {Kernel mean embedding of distributions: {A} review and beyond},
	volume = {10},
	shorttitle = {Kernel mean embedding of distributions},
	number = {1-2},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Muandet, Krikamol and Fukumizu, Kenji and Sriperumbudur, Bharath and Schölkopf, Bernhard},
	year = {2017},
	pages = {1--141},
}

@article{szabo_learning_2016,
	title = {Learning theory for distribution regression},
	volume = {17},
	number = {152},
	journal = {Journal of Machine Learning Research},
	author = {Szabó, Zoltán and Sriperumbudur, Bharath K. and Póczos, Barnabás and Gretton, Arthur},
	year = {2016},
	pages = {1--40},
}

@incollection{janzing_justifying_2015,
	title = {Justifying information-geometric causal inference},
	booktitle = {Measures of {Complexity}},
	publisher = {Springer},
	author = {Janzing, Dominik and Steudel, Bastian and Shajarisales, Naji and Schölkopf, Bernhard},
	year = {2015},
	pages = {253--265},
}

@article{louizos_causal_2017,
	title = {Causal {Effect} {Inference} with {Deep} {Latent}-{Variable} {Models}},
	url = {http://arxiv.org/abs/1705.08821},
	abstract = {Learning individual-level causal effects from observational data, such as inferring the most effective medication for a specific patient, is a problem of growing importance for policy makers. The most important aspect of inferring causal effects from observational data is the handling of confounders, factors that affect both an intervention and its outcome. A carefully designed observational study attempts to measure all important confounders. However, even if one does not have direct access to all confounders, there may exist noisy and uncertain measurement of proxies for confounders. We build on recent advances in latent variable modeling to simultaneously estimate the unknown latent space summarizing the confounders and the causal effect. Our method is based on Variational Autoencoders (VAE) which follow the causal structure of inference with proxies. We show our method is significantly more robust than existing methods, and matches the state-of-the-art on previous benchmarks focused on individual treatment effects.},
	urldate = {2018-01-05},
	journal = {arXiv:1705.08821 [cs, stat]},
	author = {Louizos, Christos and Shalit, Uri and Mooij, Joris and Sontag, David and Zemel, Richard and Welling, Max},
	month = may,
	year = {2017},
	note = {arXiv: 1705.08821},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{sella_miic_2017,
	title = {{MIIC} online: a web server to reconstruct causal or non-causal networks from non-perturbative data},
	shorttitle = {{MIIC} online},
	journal = {Bioinformatics},
	author = {Sella, Nadir and Verny, Louis and Uguzzoni, Guido and Affeldt, Séverine and Isambert, Hervé},
	year = {2017},
	keywords = {team},
}

@article{peters_identifiability_2012,
	title = {Identifiability of causal graphs using functional models},
	journal = {arXiv preprint arXiv:1202.3757},
	author = {Peters, Jonas and Mooij, Joris and Janzing, Dominik and Schölkopf, Bernhard},
	year = {2012},
}

@article{peters_identifiability_2014,
	title = {Identifiability of {Gaussian} structural equation models with equal error variances},
	volume = {101},
	issn = {1464-3510, 0006-3444},
	url = {http://arxiv.org/abs/1205.2536},
	doi = {10.1093/biomet/ast043},
	abstract = {We consider structural equation models in which variables can be written as a function of their parents and noise terms, which are assumed to be jointly independent. Corresponding to each structural equation model, there is a directed acyclic graph describing the relationships between the variables. In Gaussian structural equation models with linear functions, the graph can be identified from the joint distribution only up to Markov equivalence classes, assuming faithfulness. In this work, we prove full identifiability if all noise variables have the same variances: the directed acyclic graph can be recovered from the joint Gaussian distribution. Our result has direct implications for causal inference: if the data follow a Gaussian structural equation model with equal error variances and assuming that all variables are observed, the causal structure can be inferred from observational data only. We propose a statistical method and an algorithm that exploit our theoretical findings.},
	number = {1},
	urldate = {2018-01-04},
	journal = {Biometrika},
	author = {Peters, Jonas and Bühlmann, Peter},
	month = mar,
	year = {2014},
	note = {arXiv: 1205.2536},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning},
	pages = {219--228},
}

@article{peters_identifiability_2013,
	title = {Identifiability of {Gaussian} structural equation models with equal error variances},
	volume = {101},
	number = {1},
	journal = {Biometrika},
	author = {Peters, Jonas and Bühlmann, Peter},
	year = {2013},
	pages = {219--228},
}

@article{dorie_automated_2017,
	title = {Automated versus do-it-yourself methods for causal inference: {Lessons} learned from a data analysis competition},
	shorttitle = {Automated versus do-it-yourself methods for causal inference},
	journal = {arXiv preprint arXiv:1707.02641},
	author = {Dorie, Vincent and Hill, Jennifer and Shalit, Uri and Scott, Marc and Cervone, Dan},
	year = {2017},
}

@article{shimizu_lingam:_2014,
	title = {{LiNGAM}: {Non}-{Gaussian} methods for estimating causal structures},
	volume = {41},
	shorttitle = {{LiNGAM}},
	number = {1},
	journal = {Behaviormetrika},
	author = {Shimizu, Shohei},
	year = {2014},
	pages = {65--98},
}

@article{cole_performance_2017,
	title = {Performance {Assessment} and {Selection} of {Normalization} {Procedures} for {Single}-{Cell} {RNA}-{Seq}},
	copyright = {© 2017, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/early/2017/12/16/235382},
	doi = {10.1101/235382},
	abstract = {Due to the presence of systematic measurement biases, data normalization is an essential preprocessing step in the analysis of single-cell RNA sequencing (scRNA-seq) data. While a variety of normalization procedures are available for bulk RNA-seq, their suitability with respect to single-cell data is still largely unexplored. Furthermore, there may be multiple, competing considerations behind the assessment of normalization performance, some of them study-specific. The choice of normalization method can have a large impact on the results of downstream analyses (e.g., clustering, inference of cell lineages, differential expression analysis), and thus it is critically important to assess the performance of competing methods in order to select a suitable procedure for the study at hand. We have developed scone - a framework that implements a wide range of normalization procedures in the context of scRNA-seq, and enables the assessment of their performance based on a comprehensive set of data-driven performance metrics. The accompanying open-source Bioconductor R software package scone (available at https://bioconductor.org/packages/scone) also provides numerical and graphical summaries of expression measures, data quality assessment, and data-adaptive gene and sample filtering criteria. We demonstrate the effectiveness of scone on a selection of scRNA-seq datasets across a variety of protocols, ranging from plate- to droplet-based methods. We show that scone is able to correctly rank normalization methods according to their performance in a given dataset and that selecting the best performing normalization leads to higher agreement with independent validation data than lowly-ranked methods.},
	language = {en},
	urldate = {2018-01-03},
	journal = {bioRxiv},
	author = {Cole, Michael B. and Risso, Davide and Wagner, Allon and DeTomaso, David and Ngai, John and Purdom, Elizabeth and Dudoit, Sandrine and Yosef, Nir},
	month = dec,
	year = {2017},
	pages = {235382},
}

@article{blobaum_novel_2017,
	title = {A {Novel} {Principle} for {Causal} {Inference} in {Data} with {Small} {Error} {Variance}},
	journal = {ESANN  2017  proceedings},
	author = {Blöbaum, Patrick and Shimizu, Shohei and Washio, Takashi},
	year = {2017},
}

@article{peters_causal_2014,
	title = {Causal {Discovery} with {Continuous} {Additive} {Noise} {Models}},
	volume = {15},
	journal = {Journal of Machine Learning Research},
	author = {Peters, Jonas and Mooij, Joris M. and Janzing, Dominik and Schölkopf, Bernhard},
	year = {2014},
	pages = {2009--2053},
}

@article{bareinboim_causal_2016,
	title = {Causal inference and the data-fusion problem},
	volume = {113},
	number = {27},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bareinboim, Elias and Pearl, Judea},
	year = {2016},
	pages = {7345--7352},
}

@techreport{athey_efficient_2016,
	title = {Efficient inference of average treatment effects in high dimensions via approximate residual balancing},
	author = {Athey, Susan and Imbens, Guido W. and Wager, Stefan},
	year = {2016},
}

@article{statnikov_new_2012,
	title = {New methods for separating causes from effects in genomics data},
	volume = {13},
	number = {8},
	journal = {BMC genomics},
	author = {Statnikov, Alexander and Henaff, Mikael and Lytkin, Nikita I. and Aliferis, Constantin F.},
	year = {2012},
	pages = {S22},
}

@article{hill_challenges_2011,
	title = {Challenges with propensity score strategies in a high-dimensional setting and a potential alternative},
	volume = {46},
	number = {3},
	journal = {Multivariate Behavioral Research},
	author = {Hill, Jennifer and Weiss, Christopher and Zhai, Fuhua},
	year = {2011},
	pages = {477--513},
}

@article{zigler_estimating_2012,
	title = {Estimating causal effects of air quality regulations using principal stratification for spatially correlated multivariate intermediate outcomes},
	volume = {13},
	number = {2},
	journal = {Biostatistics},
	author = {Zigler, Corwin M. and Dominici, Francesca and Wang, Yun},
	year = {2012},
	pages = {289--302},
}

@article{grimmer_estimating_2017,
	title = {Estimating heterogeneous treatment effects and the effects of heterogeneous treatments with ensemble methods},
	volume = {25},
	number = {4},
	journal = {Political Analysis},
	author = {Grimmer, Justin and Messing, Solomon and Westwood, Sean J.},
	year = {2017},
	pages = {413--434},
}

@article{athey_recursive_2016,
	title = {Recursive partitioning for heterogeneous causal effects},
	volume = {113},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/113/27/7353},
	doi = {10.1073/pnas.1510489113},
	abstract = {In this paper we propose methods for estimating heterogeneity in causal effects in experimental and observational studies and for conducting hypothesis tests about the magnitude of differences in treatment effects across subsets of the population. We provide a data-driven approach to partition the data into subpopulations that differ in the magnitude of their treatment effects. The approach enables the construction of valid confidence intervals for treatment effects, even with many covariates relative to the sample size, and without “sparsity” assumptions. We propose an “honest” approach to estimation, whereby one sample is used to construct the partition and another to estimate treatment effects for each subpopulation. Our approach builds on regression tree methods, modified to optimize for goodness of fit in treatment effects and to account for honest estimation. Our model selection criterion anticipates that bias will be eliminated by honest estimation and also accounts for the effect of making additional splits on the variance of treatment effect estimates within each subpopulation. We address the challenge that the “ground truth” for a causal effect is not observed for any individual unit, so that standard approaches to cross-validation must be modified. Through a simulation study, we show that for our preferred method honest estimation results in nominal coverage for 90\% confidence intervals, whereas coverage ranges between 74\% and 84\% for nonhonest approaches. Honest estimation requires estimating the model with a smaller sample size; the cost in terms of mean squared error of treatment effects for our preferred method ranges between 7–22\%.},
	language = {en},
	number = {27},
	urldate = {2018-01-03},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Athey, Susan and Imbens, Guido},
	month = jul,
	year = {2016},
	pmid = {27382149},
	keywords = {causal inference, cross-validation, heterogeneous treatment effects, potential outcomes, supervised machine learning},
	pages = {7353--7360},
}

@article{athey_machine_2015,
	title = {Machine learning methods for estimating heterogeneous causal effects},
	volume = {1050},
	number = {5},
	journal = {stat},
	author = {Athey, Susan and Imbens, Guido W.},
	year = {2015},
}

@article{wager_estimation_2015,
	title = {Estimation and {Inference} of {Heterogeneous} {Treatment} {Effects} using {Random} {Forests}},
	url = {http://arxiv.org/abs/1510.04342},
	abstract = {Many scientific and engineering challenges -- ranging from personalized medicine to customized marketing recommendations -- require an understanding of treatment effect heterogeneity. In this paper, we develop a non-parametric causal forest for estimating heterogeneous treatment effects that extends Breiman's widely used random forest algorithm. In the potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect, and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms. To our knowledge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially in the presence of irrelevant covariates.},
	urldate = {2018-01-03},
	journal = {arXiv:1510.04342 [math, stat]},
	author = {Wager, Stefan and Athey, Susan},
	month = oct,
	year = {2015},
	note = {arXiv: 1510.04342},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Methodology},
}

@article{rotmensch_learning_2017,
	title = {Learning a {Health} {Knowledge} {Graph} from {Electronic} {Medical} {Records}},
	volume = {7},
	copyright = {2017 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-017-05778-z},
	doi = {10.1038/s41598-017-05778-z},
	abstract = {Demand for clinical decision support systems in medicine and self-diagnostic symptom checkers has substantially increased in recent years. Existing platforms rely on knowledge bases manually compiled through a labor-intensive process or automatically derived using simple pairwise statistics. This study explored an automated process to learn high quality knowledge bases linking diseases and symptoms directly from electronic medical records. Medical concepts were extracted from 273,174 de-identified patient records and maximum likelihood estimation of three probabilistic models was used to automatically construct knowledge graphs: logistic regression, naive Bayes classifier and a Bayesian network using noisy OR gates. A graph of disease-symptom relationships was elicited from the learned parameters and the constructed knowledge graphs were evaluated and validated, with permission, against Google’s manually-constructed knowledge graph and against expert physician opinions. Our study shows that direct and automated construction of high quality health knowledge graphs from medical records using rudimentary concept extraction is feasible. The noisy OR model produces a high quality knowledge graph reaching precision of 0.85 for a recall of 0.6 in the clinical evaluation. Noisy OR significantly outperforms all tested models across evaluation frameworks (p {\textless} 0.01).},
	language = {En},
	number = {1},
	urldate = {2018-01-03},
	journal = {Scientific Reports},
	author = {Rotmensch, Maya and Halpern, Yoni and Tlimat, Abdulhakim and Horng, Steven and Sontag, David},
	month = jul,
	year = {2017},
	pages = {5994},
}

@article{saxe_complex_2016,
	title = {A complex systems approach to causal discovery in psychiatry},
	volume = {11},
	number = {3},
	journal = {PloS one},
	author = {Saxe, Glenn N. and Statnikov, Alexander and Fenyo, David and Ren, Jiwen and Li, Zhiguo and Prasad, Meera and Wall, Dennis and Bergman, Nora and Briggs, Ernestine C. and Aliferis, Constantin},
	year = {2016},
	pages = {e0151174},
}

@article{zhang_kernel-based_2012,
	title = {Kernel-based {Conditional} {Independence} {Test} and {Application} in {Causal} {Discovery}},
	url = {http://arxiv.org/abs/1202.3775},
	abstract = {Conditional independence testing is an important problem, especially in Bayesian network learning and causal discovery. Due to the curse of dimensionality, testing for conditional independence of continuous variables is particularly challenging. We propose a Kernel-based Conditional Independence test (KCI-test), by constructing an appropriate test statistic and deriving its asymptotic distribution under the null hypothesis of conditional independence. The proposed method is computationally efficient and easy to implement. Experimental results show that it outperforms other methods, especially when the conditioning set is large or the sample size is not very large, in which case other methods encounter difficulties.},
	urldate = {2017-12-21},
	journal = {arXiv:1202.3775 [cs, stat]},
	author = {Zhang, Kun and Peters, Jonas and Janzing, Dominik and Schoelkopf, Bernhard},
	month = feb,
	year = {2012},
	note = {arXiv: 1202.3775},
	keywords = {Computer Science - Learning, HSIC, Statistics - Machine Learning, kCI, kPC},
}

@article{janzinga_information-geometric_2012,
	title = {Information-geometric approach to inferring causal directions},
	volume = {182},
	number = {183},
	journal = {Artificial Intelligence},
	author = {Janzinga, Dominik and Mooij, Joris and Zhang, Kun and Lemeirec, Jan and Zscheischler, Jakob and Daniušis, Povilas and Steudelf, Bastian and Schölkopf, Bernhard},
	year = {2012},
	pages = {1--31},
}

@article{janzing_information-geometric_2012,
	title = {Information-geometric approach to inferring causal directions},
	volume = {182},
	journal = {Artificial Intelligence},
	author = {Janzing, Dominik and Mooij, Joris and Zhang, Kun and Lemeire, Jan and Zscheischler, Jakob and Daniušis, Povilas and Steudel, Bastian and Schölkopf, Bernhard},
	year = {2012},
	pages = {1--31},
}

@article{matteson_independent_2013,
	title = {Independent component analysis via distance covariance},
	journal = {Journal of the American Statistical Association},
	author = {Matteson, David S. and Tsay, Ruey S.},
	year = {2013},
	pages = {1--16},
}

@inproceedings{gretton_nonlinear_2009,
	title = {Nonlinear directed acyclic structure learning with weakly additive noise models},
	booktitle = {Advances in neural information processing systems},
	author = {Gretton, Arthur and Spirtes, Peter and Tillman, Robert E.},
	year = {2009},
	keywords = {Additive noise model, HSIC, kPC},
	pages = {1847--1855},
}

@inproceedings{hoyer_nonlinear_2009,
	title = {Nonlinear causal discovery with additive noise models},
	booktitle = {Advances in neural information processing systems},
	author = {Hoyer, Patrik O. and Janzing, Dominik and Mooij, Joris M. and Peters, Jonas and Schölkopf, Bernhard},
	year = {2009},
	keywords = {Additive noise model},
	pages = {689--696},
}

@inproceedings{mooij_regression_2009,
	title = {Regression by dependence minimization and its application to causal inference in additive noise models},
	booktitle = {Proceedings of the 26th annual international conference on machine learning},
	publisher = {ACM},
	author = {Mooij, Joris and Janzing, Dominik and Peters, Jonas and Schölkopf, Bernhard},
	year = {2009},
	keywords = {Additive noise model, HSIC},
	pages = {745--752},
}

@article{colombo_learning_2012,
	title = {Learning high-dimensional directed acyclic graphs with latent and selection variables},
	journal = {The Annals of Statistics},
	author = {Colombo, Diego and Maathuis, Marloes H. and Kalisch, Markus and Richardson, Thomas S.},
	year = {2012},
	keywords = {RFCI},
	pages = {294--321},
}

@article{taylor_statistical_2015,
	title = {Statistical learning and selective inference},
	volume = {112},
	number = {25},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Taylor, Jonathan and Tibshirani, Robert J.},
	year = {2015},
	keywords = {selection},
	pages = {7629--7634},
}

@article{rubin_for_2008,
	title = {For objective causal inference, design trumps analysis},
	journal = {The Annals of Applied Statistics},
	author = {Rubin, Donald B.},
	year = {2008},
	pages = {808--840},
}

@article{rubin_for_2008-1,
	title = {For objective causal inference, design trumps analysis},
	journal = {The Annals of Applied Statistics},
	author = {Rubin, Donald B.},
	year = {2008},
	pages = {808--840},
}

@article{gretton_measuring_2005,
	title = {Measuring {Statistical} {Dependence} with {Hilbert}-{Schmidt} {Norms}},
	author = {Gretton, Arthur and Bousquet, Olivier and Smola, Alexander and Scholkopf, Bernhard},
	year = {2005},
	keywords = {HSIC},
}

@inproceedings{fukumizu_kernel_2008,
	title = {Kernel measures of conditional dependence},
	booktitle = {Advances in neural information processing systems},
	author = {Fukumizu, Kenji and Gretton, Arthur and Sun, Xiaohai and Schölkopf, Bernhard},
	year = {2008},
	keywords = {HSIC},
	pages = {489--496},
}

@article{pfister_kernel-based_2016,
	title = {Kernel-based tests for joint independence},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Pfister, Niklas and Bühlmann, Peter and Schölkopf, Bernhard and Peters, Jonas},
	year = {2016},
}

@article{szekely_measuring_2007,
	title = {Measuring and testing dependence by correlation of distances},
	volume = {35},
	number = {6},
	journal = {The Annals of Statistics},
	author = {SZÉKELY, GÁBOR J. and RIZZO, MARIA L. and BAKIROV, NAIL K.},
	year = {2007},
	pages = {2769--2794},
}

@article{szekely_measuring_2007-1,
	title = {Measuring and testing dependence by correlation of distances},
	volume = {35},
	number = {6},
	journal = {The annals of statistics},
	author = {Székely, Gábor J. and Rizzo, Maria L. and Bakirov, Nail K.},
	year = {2007},
	pages = {2769--2794},
}

@inproceedings{sun_kernel-based_2007,
	title = {A kernel-based causal learning algorithm},
	booktitle = {Proceedings of the 24th international conference on {Machine} learning},
	publisher = {ACM},
	author = {Sun, Xiaohai and Janzing, Dominik and Schölkopf, Bernhard and Fukumizu, Kenji},
	year = {2007},
	pages = {855--862},
}

@book{rasmussen_gaussian_2006,
	title = {Gaussian processes for machine learning},
	volume = {1},
	publisher = {MIT press Cambridge},
	author = {Rasmussen, Carl Edward and Williams, Christopher KI},
	year = {2006},
}

@article{gretton_kernel_2005,
	title = {Kernel methods for measuring independence},
	volume = {6},
	number = {Dec},
	journal = {Journal of Machine Learning Research},
	author = {Gretton, Arthur and Herbrich, Ralf and Smola, Alexander and Bousquet, Olivier and Schölkopf, Bernhard},
	year = {2005},
	pages = {2075--2129},
}

@article{rada-iglesias_is_2018,
	title = {Is {H3K4me1} at enhancers correlative or causative?},
	volume = {50},
	number = {1},
	journal = {Nature Genetics},
	author = {Rada-Iglesias, Alvaro},
	year = {2018},
	pages = {4},
}

@article{purnell_cell_2017,
	title = {Cell mechanics indicate cell fate},
	volume = {358},
	copyright = {Copyright © 2017 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuse},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/358/6370/1552.3},
	doi = {10.1126/science.358.6370.1552-c},
	abstract = {Cell Reprogramming
Gene expression changes are accompanied by biophysical phenotypes during differentiation or reprogramming, as has now been shown by measurements of cell stiffness or relative compliancy. Using real-time deformability cytometry (a microfluidic-based method that deforms cells by},
	language = {en},
	number = {6370},
	urldate = {2017-12-26},
	journal = {Science},
	author = {Purnell, Beverly A.},
	month = dec,
	year = {2017},
	pages = {1552--1553},
}

@inproceedings{sun_distinguishing_2007,
	title = {Distinguishing between cause and effect via kernel-based complexity measures for conditional distributions.},
	booktitle = {{ESANN}},
	author = {Sun, Xiaohai and Janzing, Dominik and Schölkopf, Bernhard},
	year = {2007},
	pages = {441--446},
}

@article{silva_learning_2006,
	title = {Learning the structure of linear latent variable models},
	volume = {7},
	number = {Feb},
	journal = {Journal of Machine Learning Research},
	author = {Silva, Ricardo and Scheine, Richard and Glymour, Clark and Spirtes, Peter},
	year = {2006},
	pages = {191--246},
}

@article{schurch_how_2016,
	title = {How many biological replicates are needed in an {RNA}-seq experiment and which differential expression tool should you use?},
	volume = {22},
	issn = {1355-8382, 1469-9001},
	url = {http://rnajournal.cshlp.org/content/22/6/839},
	doi = {10.1261/rna.053959.115},
	abstract = {RNA-seq is now the technology of choice for genome-wide differential gene expression experiments, but it is not clear how many biological replicates are needed to ensure valid biological interpretation of the results or which statistical tools are best for analyzing the data. An RNA-seq experiment with 48 biological replicates in each of two conditions was performed to answer these questions and provide guidelines for experimental design. With three biological replicates, nine of the 11 tools evaluated found only 20\%–40\% of the significantly differentially expressed (SDE) genes identified with the full set of 42 clean replicates. This rises to {\textgreater}85\% for the subset of SDE genes changing in expression by more than fourfold. To achieve {\textgreater}85\% for all SDE genes regardless of fold change requires more than 20 biological replicates. The same nine tools successfully control their false discovery rate at ≲5\% for all numbers of replicates, while the remaining two tools fail to control their FDR adequately, particularly for low numbers of replicates. For future RNA-seq experiments, these results suggest that at least six biological replicates should be used, rising to at least 12 when it is important to identify SDE genes for all fold changes. If fewer than 12 replicates are used, a superior combination of true positive and false positive performances makes edgeR and DESeq2 the leading tools. For higher replicate numbers, minimizing false positives is more important and DESeq marginally outperforms the other tools.},
	language = {en},
	number = {6},
	urldate = {2017-12-22},
	journal = {RNA},
	author = {Schurch, Nicholas J. and Schofield, Pietá and Gierliński, Marek and Cole, Christian and Sherstnev, Alexander and Singh, Vijender and Wrobel, Nicola and Gharbi, Karim and Simpson, Gordon G. and Owen-Hughes, Tom and Blaxter, Mark and Barton, Geoffrey J.},
	month = jun,
	year = {2016},
	pmid = {27022035},
	keywords = {RNA-seq, benchmarking, differential expression, experimental design, replication, statistical power, yeast},
	pages = {839--851},
}

@article{guo_discrepancy_2017,
	title = {The discrepancy among single nucleotide variants detected by {DNA} and {RNA} high throughput sequencing data},
	volume = {18},
	issn = {1471-2164},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5629567/},
	doi = {10.1186/s12864-017-4022-x},
	abstract = {Background
High throughput sequencing technology enables the both the human genome and transcriptome to be screened at the single nucleotide resolution. Tools have been developed to infer single nucleotide variants (SNVs) from both DNA and RNA sequencing data. To evaluate how much difference can be expected between DNA and RNA sequencing data, and among tissue sources, we designed a study to examine the single nucleotide difference among five sources of high throughput sequencing data generated from the same individual, including exome sequencing from blood, tumor and adjacent normal tissue, and RNAseq from tumor and adjacent normal tissue.

Results
Through careful quality control and analysis of the SNVs, we found little difference between DNA-DNA pairs (1\%–2\%). However, between DNA-RNA pairs, SNV differences ranged anywhere from 10\% to 20\%.

Conclusions
Only a small portion of these differences can be explained by RNA editing. Instead, the majority of the DNA-RNA differences should be attributed to technical errors from sequencing and post-processing of RNAseq data. Our analysis results suggest that SNV detection using RNAseq is subject to high false positive rates.

Electronic supplementary material
The online version of this article (doi:10.1186/s12864-017-4022-x) contains supplementary material, which is available to authorized users.},
	number = {Suppl 6},
	urldate = {2017-12-22},
	journal = {BMC Genomics},
	author = {Guo, Yan and Zhao, Shilin and Sheng, Quanhu and Samuels, David C and Shyr, Yu},
	month = oct,
	year = {2017},
	pmid = {28984205},
	pmcid = {PMC5629567},
}

@article{consortium_genetic_2017,
	title = {Genetic effects on gene expression across human tissues},
	volume = {550},
	copyright = {2017 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature24277},
	doi = {10.1038/nature24277},
	abstract = {{\textless}p{\textgreater}Samples of different body regions from hundreds of human donors are used to study how genetic variation influences gene expression levels in 44 disease-relevant tissues.{\textless}/p{\textgreater}},
	language = {En},
	number = {7675},
	urldate = {2017-12-22},
	journal = {Nature},
	author = {Consortium, GTEx},
	month = oct,
	year = {2017},
	pages = {204},
}

@book{gelman_bayesian_2014,
	title = {Bayesian data analysis},
	volume = {2},
	publisher = {CRC press Boca Raton, FL},
	author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
	year = {2014},
}

@article{frangakis_principal_2002,
	title = {Principal stratification in causal inference},
	volume = {58},
	number = {1},
	journal = {Biometrics},
	author = {Frangakis, Constantine E. and Rubin, Donald B.},
	year = {2002},
	pages = {21--29},
}

@incollection{heckerman_bayesian_2006,
	series = {Studies in {Fuzziness} and {Soft} {Computing}},
	title = {A {Bayesian} {Approach} to {Causal} {Discovery}},
	isbn = {978-3-540-30609-2 978-3-540-33486-6},
	url = {https://link.springer.com/chapter/10.1007/3-540-33486-6_1},
	abstract = {We examine the Bayesian approach to the discovery of causal DAG models and compare it to the constraint-based approach. Both approaches rely on the Causal Markov condition, but the two differ significantly in theory and practice. An important difference between the approaches is that the constraint-based approach uses categorical information about conditional-independence constraints in the domain, whereas the Bayesian approach weighs the degree to which such constraints hold. As a result, the Bayesian approach has three distinct advantages over its constraint-based counterpart. One, conclusions derived from the Bayesian approach are not susceptible to incorrect categorical decisions about independence facts that can occur with data sets of finite size. Two, using the Bayesian approach, finer distinctions among model structures—both quantitative and qualitative—can be made. Three, information from several models can be combined to make better inferences and to better account for modeling uncertainty. In addition to describing the general Bayesian approach to causal discovery, we review approximation methods for missing data and hidden variables, and illustrate differences between the Bayesian and constraint-based methods using artificial and real examples.},
	language = {en},
	urldate = {2017-12-19},
	booktitle = {Innovations in {Machine} {Learning}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Heckerman, David and Meek, Christopher and Cooper, Gregory},
	year = {2006},
	doi = {10.1007/3-540-33486-6_1},
	pages = {1--28},
}

@book{peters_elements_2017,
	title = {Elements of causal inference: foundations and learning algorithms},
	volume = {12},
	shorttitle = {Elements of causal inference},
	author = {Peters, Jonas and Janzing, Dominik and Schölkopf, Bernhard},
	year = {2017},
}

@article{friedman_using_2000,
	title = {Using {Bayesian} networks to analyze expression data},
	volume = {7},
	number = {3-4},
	journal = {Journal of computational biology},
	author = {Friedman, Nir and Linial, Michal and Nachman, Iftach and Pe'er, Dana},
	year = {2000},
	pages = {601--620},
}

@article{tashiro_estimation_2012,
	title = {Estimation of causal orders in a linear non-{Gaussian} acyclic model: a method robust against latent confounders},
	shorttitle = {Estimation of causal orders in a linear non-{Gaussian} acyclic model},
	journal = {Artificial Neural Networks and Machine Learning–ICANN 2012},
	author = {Tashiro, Tatsuya and Shimizu, Shohei and Hyvärinen, Aapo and Washio, Takashi},
	year = {2012},
	keywords = {LINGAM},
	pages = {491--498},
}

@article{shimizu_linear_2006,
	title = {A linear non-{Gaussian} acyclic model for causal discovery},
	volume = {7},
	number = {Oct},
	journal = {Journal of Machine Learning Research},
	author = {Shimizu, Shohei and Hoyer, Patrik O. and Hyvärinen, Aapo and Kerminen, Antti},
	year = {2006},
	keywords = {LINGAM},
	pages = {2003--2030},
}

@inproceedings{inazumi_use_2010,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Use of {Prior} {Knowledge} in a {Non}-{Gaussian} {Method} for {Learning} {Linear} {Structural} {Equation} {Models}},
	isbn = {978-3-642-15994-7 978-3-642-15995-4},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-15995-4_28},
	doi = {10.1007/978-3-642-15995-4_28},
	abstract = {We discuss causal structure learning based on linear structural equation models. Conventional learning methods most often assume Gaussianity and create many indistinguishable models. Therefore, in many cases it is difficult to obtain much information on the structure. Recently, a non-Gaussian learning method called LiNGAM has been proposed to identify the model structure without using prior knowledge on the structure. However, more efficient learning can be achieved if some prior knowledge on a part of the structure is available. In this paper, we propose to use prior knowledge to improve the performance of a state-of-art non-Gaussian method. Experiments on artificial data show that the accuracy and computational time are significantly improved even if the amount of prior knowledge is not so large.},
	language = {en},
	urldate = {2017-12-18},
	booktitle = {Latent {Variable} {Analysis} and {Signal} {Separation}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Inazumi, Takanori and Shimizu, Shohei and Washio, Takashi},
	month = sep,
	year = {2010},
	keywords = {LINGAM},
	pages = {221--228},
}

@article{shimizu_directlingam:_2011,
	title = {{DirectLiNGAM}: {A} direct method for learning a linear non-{Gaussian} structural equation model},
	volume = {12},
	shorttitle = {{DirectLiNGAM}},
	number = {Apr},
	journal = {Journal of Machine Learning Research},
	author = {Shimizu, Shohei and Inazumi, Takanori and Sogawa, Yasuhiro and Hyvärinen, Aapo and Kawahara, Yoshinobu and Washio, Takashi and Hoyer, Patrik O. and Bollen, Kenneth},
	year = {2011},
	keywords = {LINGAM},
	pages = {1225--1248},
}

@book{pearl_causality_2009,
	title = {Causality},
	publisher = {Cambridge university press},
	author = {Pearl, Judea},
	year = {2009},
}

@article{meinshausen_methods_2016,
	title = {Methods for causal inference from gene perturbation experiments and validation},
	volume = {113},
	number = {27},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Meinshausen, Nicolai and Hauser, Alain and Mooij, Joris M. and Peters, Jonas and Versteeg, Philip and Bühlmann, Peter},
	year = {2016},
	pages = {7361--7368},
}

@article{peters_causal_2016,
	title = {Causal inference by using invariant prediction: identification and confidence intervals},
	volume = {78},
	shorttitle = {Causal inference by using invariant prediction},
	number = {5},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Peters, Jonas and Bühlmann, Peter and Meinshausen, Nicolai},
	year = {2016},
	pages = {947--1012},
}

@inproceedings{meek_causal_1995,
	title = {Causal inference and causal explanation with background knowledge},
	booktitle = {Proceedings of the {Eleventh} conference on {Uncertainty} in artificial intelligence},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Meek, Christopher},
	year = {1995},
	pages = {403--410},
}

@inproceedings{lopez-paz_towards_2015,
	title = {Towards a learning theory of cause-effect inference},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Lopez-Paz, David and Muandet, Krikamol and Schölkopf, Bernhard and Tolstikhin, Iliya},
	year = {2015},
	pages = {1452--1461},
}

@article{triantafillou_constraint-based_2015,
	title = {Constraint-based causal discovery from multiple interventions over overlapping variable sets.},
	volume = {16},
	journal = {Journal of Machine Learning Research},
	author = {Triantafillou, Sofia and Tsamardinos, Ioannis},
	year = {2015},
	pages = {2147--2205},
}

@article{mooij_distinguishing_2016,
	title = {Distinguishing {Cause} from {Effect} {Using} {Observational} {Data}: {Methods} and {Benchmarks}},
	volume = {17},
	shorttitle = {Distinguishing {Cause} from {Effect} {Using} {Observational} {Data}},
	url = {https://dare.uva.nl/search?identifier=f4e35a2b-cd22-41a4-93e5-7f0b1cf8090a},
	number = {32},
	urldate = {2017-12-18},
	journal = {Journal of Machine Learning Research},
	author = {Mooij, J. M. and Peters, J. and Janzing, D. and Zscheischler, J. and Schölkopf, B.},
	year = {2016},
}

@article{harris_pc_2013,
	title = {{PC} {Algorithm} for {Nonparanormal} {Graphical} {Models}},
	volume = {14},
	url = {http://www.jmlr.org/papers/v14/harris13a.html},
	urldate = {2017-12-18},
	journal = {Journal of Machine Learning Research},
	author = {Harris, Naftali and Drton, Mathias},
	year = {2013},
	pages = {3365--3383},
}

@inproceedings{scheines_introduction_1997,
	title = {An {Introduction} to {Causal} {Inference}},
	abstract = {developed a theory of statistical causal inference. In his presentation at the Notre Dame},
	booktitle = {Causality in {Crisis}? {University} of {Notre} {Dame}},
	publisher = {Press},
	author = {Scheines, Richard},
	year = {1997},
	pages = {185--200},
}

@article{heckerman_learning_1995,
	title = {Learning {Bayesian} networks: {The} combination of knowledge and statistical data},
	volume = {20},
	issn = {0885-6125, 1573-0565},
	shorttitle = {Learning {Bayesian} networks},
	url = {https://link.springer.com/article/10.1007/BF00994016},
	doi = {10.1007/BF00994016},
	abstract = {We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption oflikelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user's priors for network parameters can be encoded in a single Bayesian network for the next case to be seen—aprior network—and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at mostk=1 parent. For the general case (k{\textgreater}1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches.},
	language = {en},
	number = {3},
	urldate = {2017-12-18},
	journal = {Machine Learning},
	author = {Heckerman, David and Geiger, Dan and Chickering, David M.},
	month = sep,
	year = {1995},
	pages = {197--243},
}

@article{hyttinen_discovering_2013,
	title = {Discovering {Cyclic} {Causal} {Models} with {Latent} {Variables}: {A} {General} {SAT}-{Based} {Procedure}},
	shorttitle = {Discovering {Cyclic} {Causal} {Models} with {Latent} {Variables}},
	url = {http://arxiv.org/abs/1309.6836},
	abstract = {We present a very general approach to learning the structure of causal models based on d-separation constraints, obtained from any given set of overlapping passive observational or experimental data sets. The procedure allows for both directed cycles (feedback loops) and the presence of latent variables. Our approach is based on a logical representation of causal pathways, which permits the integration of quite general background knowledge, and inference is performed using a Boolean satisfiability (SAT) solver. The procedure is complete in that it exhausts the available information on whether any given edge can be determined to be present or absent, and returns "unknown" otherwise. Many existing constraint-based causal discovery algorithms can be seen as special cases, tailored to circumstances in which one or more restricting assumptions apply. Simulations illustrate the effect of these assumptions on discovery and how the present algorithm scales.},
	urldate = {2017-12-18},
	journal = {arXiv:1309.6836 [cs]},
	author = {Hyttinen, Antti and Hoyer, Patrik O. and Eberhardt, Frederick and Jarvisalo, Matti},
	month = sep,
	year = {2013},
	note = {arXiv: 1309.6836},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{frot_learning_2017,
	title = {Learning {Directed} {Acyclic} {Graphs} with {Hidden} {Variables} via {Latent} {Gaussian} {Graphical} {Model} {Selection}},
	url = {http://arxiv.org/abs/1708.01151},
	abstract = {We introduce a new method to estimate the Markov equivalence class of a directed acyclic graph (DAG) in the presence of hidden variables, in settings where the underlying DAG among the observed variables is sparse, and there are a few hidden variables that have a direct effect on many of the observed ones. Building on the so-called low rank plus sparse framework, we suggest a two-stage approach which first removes unwanted variation using latent Gaussian graphical model selection, and then estimates the Markov equivalence class of the underlying DAG by applying GES. This approach is consistent in certain high-dimensional regimes and performs favourably when compared to the state of the art, both in terms of graphical structure recovery and total causal effect estimation.},
	urldate = {2017-12-18},
	journal = {arXiv:1708.01151 [stat]},
	author = {Frot, Benjamin and Nandy, Preetam and Maathuis, Marloes H.},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.01151},
	keywords = {Statistics - Methodology},
}

@article{drton_structure_2017,
	title = {Structure {Learning} in {Graphical} {Modeling}},
	volume = {4},
	url = {https://doi.org/10.1146/annurev-statistics-060116-053803},
	doi = {10.1146/annurev-statistics-060116-053803},
	abstract = {A graphical model is a statistical model that is associated with a graph whose nodes correspond to variables of interest. The edges of the graph reflect allowed conditional dependencies among the variables. Graphical models have computationally convenient factorization properties and have long been a valuable tool for tractable modeling of multivariate distributions. More recently, applications such as reconstructing gene regulatory networks from gene expression data have driven major advances in structure learning, that is, estimating the graph underlying a model. We review some of these advances and discuss methods such as the graphical lasso and neighborhood selection for undirected graphical models (or Markov random fields) and the PC algorithm and score-based search methods for directed graphical models (or Bayesian networks). We further review extensions that account for effects of latent variables and heterogeneous data sources.},
	number = {1},
	urldate = {2017-12-18},
	journal = {Annual Review of Statistics and Its Application},
	author = {Drton, Mathias and Maathuis, Marloes H.},
	year = {2017},
	pages = {365--393},
}

@article{malec_literature-based_2017,
	title = {Literature-{Based} {Discovery} of {Confounding} in {Observational} {Clinical} {Data}},
	volume = {2016},
	issn = {1942-597X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5333204/},
	abstract = {Observational data recorded in the Electronic Health Record (EHR) can help us better understand the effects of therapeutic agents in routine clinical practice. As such data were not collected for research purposes, their reuse for research must compensate for additional information that may bias analyses and lead to faulty conclusions. Confounding is present when factors aside from the given predictor(s) affect the response of interest. However, these additional factors may not be known at the outset. In this paper, we present a scalable literature-based confounding variable discovery method for biomedical research applications with pharmacovigilance as our use case. We hypothesized that statistical models, adjusted with literature-derived confounders, will more accurately identify causative drug-adverse drug event (ADE) relationships. We evaluated our method with a curated reference standard, and found a pattern of improved performance {\textasciitilde} 5\% in two out of three models for gastrointestinal bleeding (pre-adjusted Area Under Curve ≥ 0.6).},
	urldate = {2017-12-18},
	journal = {AMIA Annual Symposium Proceedings},
	author = {Malec, Scott A. and Wei, Peng and Xu, Hua and Bernstam, Elmer V. and Myneni, Sahiti and Cohen, Trevor},
	month = feb,
	year = {2017},
	pmid = {28269951},
	pmcid = {PMC5333204},
	pages = {1920--1929},
}

@article{nandy_estimating_2017,
	title = {Estimating the effect of joint interventions from observational data in sparse high-dimensional settings},
	volume = {45},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1494921953},
	doi = {10.1214/16-AOS1462},
	abstract = {We consider the estimation of joint causal effects from observational data. In particular, we propose new methods to estimate the effect of multiple simultaneous interventions (e.g., multiple gene knockouts), under the assumption that the observational data come from an unknown linear structural equation model with independent errors. We derive asymptotic variances of our estimators when the underlying causal structure is partly known, as well as high-dimensional consistency when the causal structure is fully unknown and the joint distribution is multivariate Gaussian. We also propose a generalization of our methodology to the class of nonparanormal distributions. We evaluate the estimators in simulation studies and also illustrate them on data from the DREAM4 challenge.},
	language = {EN},
	number = {2},
	urldate = {2017-12-18},
	journal = {The Annals of Statistics},
	author = {Nandy, Preetam and Maathuis, Marloes H. and Richardson, Thomas S.},
	month = apr,
	year = {2017},
	zmnumber = {06754746},
	keywords = {Causal inference, directed acyclic graph (DAG), high-dimensional data, joint causal effects, linear structural equation model (linear SEM), multiple simultaneous interventions, nonparanormal distribution},
	pages = {647--674},
}

@inproceedings{affeldt_3off2:_2016,
	title = {3off2: {A} network reconstruction algorithm based on 2-point and 3-point information statistics},
	volume = {17},
	booktitle = {{BMC} bioinformatics},
	publisher = {BioMed Central Ltd},
	author = {Affeldt, Séverine and Verny, Louis and Isambert, Hervé},
	year = {2016},
	keywords = {team},
	pages = {12},
}

@inproceedings{affeldt_robust_2015,
	title = {Robust reconstruction of causal graphical models based on conditional 2-point and 3-point information},
	booktitle = {Proceedings of the {UAI} 2015 {Conference} on {Advances} in {Causal} {Inference}-{Volume} 1504},
	publisher = {CEUR-WS. org},
	author = {Affeldt, Séverine and Isambert, Hervé},
	year = {2015},
	keywords = {team},
	pages = {1--29},
}

@book{pearl_causal_2016,
	title = {Causal inference in statistics: a primer},
	publisher = {John Wiley \& Sons},
	author = {Pearl, Judea and Glymour, Madelyn and Jewell, Nicholas P},
	year = {2016},
}
